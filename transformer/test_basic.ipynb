{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MZR5xHg676jH"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<module 'models' from 'c:\\\\Users\\\\admin\\\\Documents\\\\lista-5-KunickiKarol\\\\models.py'>"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch.utils\n",
        "import torch.distributions\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision\n",
        "import os\n",
        "\n",
        "import gdown\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import transformers\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import AdamW, AutoModel, AutoTokenizer\n",
        "\n",
        "from utils.train import *\n",
        "from models import *\n",
        "\n",
        "root = '.'\n",
        "\n",
        "from importlib import reload\n",
        "import utils.train\n",
        "reload(utils.train)\n",
        "\n",
        "from importlib import reload\n",
        "import models\n",
        "\n",
        "reload(models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Up6e-XQ76jJ",
        "outputId": "66a9c32d-bb75-4a7f-fb90-426f9090e049"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.manual_seed(0)\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"url = 'https://drive.google.com/uc?id=1HSnB-D0dKDI2bE9iOsp-Vr8tumihdvbH'\\noutput = 'data.csv'\\n\\ngdown.download(url, output, quiet=False)\""
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"url = 'https://drive.google.com/uc?id=1HSnB-D0dKDI2bE9iOsp-Vr8tumihdvbH'\n",
        "output = 'data.csv'\n",
        "\n",
        "gdown.download(url, output, quiet=False)\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Test base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'list' object has no attribute 'startswith'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[5], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     12\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m\n\u001b[1;32m---> 14\u001b[0m model \u001b[38;5;241m=\u001b[39m get_model(pretrained_model_name, model_type, device)\n\u001b[0;32m     16\u001b[0m train_dataloader, val_dataloader, test_data, weights \u001b[38;5;241m=\u001b[39m tokenize_data(pretrained_model_name, max_length, padding, truncation, batch_size, device)\n\u001b[0;32m     18\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optimizer(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr)\n",
            "File \u001b[1;32mc:\\Users\\admin\\Documents\\lista-5-KunickiKarol\\utils\\train.py:204\u001b[0m, in \u001b[0;36mget_model\u001b[1;34m(pretrained_model_name, model_type, device)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_model\u001b[39m(pretrained_model_name, model_type, device):\n\u001b[1;32m--> 204\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pretrained_model_name\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroberta\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    205\u001b[0m         non_freeze \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpooler\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'startswith'"
          ]
        }
      ],
      "source": [
        "exp_title = 'tmp'\n",
        "# xlm-roberta-base xlm-roberta-large bert-base-uncased bert-large-uncased roberta-base roberta-large\n",
        "pretrained_model_name = 'roberta-base'\n",
        "model_type = Ext_Arch\n",
        "epochs=1\n",
        "lr = 0.0005\n",
        "optimizer = torch.optim.AdamW\n",
        "loss_func = nn.NLLLoss\n",
        "max_length = 25 \n",
        "padding = 'max_length'\n",
        "truncation=True\n",
        "batch_size = 32\n",
        "\n",
        "model = get_model(pretrained_model_name, model_type, device)\n",
        "\n",
        "train_dataloader, val_dataloader, test_data, weights = tokenize_data(pretrained_model_name, max_length, padding, truncation, batch_size, device)\n",
        "\n",
        "optimizer = optimizer(model.parameters(), lr=lr)\n",
        "loss_func  = loss_func(weight=weights)\n",
        "\n",
        "\n",
        "# batch_size, epochs, lr,  optimizer, l1_weight, weight_decay, distribution, size_NICE\n",
        "for exp_index, _ in zip(range(1), range(1)):\n",
        "\n",
        "    do_exp(model, train_dataloader, val_dataloader, test_data, device, loss_func, optimizer, epochs, lr, \n",
        "           batch_size, max_length, padding, truncation, pretrained_model_name, exp_title, exp_index, root)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Test padding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment: 0 / 15\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.693\tValidation Loss: 0.687\tTime: 5.828117370605469\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.687\tValidation Loss: 0.683\tTime: 5.079919815063477\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.684\tValidation Loss: 0.680\tTime: 5.064733505249023\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.685\tValidation Loss: 0.678\tTime: 5.073146820068359\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.678\tValidation Loss: 0.676\tTime: 5.117013931274414\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.682\tValidation Loss: 0.677\tTime: 3.6167967319488525\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.677\tValidation Loss: 0.672\tTime: 5.1002724170684814\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.676\tValidation Loss: 0.674\tTime: 3.639496326446533\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.673\tValidation Loss: 0.669\tTime: 5.110859394073486\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.671\tValidation Loss: 0.669\tTime: 3.642477035522461\n",
            "Saving model 0\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 1 / 15\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.693\tValidation Loss: 0.687\tTime: 13.798702955245972\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.688\tValidation Loss: 0.682\tTime: 14.047613382339478\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.684\tValidation Loss: 0.680\tTime: 14.365260124206543\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.684\tValidation Loss: 0.679\tTime: 14.600563764572144\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.681\tValidation Loss: 0.678\tTime: 14.71591830253601\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.679\tValidation Loss: 0.674\tTime: 14.852901220321655\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.676\tValidation Loss: 0.670\tTime: 13.581693410873413\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.674\tValidation Loss: 0.665\tTime: 14.089348077774048\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.671\tValidation Loss: 0.665\tTime: 10.922836065292358\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.666\tValidation Loss: 0.661\tTime: 13.228285789489746\n",
            "Saving model 1\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 2 / 15\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.677\tValidation Loss: 0.663\tTime: 4.125685214996338\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.645\tValidation Loss: 0.635\tTime: 3.8647375106811523\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.621\tValidation Loss: 0.605\tTime: 3.9899885654449463\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.591\tValidation Loss: 0.580\tTime: 4.341428995132446\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.568\tValidation Loss: 0.556\tTime: 4.275224447250366\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.552\tValidation Loss: 0.534\tTime: 4.190072774887085\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.527\tValidation Loss: 0.514\tTime: 4.14766263961792\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.511\tValidation Loss: 0.498\tTime: 4.1418070793151855\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.495\tValidation Loss: 0.484\tTime: 4.121216773986816\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.476\tValidation Loss: 0.469\tTime: 4.140202760696411\n",
            "Saving model 2\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment: 3 / 15\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.687\tValidation Loss: 0.683\tTime: 4.406893253326416\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.680\tValidation Loss: 0.674\tTime: 4.415425539016724\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.670\tValidation Loss: 0.664\tTime: 4.342432260513306\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.654\tValidation Loss: 0.650\tTime: 4.348204851150513\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.637\tValidation Loss: 0.628\tTime: 4.377101182937622\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.611\tValidation Loss: 0.603\tTime: 4.317710638046265\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.580\tValidation Loss: 0.568\tTime: 4.328960418701172\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.541\tValidation Loss: 0.527\tTime: 4.3431103229522705\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.498\tValidation Loss: 0.483\tTime: 4.33643364906311\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.453\tValidation Loss: 0.441\tTime: 4.365661382675171\n",
            "Saving model 3\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment: 4 / 15\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.688\tValidation Loss: 0.686\tTime: 13.466208696365356\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.670\tValidation Loss: 0.662\tTime: 13.288357496261597\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.650\tValidation Loss: 0.646\tTime: 13.223090887069702\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.627\tValidation Loss: 0.618\tTime: 13.300848245620728\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.597\tValidation Loss: 0.591\tTime: 13.228503942489624\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.557\tValidation Loss: 0.557\tTime: 13.226236343383789\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.515\tValidation Loss: 0.511\tTime: 13.35335111618042\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.466\tValidation Loss: 0.459\tTime: 13.398926496505737\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.424\tValidation Loss: 0.416\tTime: 14.582383871078491\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.377\tValidation Loss: 0.378\tTime: 14.719169855117798\n",
            "Saving model 4\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 5 / 15\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.690\tValidation Loss: 0.688\tTime: 7.432201385498047\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.688\tValidation Loss: 0.685\tTime: 7.150083065032959\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.684\tValidation Loss: 0.681\tTime: 7.6093223094940186\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.682\tValidation Loss: 0.680\tTime: 6.939954996109009\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.680\tValidation Loss: 0.678\tTime: 7.509068727493286\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.678\tValidation Loss: 0.675\tTime: 6.961355686187744\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.677\tValidation Loss: 0.674\tTime: 7.500216007232666\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.674\tValidation Loss: 0.672\tTime: 6.963291168212891\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.673\tValidation Loss: 0.671\tTime: 7.321432590484619\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.669\tValidation Loss: 0.670\tTime: 7.1057257652282715\n",
            "Saving model 5\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 6 / 15\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.692\tValidation Loss: 0.684\tTime: 16.10946035385132\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.690\tValidation Loss: 0.692\tTime: 11.716501951217651\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.686\tValidation Loss: 0.681\tTime: 19.618680953979492\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.680\tValidation Loss: 0.675\tTime: 19.6559419631958\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.680\tValidation Loss: 0.674\tTime: 16.419596195220947\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.675\tValidation Loss: 0.672\tTime: 16.140137910842896\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.672\tValidation Loss: 0.668\tTime: 16.406795740127563\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.668\tValidation Loss: 0.664\tTime: 16.53765320777893\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.665\tValidation Loss: 0.662\tTime: 16.434743404388428\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.666\tValidation Loss: 0.660\tTime: 16.293975353240967\n",
            "Saving model 6\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 7 / 15\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.677\tValidation Loss: 0.655\tTime: 6.296748876571655\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.640\tValidation Loss: 0.620\tTime: 5.635801076889038\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.613\tValidation Loss: 0.595\tTime: 5.551161527633667\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.591\tValidation Loss: 0.575\tTime: 5.623586654663086\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.570\tValidation Loss: 0.548\tTime: 5.394350051879883\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.546\tValidation Loss: 0.534\tTime: 5.633922576904297\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.526\tValidation Loss: 0.508\tTime: 5.542198657989502\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.518\tValidation Loss: 0.489\tTime: 5.430284023284912\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.492\tValidation Loss: 0.472\tTime: 5.501290321350098\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.472\tValidation Loss: 0.457\tTime: 5.38045334815979\n",
            "Saving model 7\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment: 8 / 15\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.688\tValidation Loss: 0.686\tTime: 5.745516300201416\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.680\tValidation Loss: 0.674\tTime: 5.538600206375122\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.668\tValidation Loss: 0.662\tTime: 5.6431989669799805\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.653\tValidation Loss: 0.648\tTime: 5.631262540817261\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.634\tValidation Loss: 0.626\tTime: 5.554235458374023\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.609\tValidation Loss: 0.599\tTime: 5.075707197189331\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.570\tValidation Loss: 0.561\tTime: 5.652823448181152\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.532\tValidation Loss: 0.521\tTime: 5.088184118270874\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.485\tValidation Loss: 0.479\tTime: 5.496966361999512\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.445\tValidation Loss: 0.433\tTime: 5.437357425689697\n",
            "Saving model 8\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment: 9 / 15\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.686\tValidation Loss: 0.676\tTime: 14.659739255905151\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.667\tValidation Loss: 0.664\tTime: 14.71202826499939\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.648\tValidation Loss: 0.640\tTime: 14.168498992919922\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.623\tValidation Loss: 0.619\tTime: 14.323237180709839\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.590\tValidation Loss: 0.594\tTime: 14.54472827911377\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.554\tValidation Loss: 0.551\tTime: 14.378467321395874\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.511\tValidation Loss: 0.504\tTime: 14.444596529006958\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.462\tValidation Loss: 0.461\tTime: 14.234448909759521\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.422\tValidation Loss: 0.420\tTime: 14.10874605178833\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.379\tValidation Loss: 0.382\tTime: 14.169330596923828\n",
            "Saving model 9\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 10 / 15\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.691\tValidation Loss: 0.688\tTime: 7.515947341918945\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.689\tValidation Loss: 0.686\tTime: 7.118050813674927\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.686\tValidation Loss: 0.683\tTime: 7.321533441543579\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.682\tValidation Loss: 0.681\tTime: 7.182263135910034\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.678\tValidation Loss: 0.675\tTime: 7.190138816833496\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.679\tValidation Loss: 0.675\tTime: 7.143956899642944\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.677\tValidation Loss: 0.674\tTime: 7.128328323364258\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.677\tValidation Loss: 0.678\tTime: 4.531370401382446\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.674\tValidation Loss: 0.670\tTime: 6.479141712188721\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.670\tValidation Loss: 0.666\tTime: 7.131145715713501\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\admin\\anaconda3\\envs\\www\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "c:\\Users\\admin\\anaconda3\\envs\\www\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "c:\\Users\\admin\\anaconda3\\envs\\www\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving model 10\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 11 / 15\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.692\tValidation Loss: 0.688\tTime: 16.81704807281494\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.689\tValidation Loss: 0.683\tTime: 16.562682151794434\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.684\tValidation Loss: 0.686\tTime: 12.175836324691772\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.684\tValidation Loss: 0.680\tTime: 16.17666244506836\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.680\tValidation Loss: 0.675\tTime: 16.73071575164795\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.677\tValidation Loss: 0.673\tTime: 16.537555694580078\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.675\tValidation Loss: 0.670\tTime: 16.165260076522827\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.674\tValidation Loss: 0.669\tTime: 15.95328974723816\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.673\tValidation Loss: 0.666\tTime: 16.363667726516724\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.670\tValidation Loss: 0.667\tTime: 12.208357334136963\n",
            "Saving model 11\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 12 / 15\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.677\tValidation Loss: 0.660\tTime: 5.986858367919922\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.649\tValidation Loss: 0.630\tTime: 5.306065797805786\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.622\tValidation Loss: 0.603\tTime: 5.3884289264678955\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.597\tValidation Loss: 0.587\tTime: 5.5148069858551025\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.571\tValidation Loss: 0.559\tTime: 5.4272356033325195\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.558\tValidation Loss: 0.537\tTime: 5.390583038330078\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.532\tValidation Loss: 0.514\tTime: 5.246319532394409\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.506\tValidation Loss: 0.494\tTime: 5.399385452270508\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.497\tValidation Loss: 0.476\tTime: 5.183516502380371\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.480\tValidation Loss: 0.463\tTime: 5.2699103355407715\n",
            "Saving model 12\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment: 13 / 15\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.689\tValidation Loss: 0.683\tTime: 5.539824962615967\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.679\tValidation Loss: 0.675\tTime: 5.235270977020264\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.668\tValidation Loss: 0.661\tTime: 5.354001045227051\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.652\tValidation Loss: 0.647\tTime: 6.654649972915649\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.634\tValidation Loss: 0.626\tTime: 6.655452728271484\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.607\tValidation Loss: 0.601\tTime: 6.8301002979278564\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.574\tValidation Loss: 0.564\tTime: 6.764105558395386\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.537\tValidation Loss: 0.523\tTime: 6.351385593414307\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.492\tValidation Loss: 0.481\tTime: 6.013921499252319\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.453\tValidation Loss: 0.435\tTime: 5.972805023193359\n",
            "Saving model 13\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment: 14 / 15\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.689\tValidation Loss: 0.682\tTime: 14.10517430305481\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.673\tValidation Loss: 0.665\tTime: 14.25537371635437\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.654\tValidation Loss: 0.657\tTime: 14.728390455245972\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.634\tValidation Loss: 0.635\tTime: 14.328200578689575\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.605\tValidation Loss: 0.599\tTime: 14.246756315231323\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.571\tValidation Loss: 0.572\tTime: 14.191677570343018\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.531\tValidation Loss: 0.529\tTime: 14.414186954498291\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.493\tValidation Loss: 0.489\tTime: 14.305181503295898\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.448\tValidation Loss: 0.446\tTime: 14.224855184555054\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.401\tValidation Loss: 0.406\tTime: 14.16176700592041\n",
            "Saving model 14\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n"
          ]
        }
      ],
      "source": [
        "paddings = ['max_length', 'longest', True]\n",
        "batch_size = 32\n",
        "optimizer_type = torch.optim.AdamW\n",
        "lr = 0.00001\n",
        "exp_title = 'test_padding'\n",
        "# xlm-roberta-base xlm-roberta-large bert-base-uncased roberta-base roberta-large\n",
        "pretrained_model_names = ['xlm-roberta-base', 'xlm-roberta-large', 'bert-base-uncased', 'roberta-base', 'roberta-large']\n",
        "loss_func_type = nn.NLLLoss\n",
        "max_length = 25 \n",
        "truncation=True\n",
        "model_type = Ext_Arch\n",
        "epochs=10\n",
        "\n",
        "exp_index = 0\n",
        "total_exp = len(paddings) * len(pretrained_model_names)\n",
        "for padding in paddings:\n",
        "    for pretrained_model_name in pretrained_model_names:\n",
        "        model = get_model(pretrained_model_name, model_type, device)\n",
        "        train_dataloader, val_dataloader, test_data, weights = tokenize_data(pretrained_model_name, max_length, padding, truncation, batch_size, device)\n",
        "        loss_func  = loss_func_type(weight=weights)\n",
        "\n",
        "        print(f'Experiment: {exp_index} / {total_exp}')\n",
        "        optimizer = optimizer_type(model.parameters(), lr=lr)\n",
        "        do_exp(model, train_dataloader, val_dataloader, test_data, device, loss_func, optimizer, epochs, lr, \n",
        "            batch_size, max_length, padding, truncation, pretrained_model_name, exp_title, exp_index, root)\n",
        "        exp_index += 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Test max length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment: 0 / 25\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.692\tValidation Loss: 0.691\tTime: 8.812742471694946\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.693\tValidation Loss: 0.690\tTime: 8.117653846740723\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.692\tValidation Loss: 0.692\tTime: 5.764745712280273\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.690\tValidation Loss: 0.691\tTime: 5.550002574920654\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.692\tValidation Loss: 0.692\tTime: 5.094322919845581\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.691\tValidation Loss: 0.692\tTime: 4.861774206161499\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.694\tValidation Loss: 0.693\tTime: 5.440462827682495\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.691\tValidation Loss: 0.691\tTime: 5.017654657363892\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.692\tValidation Loss: 0.690\tTime: 7.8777384757995605\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.691\tValidation Loss: 0.691\tTime: 4.714877605438232\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\admin\\anaconda3\\envs\\www\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "c:\\Users\\admin\\anaconda3\\envs\\www\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "c:\\Users\\admin\\anaconda3\\envs\\www\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving model 0\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 1 / 25\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.692\tValidation Loss: 0.692\tTime: 12.424054145812988\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.694\tValidation Loss: 0.690\tTime: 12.694241046905518\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.693\tValidation Loss: 0.688\tTime: 12.750400304794312\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.694\tValidation Loss: 0.690\tTime: 8.203584909439087\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.694\tValidation Loss: 0.697\tTime: 7.954288005828857\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.693\tValidation Loss: 0.691\tTime: 7.657604932785034\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.692\tValidation Loss: 0.687\tTime: 12.424090385437012\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.691\tValidation Loss: 0.689\tTime: 7.577706813812256\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.693\tValidation Loss: 0.687\tTime: 7.427446365356445\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.692\tValidation Loss: 0.692\tTime: 7.349931001663208\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\admin\\anaconda3\\envs\\www\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "c:\\Users\\admin\\anaconda3\\envs\\www\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "c:\\Users\\admin\\anaconda3\\envs\\www\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving model 1\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 2 / 25\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.692\tValidation Loss: 0.683\tTime: 4.9694600105285645\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.682\tValidation Loss: 0.677\tTime: 5.193772554397583\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.676\tValidation Loss: 0.671\tTime: 5.653102874755859\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.674\tValidation Loss: 0.666\tTime: 5.6546690464019775\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.670\tValidation Loss: 0.665\tTime: 5.825898170471191\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.668\tValidation Loss: 0.657\tTime: 5.701836824417114\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.663\tValidation Loss: 0.653\tTime: 5.56666898727417\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.656\tValidation Loss: 0.649\tTime: 5.28908634185791\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.654\tValidation Loss: 0.643\tTime: 5.543583393096924\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.649\tValidation Loss: 0.638\tTime: 5.301458120346069\n",
            "Saving model 2\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment: 3 / 25\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.692\tValidation Loss: 0.690\tTime: 5.730903148651123\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.691\tValidation Loss: 0.689\tTime: 5.321305513381958\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.691\tValidation Loss: 0.692\tTime: 5.046828508377075\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.691\tValidation Loss: 0.688\tTime: 5.784973859786987\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.688\tValidation Loss: 0.687\tTime: 6.313160419464111\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.688\tValidation Loss: 0.686\tTime: 6.00165057182312\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.687\tValidation Loss: 0.687\tTime: 5.240798234939575\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.686\tValidation Loss: 0.684\tTime: 6.149043560028076\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.684\tValidation Loss: 0.682\tTime: 6.077056646347046\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.682\tValidation Loss: 0.681\tTime: 6.0362324714660645\n",
            "Saving model 3\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment: 4 / 25\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.695\tValidation Loss: 0.689\tTime: 11.472577810287476\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.690\tValidation Loss: 0.685\tTime: 10.23245096206665\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.686\tValidation Loss: 0.681\tTime: 10.253264427185059\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.684\tValidation Loss: 0.680\tTime: 10.130308866500854\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.682\tValidation Loss: 0.680\tTime: 10.252916812896729\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.680\tValidation Loss: 0.673\tTime: 10.25270938873291\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.675\tValidation Loss: 0.672\tTime: 10.006122827529907\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.669\tValidation Loss: 0.689\tTime: 7.778691291809082\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.670\tValidation Loss: 0.662\tTime: 9.70534896850586\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.665\tValidation Loss: 0.659\tTime: 9.888771533966064\n",
            "Saving model 4\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 5 / 25\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.693\tValidation Loss: 0.688\tTime: 7.287843704223633\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.688\tValidation Loss: 0.685\tTime: 7.355474472045898\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.688\tValidation Loss: 0.682\tTime: 6.942417860031128\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.686\tValidation Loss: 0.681\tTime: 8.160885095596313\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.682\tValidation Loss: 0.680\tTime: 7.8632261753082275\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.681\tValidation Loss: 0.679\tTime: 7.968052387237549\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.681\tValidation Loss: 0.676\tTime: 8.046017408370972\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.680\tValidation Loss: 0.674\tTime: 7.843231678009033\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.679\tValidation Loss: 0.677\tTime: 5.175809860229492\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.678\tValidation Loss: 0.674\tTime: 7.695468425750732\n",
            "Saving model 5\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 6 / 25\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.693\tValidation Loss: 0.688\tTime: 17.664130449295044\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.693\tValidation Loss: 0.689\tTime: 11.746084451675415\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.691\tValidation Loss: 0.688\tTime: 15.736031293869019\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.691\tValidation Loss: 0.686\tTime: 15.723692178726196\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.687\tValidation Loss: 0.684\tTime: 15.147520780563354\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.687\tValidation Loss: 0.682\tTime: 15.199318170547485\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.687\tValidation Loss: 0.682\tTime: 15.053837299346924\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.684\tValidation Loss: 0.683\tTime: 11.102465867996216\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.685\tValidation Loss: 0.684\tTime: 10.974021673202515\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.681\tValidation Loss: 0.679\tTime: 15.518575429916382\n",
            "Saving model 6\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 7 / 25\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.685\tValidation Loss: 0.672\tTime: 5.541117429733276\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.667\tValidation Loss: 0.655\tTime: 4.938250780105591\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.652\tValidation Loss: 0.639\tTime: 5.159765720367432\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.638\tValidation Loss: 0.624\tTime: 5.0498597621917725\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.622\tValidation Loss: 0.610\tTime: 5.087071418762207\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.610\tValidation Loss: 0.595\tTime: 5.600816965103149\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.597\tValidation Loss: 0.586\tTime: 6.187213182449341\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.583\tValidation Loss: 0.567\tTime: 6.062765836715698\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.572\tValidation Loss: 0.556\tTime: 6.01166844367981\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.566\tValidation Loss: 0.545\tTime: 5.987002372741699\n",
            "Saving model 7\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment: 8 / 25\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.690\tValidation Loss: 0.688\tTime: 6.1144068241119385\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.684\tValidation Loss: 0.680\tTime: 5.861772060394287\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.677\tValidation Loss: 0.673\tTime: 5.833453178405762\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.668\tValidation Loss: 0.664\tTime: 5.949560165405273\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.660\tValidation Loss: 0.654\tTime: 6.048179388046265\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.644\tValidation Loss: 0.639\tTime: 6.440303325653076\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.627\tValidation Loss: 0.620\tTime: 6.21225905418396\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.607\tValidation Loss: 0.600\tTime: 6.752727270126343\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.584\tValidation Loss: 0.575\tTime: 5.965551376342773\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.561\tValidation Loss: 0.547\tTime: 6.720929861068726\n",
            "Saving model 8\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment: 9 / 25\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.687\tValidation Loss: 0.681\tTime: 13.685479402542114\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.679\tValidation Loss: 0.672\tTime: 13.703578233718872\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.671\tValidation Loss: 0.666\tTime: 13.246955394744873\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.662\tValidation Loss: 0.656\tTime: 13.169432163238525\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.652\tValidation Loss: 0.647\tTime: 13.114737749099731\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.639\tValidation Loss: 0.635\tTime: 13.00778317451477\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.619\tValidation Loss: 0.620\tTime: 13.204119443893433\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.601\tValidation Loss: 0.604\tTime: 13.086272716522217\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.576\tValidation Loss: 0.584\tTime: 12.877650499343872\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.550\tValidation Loss: 0.559\tTime: 12.831454753875732\n",
            "Saving model 9\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 10 / 25\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.692\tValidation Loss: 0.688\tTime: 8.873637437820435\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.687\tValidation Loss: 0.685\tTime: 8.762260675430298\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.685\tValidation Loss: 0.683\tTime: 9.289421558380127\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.683\tValidation Loss: 0.679\tTime: 8.99450135231018\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.680\tValidation Loss: 0.677\tTime: 8.902461528778076\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.680\tValidation Loss: 0.674\tTime: 9.013597011566162\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.677\tValidation Loss: 0.672\tTime: 8.718168020248413\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.672\tValidation Loss: 0.671\tTime: 8.846110820770264\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.672\tValidation Loss: 0.670\tTime: 9.034307718276978\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.669\tValidation Loss: 0.667\tTime: 8.910035848617554\n",
            "Saving model 10\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 11 / 25\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.693\tValidation Loss: 0.685\tTime: 21.906705379486084\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.686\tValidation Loss: 0.682\tTime: 21.169379949569702\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.683\tValidation Loss: 0.678\tTime: 21.071311235427856\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.684\tValidation Loss: 0.676\tTime: 21.098023891448975\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.681\tValidation Loss: 0.676\tTime: 17.176130533218384\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.677\tValidation Loss: 0.669\tTime: 20.75090789794922\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.676\tValidation Loss: 0.671\tTime: 16.958154678344727\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.671\tValidation Loss: 0.668\tTime: 21.154996633529663\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.668\tValidation Loss: 0.662\tTime: 22.036933660507202\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.667\tValidation Loss: 0.662\tTime: 21.72312641143799\n",
            "Saving model 11\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 12 / 25\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.679\tValidation Loss: 0.658\tTime: 7.377567529678345\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.648\tValidation Loss: 0.633\tTime: 7.04286527633667\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.622\tValidation Loss: 0.602\tTime: 7.017868280410767\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.600\tValidation Loss: 0.581\tTime: 6.920946359634399\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.575\tValidation Loss: 0.560\tTime: 6.973083734512329\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.554\tValidation Loss: 0.543\tTime: 6.957084894180298\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.537\tValidation Loss: 0.518\tTime: 6.964994430541992\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.521\tValidation Loss: 0.501\tTime: 6.774717569351196\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.503\tValidation Loss: 0.491\tTime: 6.853826999664307\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.490\tValidation Loss: 0.475\tTime: 6.648522853851318\n",
            "Saving model 12\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment: 13 / 25\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.689\tValidation Loss: 0.687\tTime: 7.067406177520752\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.680\tValidation Loss: 0.677\tTime: 6.862482309341431\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.671\tValidation Loss: 0.667\tTime: 6.849817276000977\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.656\tValidation Loss: 0.649\tTime: 6.9590654373168945\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.636\tValidation Loss: 0.629\tTime: 6.875838756561279\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.610\tValidation Loss: 0.600\tTime: 6.9843833446502686\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.575\tValidation Loss: 0.564\tTime: 6.932060480117798\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.538\tValidation Loss: 0.525\tTime: 6.850636720657349\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.492\tValidation Loss: 0.481\tTime: 7.036299705505371\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.451\tValidation Loss: 0.438\tTime: 6.832125186920166\n",
            "Saving model 13\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment: 14 / 25\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.686\tValidation Loss: 0.678\tTime: 18.983150243759155\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.672\tValidation Loss: 0.667\tTime: 19.536311149597168\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.652\tValidation Loss: 0.645\tTime: 19.454381942749023\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.632\tValidation Loss: 0.625\tTime: 19.50289750099182\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.602\tValidation Loss: 0.610\tTime: 19.618192195892334\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.568\tValidation Loss: 0.573\tTime: 19.61993098258972\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.529\tValidation Loss: 0.525\tTime: 20.100154876708984\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.482\tValidation Loss: 0.478\tTime: 19.565126419067383\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.433\tValidation Loss: 0.433\tTime: 19.24372887611389\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.398\tValidation Loss: 0.406\tTime: 19.340168952941895\n",
            "Saving model 14\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 15 / 25\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.690\tValidation Loss: 0.685\tTime: 12.316566944122314\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.684\tValidation Loss: 0.683\tTime: 12.091970205307007\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.679\tValidation Loss: 0.675\tTime: 12.093870878219604\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.675\tValidation Loss: 0.671\tTime: 12.004104137420654\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.671\tValidation Loss: 0.671\tTime: 12.057994604110718\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.672\tValidation Loss: 0.664\tTime: 12.647785902023315\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.662\tValidation Loss: 0.660\tTime: 12.656339406967163\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.665\tValidation Loss: 0.657\tTime: 12.503082036972046\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.656\tValidation Loss: 0.655\tTime: 12.16567349433899\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.651\tValidation Loss: 0.650\tTime: 12.535141944885254\n",
            "Saving model 15\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 16 / 25\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.686\tValidation Loss: 0.679\tTime: 31.577954053878784\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.679\tValidation Loss: 0.669\tTime: 30.4199697971344\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.669\tValidation Loss: 0.656\tTime: 29.02452039718628\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.666\tValidation Loss: 0.649\tTime: 28.794909954071045\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.659\tValidation Loss: 0.644\tTime: 28.76975655555725\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.646\tValidation Loss: 0.639\tTime: 28.242079496383667\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.641\tValidation Loss: 0.635\tTime: 30.179387092590332\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.632\tValidation Loss: 0.619\tTime: 29.324082612991333\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.627\tValidation Loss: 0.608\tTime: 31.836010932922363\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.624\tValidation Loss: 0.614\tTime: 26.846079111099243\n",
            "Saving model 16\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 17 / 25\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.670\tValidation Loss: 0.630\tTime: 10.927802085876465\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.613\tValidation Loss: 0.580\tTime: 10.300832271575928\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.571\tValidation Loss: 0.535\tTime: 10.345612525939941\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.526\tValidation Loss: 0.493\tTime: 10.29512333869934\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.497\tValidation Loss: 0.462\tTime: 10.25535535812378\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.470\tValidation Loss: 0.436\tTime: 10.265230178833008\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.439\tValidation Loss: 0.408\tTime: 10.176056861877441\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.419\tValidation Loss: 0.381\tTime: 10.093882322311401\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.393\tValidation Loss: 0.360\tTime: 10.155921936035156\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.381\tValidation Loss: 0.343\tTime: 10.14994192123413\n",
            "Saving model 17\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment: 18 / 25\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.685\tValidation Loss: 0.679\tTime: 10.7483549118042\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.671\tValidation Loss: 0.663\tTime: 10.757991552352905\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.651\tValidation Loss: 0.640\tTime: 10.783648014068604\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.622\tValidation Loss: 0.609\tTime: 10.659559726715088\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.581\tValidation Loss: 0.567\tTime: 10.684406757354736\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.533\tValidation Loss: 0.514\tTime: 10.659041404724121\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.477\tValidation Loss: 0.454\tTime: 10.9629967212677\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.415\tValidation Loss: 0.396\tTime: 10.914480686187744\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.354\tValidation Loss: 0.337\tTime: 10.791808128356934\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.309\tValidation Loss: 0.292\tTime: 10.761107206344604\n",
            "Saving model 18\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment: 19 / 25\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.663\tValidation Loss: 0.626\tTime: 30.084234952926636\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.621\tValidation Loss: 0.594\tTime: 29.66199564933777\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.586\tValidation Loss: 0.564\tTime: 27.67921495437622\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.540\tValidation Loss: 0.507\tTime: 26.662452936172485\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.486\tValidation Loss: 0.456\tTime: 29.752474546432495\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.426\tValidation Loss: 0.395\tTime: 30.687278747558594\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.370\tValidation Loss: 0.344\tTime: 29.911648750305176\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.314\tValidation Loss: 0.293\tTime: 29.34770393371582\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.275\tValidation Loss: 0.255\tTime: 28.913852214813232\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.240\tValidation Loss: 0.226\tTime: 26.33548402786255\n",
            "Saving model 19\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 20 / 25\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.690\tValidation Loss: 0.686\tTime: 19.573236227035522\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.686\tValidation Loss: 0.679\tTime: 19.3085880279541\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.681\tValidation Loss: 0.675\tTime: 18.838541984558105\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.672\tValidation Loss: 0.678\tTime: 16.52483367919922\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.668\tValidation Loss: 0.669\tTime: 19.536864042282104\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.667\tValidation Loss: 0.662\tTime: 19.98938012123108\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.664\tValidation Loss: 0.657\tTime: 20.356183767318726\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.658\tValidation Loss: 0.657\tTime: 20.105310678482056\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.655\tValidation Loss: 0.655\tTime: 20.35958981513977\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.654\tValidation Loss: 0.647\tTime: 19.448338747024536\n",
            "Saving model 20\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 21 / 25\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.689\tValidation Loss: 0.678\tTime: 45.77261829376221\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.680\tValidation Loss: 0.667\tTime: 46.187854528427124\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.674\tValidation Loss: 0.660\tTime: 46.274282455444336\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.663\tValidation Loss: 0.655\tTime: 46.58293437957764\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.656\tValidation Loss: 0.645\tTime: 46.373814821243286\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.650\tValidation Loss: 0.634\tTime: 46.2965292930603\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.648\tValidation Loss: 0.629\tTime: 46.44309854507446\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.642\tValidation Loss: 0.624\tTime: 46.59806036949158\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.634\tValidation Loss: 0.622\tTime: 46.54117679595947\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.627\tValidation Loss: 0.609\tTime: 46.815603256225586\n",
            "Saving model 21\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 22 / 25\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.653\tValidation Loss: 0.613\tTime: 17.986011505126953\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.600\tValidation Loss: 0.559\tTime: 17.48528242111206\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.553\tValidation Loss: 0.518\tTime: 17.67738914489746\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.518\tValidation Loss: 0.479\tTime: 17.168651342391968\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.488\tValidation Loss: 0.449\tTime: 16.705912113189697\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.447\tValidation Loss: 0.423\tTime: 16.76060652732849\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.426\tValidation Loss: 0.386\tTime: 17.848239421844482\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.406\tValidation Loss: 0.377\tTime: 17.94381284713745\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.390\tValidation Loss: 0.348\tTime: 17.726368188858032\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.374\tValidation Loss: 0.332\tTime: 17.941875457763672\n",
            "Saving model 22\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment: 23 / 25\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.685\tValidation Loss: 0.680\tTime: 18.374305963516235\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.672\tValidation Loss: 0.664\tTime: 18.43141794204712\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.651\tValidation Loss: 0.641\tTime: 17.942481994628906\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.620\tValidation Loss: 0.608\tTime: 18.005889892578125\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.581\tValidation Loss: 0.566\tTime: 18.15661096572876\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.538\tValidation Loss: 0.514\tTime: 18.043136596679688\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.467\tValidation Loss: 0.450\tTime: 17.896178483963013\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.411\tValidation Loss: 0.397\tTime: 17.95550799369812\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.358\tValidation Loss: 0.335\tTime: 16.692752838134766\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.309\tValidation Loss: 0.288\tTime: 18.2028591632843\n",
            "Saving model 23\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment: 24 / 25\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.664\tValidation Loss: 0.625\tTime: 44.421804904937744\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.622\tValidation Loss: 0.589\tTime: 44.94846177101135\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.581\tValidation Loss: 0.557\tTime: 44.80114221572876\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.532\tValidation Loss: 0.506\tTime: 44.691988945007324\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.480\tValidation Loss: 0.455\tTime: 44.83813238143921\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.418\tValidation Loss: 0.389\tTime: 45.17899203300476\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.363\tValidation Loss: 0.331\tTime: 44.95526123046875\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.309\tValidation Loss: 0.286\tTime: 44.99066734313965\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.275\tValidation Loss: 0.244\tTime: 44.976181983947754\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.239\tValidation Loss: 0.211\tTime: 44.82421898841858\n",
            "Saving model 24\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n"
          ]
        }
      ],
      "source": [
        "max_lengths = [5, 15, 25, 50, 100]\n",
        "batch_size = 32\n",
        "optimizer_type = torch.optim.AdamW\n",
        "lr = 0.00001\n",
        "exp_title = 'test_len'\n",
        "# xlm-roberta-base xlm-roberta-large bert-base-uncased roberta-base roberta-large\n",
        "pretrained_model_names = ['xlm-roberta-base', 'xlm-roberta-large', 'bert-base-uncased', 'roberta-base', 'roberta-large']\n",
        "loss_func_type = nn.NLLLoss\n",
        "padding = 'max_length'\n",
        "truncation=True\n",
        "model_type = Ext_Arch\n",
        "epochs=10\n",
        "\n",
        "exp_index = 0\n",
        "total_exp = len(max_lengths) * len(pretrained_model_names)\n",
        "for max_length in max_lengths:\n",
        "    for pretrained_model_name in pretrained_model_names:\n",
        "        model = get_model(pretrained_model_name, model_type, device)\n",
        "        train_dataloader, val_dataloader, test_data, weights = tokenize_data(pretrained_model_name, max_length, padding, truncation, batch_size, device)\n",
        "        loss_func  = loss_func_type(weight=weights)\n",
        "\n",
        "        print(f'Experiment: {exp_index} / {total_exp}')\n",
        "        optimizer = optimizer_type(model.parameters(), lr=lr)\n",
        "        do_exp(model, train_dataloader, val_dataloader, test_data, device, loss_func, optimizer, epochs, lr, \n",
        "            batch_size, max_length, padding, truncation, pretrained_model_name, exp_title, exp_index, root)\n",
        "        exp_index += 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Test extenstions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment: 0 / 15\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.694\tValidation Loss: 0.691\tTime: 7.146329879760742\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.692\tValidation Loss: 0.690\tTime: 6.948671579360962\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.691\tValidation Loss: 0.689\tTime: 6.587079286575317\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.689\tValidation Loss: 0.689\tTime: 7.3470189571380615\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.690\tValidation Loss: 0.688\tTime: 7.274915456771851\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.689\tValidation Loss: 0.688\tTime: 5.560637712478638\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.687\tValidation Loss: 0.687\tTime: 7.7297279834747314\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.687\tValidation Loss: 0.686\tTime: 8.189168214797974\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.687\tValidation Loss: 0.685\tTime: 8.198838472366333\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.685\tValidation Loss: 0.685\tTime: 8.248032093048096\n",
            "Saving model 0\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 1 / 15\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.691\tValidation Loss: 0.688\tTime: 22.840914964675903\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.692\tValidation Loss: 0.689\tTime: 18.599158763885498\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.691\tValidation Loss: 0.689\tTime: 17.78545618057251\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.693\tValidation Loss: 0.689\tTime: 16.777297735214233\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.690\tValidation Loss: 0.688\tTime: 20.23925518989563\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.688\tValidation Loss: 0.686\tTime: 21.215460062026978\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.688\tValidation Loss: 0.685\tTime: 20.321398735046387\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.688\tValidation Loss: 0.685\tTime: 19.81214952468872\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.687\tValidation Loss: 0.685\tTime: 20.007126092910767\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.686\tValidation Loss: 0.684\tTime: 16.641146898269653\n",
            "Saving model 1\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 2 / 15\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.690\tValidation Loss: 0.684\tTime: 6.286014080047607\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.682\tValidation Loss: 0.679\tTime: 6.020208835601807\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.676\tValidation Loss: 0.673\tTime: 5.960763931274414\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.670\tValidation Loss: 0.667\tTime: 5.813831090927124\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.662\tValidation Loss: 0.658\tTime: 5.80414605140686\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.655\tValidation Loss: 0.653\tTime: 5.831721544265747\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.650\tValidation Loss: 0.646\tTime: 5.822268962860107\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.644\tValidation Loss: 0.641\tTime: 5.825105667114258\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.637\tValidation Loss: 0.634\tTime: 6.08603572845459\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.631\tValidation Loss: 0.628\tTime: 6.037840843200684\n",
            "Saving model 2\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment: 3 / 15\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.690\tValidation Loss: 0.681\tTime: 7.660330772399902\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.673\tValidation Loss: 0.667\tTime: 7.493099927902222\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.659\tValidation Loss: 0.653\tTime: 6.088603496551514\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.644\tValidation Loss: 0.639\tTime: 7.345668077468872\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.627\tValidation Loss: 0.625\tTime: 7.267766714096069\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.613\tValidation Loss: 0.610\tTime: 7.353262424468994\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.599\tValidation Loss: 0.595\tTime: 7.369932413101196\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.577\tValidation Loss: 0.578\tTime: 7.754302263259888\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.561\tValidation Loss: 0.561\tTime: 6.139571189880371\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.544\tValidation Loss: 0.543\tTime: 5.535022258758545\n",
            "Saving model 3\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment: 4 / 15\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.683\tValidation Loss: 0.678\tTime: 15.699393033981323\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.657\tValidation Loss: 0.655\tTime: 16.481475591659546\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.631\tValidation Loss: 0.634\tTime: 16.879501819610596\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.606\tValidation Loss: 0.610\tTime: 15.489556312561035\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.581\tValidation Loss: 0.588\tTime: 16.078702449798584\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.562\tValidation Loss: 0.570\tTime: 14.971506118774414\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.540\tValidation Loss: 0.552\tTime: 16.06210207939148\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.511\tValidation Loss: 0.526\tTime: 14.733539581298828\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.486\tValidation Loss: 0.502\tTime: 14.384119749069214\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.469\tValidation Loss: 0.480\tTime: 14.449822425842285\n",
            "Saving model 4\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 5 / 15\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.693\tValidation Loss: 0.686\tTime: 6.519674062728882\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.689\tValidation Loss: 0.684\tTime: 6.573986053466797\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.686\tValidation Loss: 0.681\tTime: 6.5227460861206055\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.682\tValidation Loss: 0.680\tTime: 6.643600702285767\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.681\tValidation Loss: 0.679\tTime: 6.946926593780518\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.677\tValidation Loss: 0.677\tTime: 6.959816217422485\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.674\tValidation Loss: 0.672\tTime: 7.355088233947754\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.676\tValidation Loss: 0.670\tTime: 8.651425123214722\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.670\tValidation Loss: 0.666\tTime: 8.581621646881104\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.670\tValidation Loss: 0.667\tTime: 6.021705389022827\n",
            "Saving model 5\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 6 / 15\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.693\tValidation Loss: 0.692\tTime: 20.73176598548889\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.689\tValidation Loss: 0.684\tTime: 21.32468605041504\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.686\tValidation Loss: 0.683\tTime: 18.834872007369995\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.682\tValidation Loss: 0.678\tTime: 16.901832580566406\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.682\tValidation Loss: 0.678\tTime: 16.06432604789734\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.679\tValidation Loss: 0.673\tTime: 15.909268617630005\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.675\tValidation Loss: 0.670\tTime: 16.553620100021362\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.671\tValidation Loss: 0.670\tTime: 16.100958108901978\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.671\tValidation Loss: 0.667\tTime: 16.42398476600647\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.667\tValidation Loss: 0.663\tTime: 16.41805672645569\n",
            "Saving model 6\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 7 / 15\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.679\tValidation Loss: 0.664\tTime: 5.743050575256348\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.647\tValidation Loss: 0.626\tTime: 5.370124578475952\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.620\tValidation Loss: 0.601\tTime: 5.363969326019287\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.595\tValidation Loss: 0.579\tTime: 5.364017009735107\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.575\tValidation Loss: 0.554\tTime: 5.352391004562378\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.548\tValidation Loss: 0.532\tTime: 5.335633039474487\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.534\tValidation Loss: 0.516\tTime: 5.311072111129761\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.512\tValidation Loss: 0.497\tTime: 5.7577292919158936\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.498\tValidation Loss: 0.478\tTime: 5.544986724853516\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.479\tValidation Loss: 0.470\tTime: 5.88642144203186\n",
            "Saving model 7\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment: 8 / 15\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.689\tValidation Loss: 0.683\tTime: 6.221113920211792\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.680\tValidation Loss: 0.676\tTime: 6.012844800949097\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.668\tValidation Loss: 0.662\tTime: 5.520682334899902\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.654\tValidation Loss: 0.648\tTime: 5.5666663646698\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.634\tValidation Loss: 0.624\tTime: 5.52692437171936\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.604\tValidation Loss: 0.596\tTime: 5.63297963142395\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.571\tValidation Loss: 0.559\tTime: 5.678534507751465\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.528\tValidation Loss: 0.517\tTime: 5.7525670528411865\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.489\tValidation Loss: 0.478\tTime: 5.500933408737183\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.444\tValidation Loss: 0.435\tTime: 5.576076984405518\n",
            "Saving model 8\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment: 9 / 15\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.684\tValidation Loss: 0.673\tTime: 16.060173511505127\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.670\tValidation Loss: 0.662\tTime: 15.271193981170654\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.652\tValidation Loss: 0.644\tTime: 14.70195722579956\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.627\tValidation Loss: 0.622\tTime: 14.954644918441772\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.598\tValidation Loss: 0.594\tTime: 15.629018306732178\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.564\tValidation Loss: 0.568\tTime: 14.741755962371826\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.526\tValidation Loss: 0.521\tTime: 14.7472562789917\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.471\tValidation Loss: 0.478\tTime: 15.270419836044312\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.431\tValidation Loss: 0.429\tTime: 14.54946756362915\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.391\tValidation Loss: 0.390\tTime: 15.007858991622925\n",
            "Saving model 9\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 10 / 15\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.692\tValidation Loss: 0.688\tTime: 7.086605072021484\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.686\tValidation Loss: 0.686\tTime: 6.727604150772095\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.686\tValidation Loss: 0.682\tTime: 7.1646904945373535\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.686\tValidation Loss: 0.681\tTime: 6.7006001472473145\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.681\tValidation Loss: 0.679\tTime: 7.404717445373535\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.684\tValidation Loss: 0.676\tTime: 7.193736553192139\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.683\tValidation Loss: 0.675\tTime: 6.782495975494385\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.676\tValidation Loss: 0.672\tTime: 6.501909255981445\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.674\tValidation Loss: 0.669\tTime: 6.418481826782227\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.671\tValidation Loss: 0.667\tTime: 6.600303649902344\n",
            "Saving model 10\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 11 / 15\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.692\tValidation Loss: 0.690\tTime: 17.024182081222534\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.689\tValidation Loss: 0.686\tTime: 16.224673748016357\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.690\tValidation Loss: 0.686\tTime: 16.614535331726074\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.690\tValidation Loss: 0.683\tTime: 15.9786696434021\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.686\tValidation Loss: 0.682\tTime: 16.62297034263611\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.684\tValidation Loss: 0.682\tTime: 14.316040992736816\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.681\tValidation Loss: 0.677\tTime: 15.653059244155884\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.683\tValidation Loss: 0.676\tTime: 16.238927602767944\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.680\tValidation Loss: 0.674\tTime: 17.688892126083374\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.678\tValidation Loss: 0.672\tTime: 17.014779567718506\n",
            "Saving model 11\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 12 / 15\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.684\tValidation Loss: 0.674\tTime: 5.873449325561523\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.665\tValidation Loss: 0.655\tTime: 5.448086977005005\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.642\tValidation Loss: 0.622\tTime: 5.937782287597656\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.615\tValidation Loss: 0.590\tTime: 5.6598217487335205\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.582\tValidation Loss: 0.553\tTime: 5.4382336139678955\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.554\tValidation Loss: 0.523\tTime: 5.504979133605957\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.513\tValidation Loss: 0.489\tTime: 5.79761815071106\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.490\tValidation Loss: 0.459\tTime: 5.410641670227051\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.463\tValidation Loss: 0.435\tTime: 5.4542810916900635\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.433\tValidation Loss: 0.412\tTime: 5.358037233352661\n",
            "Saving model 12\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment: 13 / 15\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.694\tValidation Loss: 0.689\tTime: 5.663397312164307\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.687\tValidation Loss: 0.684\tTime: 5.526149034500122\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.681\tValidation Loss: 0.675\tTime: 5.5631422996521\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.668\tValidation Loss: 0.661\tTime: 5.674723148345947\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.647\tValidation Loss: 0.634\tTime: 5.579456806182861\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.609\tValidation Loss: 0.587\tTime: 5.520774602890015\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.552\tValidation Loss: 0.522\tTime: 5.680488586425781\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.479\tValidation Loss: 0.446\tTime: 5.699760913848877\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.401\tValidation Loss: 0.369\tTime: 5.552675008773804\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.338\tValidation Loss: 0.311\tTime: 5.551886558532715\n",
            "Saving model 13\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment: 14 / 15\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.690\tValidation Loss: 0.684\tTime: 17.083261966705322\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.683\tValidation Loss: 0.679\tTime: 15.820450782775879\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.673\tValidation Loss: 0.668\tTime: 15.93280553817749\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.655\tValidation Loss: 0.643\tTime: 16.297068119049072\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.628\tValidation Loss: 0.611\tTime: 16.055872440338135\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.585\tValidation Loss: 0.581\tTime: 15.3163583278656\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.533\tValidation Loss: 0.515\tTime: 14.880113124847412\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.474\tValidation Loss: 0.460\tTime: 15.416043758392334\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.408\tValidation Loss: 0.400\tTime: 15.420050859451294\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.354\tValidation Loss: 0.350\tTime: 16.079720735549927\n",
            "Saving model 14\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n"
          ]
        }
      ],
      "source": [
        "model_types = [Small_Ext_Arch, Ext_Arch, Big_Ext_Arch]\n",
        "batch_size = 32\n",
        "optimizer_type = torch.optim.AdamW\n",
        "lr = 0.00001\n",
        "exp_title = 'test_ext3'\n",
        "# xlm-roberta-base xlm-roberta-large bert-base-uncased roberta-base roberta-large\n",
        "pretrained_model_names = ['xlm-roberta-base', 'xlm-roberta-large', 'bert-base-uncased', 'roberta-base', 'roberta-large']\n",
        "loss_func_type = nn.NLLLoss\n",
        "max_length = 25 \n",
        "padding = 'max_length'\n",
        "truncation=True\n",
        "epochs=10\n",
        "\n",
        "exp_index = 0\n",
        "total_exp = len(model_types) * len(pretrained_model_names)\n",
        "for model_type in model_types:\n",
        "    for pretrained_model_name in pretrained_model_names:\n",
        "        model = get_model(pretrained_model_name, model_type, device)\n",
        "        train_dataloader, val_dataloader, test_data, weights = tokenize_data(pretrained_model_name, max_length, padding, truncation, batch_size, device)\n",
        "        loss_func  = loss_func_type(weight=weights)\n",
        "\n",
        "        print(f'Experiment: {exp_index} / {total_exp}')\n",
        "        optimizer = optimizer_type(model.parameters(), lr=lr)\n",
        "        do_exp(model, train_dataloader, val_dataloader, test_data, device, loss_func, optimizer, epochs, lr, \n",
        "            batch_size, max_length, padding, truncation, pretrained_model_name, exp_title, exp_index, root)\n",
        "        exp_index += 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGDFq20A76jL"
      },
      "source": [
        "# Test optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        },
        "id": "9G32bY6X8pCY",
        "outputId": "e30e1ddd-fb2c-438e-e7a8-67fb815d71ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment: 0 / 30\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.690\tValidation Loss: 0.687\tTime: 8.886741399765015\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.689\tValidation Loss: 0.684\tTime: 8.903512716293335\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.685\tValidation Loss: 0.683\tTime: 9.18180537223816\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.682\tValidation Loss: 0.683\tTime: 6.307824611663818\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.678\tValidation Loss: 0.676\tTime: 8.876282930374146\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.680\tValidation Loss: 0.679\tTime: 6.232004165649414\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.674\tValidation Loss: 0.672\tTime: 8.552675247192383\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.675\tValidation Loss: 0.673\tTime: 6.162191390991211\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.672\tValidation Loss: 0.669\tTime: 8.899298906326294\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.671\tValidation Loss: 0.668\tTime: 8.789936542510986\n",
            "Saving model 0\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 1 / 30\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.691\tValidation Loss: 0.685\tTime: 21.54394817352295\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.687\tValidation Loss: 0.683\tTime: 22.056220293045044\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.682\tValidation Loss: 0.679\tTime: 21.028247833251953\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.682\tValidation Loss: 0.677\tTime: 22.047306060791016\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.679\tValidation Loss: 0.674\tTime: 22.224452018737793\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.677\tValidation Loss: 0.672\tTime: 22.12669801712036\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.673\tValidation Loss: 0.670\tTime: 22.21597695350647\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.668\tValidation Loss: 0.667\tTime: 21.820892333984375\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.671\tValidation Loss: 0.665\tTime: 21.674930334091187\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.667\tValidation Loss: 0.665\tTime: 17.115543127059937\n",
            "Saving model 1\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 2 / 30\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.670\tValidation Loss: 0.647\tTime: 7.124663352966309\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.639\tValidation Loss: 0.624\tTime: 6.652806043624878\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.614\tValidation Loss: 0.595\tTime: 6.680616617202759\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.596\tValidation Loss: 0.580\tTime: 6.667170763015747\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.569\tValidation Loss: 0.556\tTime: 6.61479926109314\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.549\tValidation Loss: 0.536\tTime: 6.74151086807251\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.534\tValidation Loss: 0.520\tTime: 6.725834369659424\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.515\tValidation Loss: 0.496\tTime: 6.478112459182739\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.501\tValidation Loss: 0.481\tTime: 6.608818769454956\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.479\tValidation Loss: 0.468\tTime: 6.389398813247681\n",
            "Saving model 2\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment: 3 / 30\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.688\tValidation Loss: 0.683\tTime: 6.86081337928772\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.680\tValidation Loss: 0.674\tTime: 6.721158266067505\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.669\tValidation Loss: 0.664\tTime: 6.866678237915039\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.656\tValidation Loss: 0.650\tTime: 7.36488938331604\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.637\tValidation Loss: 0.628\tTime: 7.184107542037964\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.611\tValidation Loss: 0.601\tTime: 7.191483497619629\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.578\tValidation Loss: 0.567\tTime: 7.343358039855957\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.539\tValidation Loss: 0.528\tTime: 7.01052713394165\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.497\tValidation Loss: 0.485\tTime: 7.058755874633789\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.455\tValidation Loss: 0.440\tTime: 7.1811254024505615\n",
            "Saving model 3\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment: 4 / 30\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.690\tValidation Loss: 0.683\tTime: 19.742704153060913\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.673\tValidation Loss: 0.681\tTime: 19.845255851745605\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.657\tValidation Loss: 0.650\tTime: 19.615994930267334\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.631\tValidation Loss: 0.623\tTime: 19.21827459335327\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.602\tValidation Loss: 0.592\tTime: 19.373279094696045\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.561\tValidation Loss: 0.555\tTime: 19.381141901016235\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.522\tValidation Loss: 0.514\tTime: 19.246169090270996\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.478\tValidation Loss: 0.480\tTime: 19.175909757614136\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.428\tValidation Loss: 0.428\tTime: 19.026747941970825\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.389\tValidation Loss: 0.398\tTime: 19.577187299728394\n",
            "Saving model 4\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 5 / 30\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.692\tValidation Loss: 0.689\tTime: 8.660744667053223\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.692\tValidation Loss: 0.689\tTime: 8.889979362487793\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.691\tValidation Loss: 0.687\tTime: 9.254297733306885\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.689\tValidation Loss: 0.685\tTime: 9.072462320327759\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.687\tValidation Loss: 0.686\tTime: 6.588518381118774\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.684\tValidation Loss: 0.682\tTime: 9.203043460845947\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.685\tValidation Loss: 0.682\tTime: 9.39194369316101\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.683\tValidation Loss: 0.679\tTime: 9.139965534210205\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.681\tValidation Loss: 0.678\tTime: 9.307469844818115\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.679\tValidation Loss: 0.677\tTime: 9.070831775665283\n",
            "Saving model 5\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 6 / 30\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.698\tValidation Loss: 0.694\tTime: 21.502281427383423\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.691\tValidation Loss: 0.688\tTime: 21.453293561935425\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.688\tValidation Loss: 0.686\tTime: 21.391685009002686\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.689\tValidation Loss: 0.685\tTime: 21.310009241104126\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.687\tValidation Loss: 0.682\tTime: 21.290555953979492\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.683\tValidation Loss: 0.681\tTime: 21.483662128448486\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.686\tValidation Loss: 0.679\tTime: 21.827957153320312\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.682\tValidation Loss: 0.677\tTime: 21.96551537513733\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.682\tValidation Loss: 0.676\tTime: 21.929251432418823\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.679\tValidation Loss: 0.675\tTime: 22.123238563537598\n",
            "Saving model 6\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 7 / 30\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.687\tValidation Loss: 0.683\tTime: 7.4970786571502686\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.678\tValidation Loss: 0.670\tTime: 6.987112283706665\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.665\tValidation Loss: 0.656\tTime: 6.817616701126099\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.650\tValidation Loss: 0.641\tTime: 6.8853840827941895\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.635\tValidation Loss: 0.625\tTime: 6.739208698272705\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.621\tValidation Loss: 0.610\tTime: 6.6640400886535645\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.606\tValidation Loss: 0.595\tTime: 6.859172344207764\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.588\tValidation Loss: 0.581\tTime: 6.711612939834595\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.575\tValidation Loss: 0.561\tTime: 6.609407186508179\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.561\tValidation Loss: 0.547\tTime: 6.972998142242432\n",
            "Saving model 7\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment: 8 / 30\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.694\tValidation Loss: 0.692\tTime: 7.0318145751953125\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.690\tValidation Loss: 0.688\tTime: 7.01957631111145\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.686\tValidation Loss: 0.684\tTime: 6.909364700317383\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.683\tValidation Loss: 0.680\tTime: 6.794003248214722\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.679\tValidation Loss: 0.674\tTime: 6.791181564331055\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.672\tValidation Loss: 0.670\tTime: 6.776438474655151\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.664\tValidation Loss: 0.661\tTime: 6.724822044372559\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.655\tValidation Loss: 0.651\tTime: 7.126960039138794\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.644\tValidation Loss: 0.638\tTime: 7.084523916244507\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.626\tValidation Loss: 0.621\tTime: 7.193096399307251\n",
            "Saving model 8\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment: 9 / 30\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.692\tValidation Loss: 0.689\tTime: 19.431975603103638\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.687\tValidation Loss: 0.684\tTime: 19.331300258636475\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.683\tValidation Loss: 0.680\tTime: 19.68441128730774\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.676\tValidation Loss: 0.671\tTime: 19.558937788009644\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.669\tValidation Loss: 0.665\tTime: 19.61746859550476\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.657\tValidation Loss: 0.657\tTime: 19.401849269866943\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.648\tValidation Loss: 0.646\tTime: 19.140502452850342\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.633\tValidation Loss: 0.633\tTime: 19.25679349899292\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.612\tValidation Loss: 0.612\tTime: 19.259310483932495\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.587\tValidation Loss: 0.591\tTime: 19.086148023605347\n",
            "Saving model 9\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 10 / 30\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.691\tValidation Loss: 0.689\tTime: 8.2749764919281\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.688\tValidation Loss: 0.682\tTime: 8.56799030303955\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.687\tValidation Loss: 0.680\tTime: 8.96423864364624\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.682\tValidation Loss: 0.680\tTime: 8.877411842346191\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.681\tValidation Loss: 0.678\tTime: 9.167542695999146\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.679\tValidation Loss: 0.675\tTime: 8.931710958480835\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.676\tValidation Loss: 0.671\tTime: 9.203707218170166\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.676\tValidation Loss: 0.670\tTime: 8.856110334396362\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.674\tValidation Loss: 0.669\tTime: 9.142606258392334\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.669\tValidation Loss: 0.666\tTime: 9.19976258277893\n",
            "Saving model 10\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 11 / 30\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.692\tValidation Loss: 0.687\tTime: 21.87855100631714\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.688\tValidation Loss: 0.685\tTime: 22.112922191619873\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.684\tValidation Loss: 0.679\tTime: 21.90340566635132\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.684\tValidation Loss: 0.677\tTime: 21.848660945892334\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.679\tValidation Loss: 0.674\tTime: 21.663354873657227\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.678\tValidation Loss: 0.671\tTime: 21.29695200920105\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.675\tValidation Loss: 0.669\tTime: 20.952786922454834\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.671\tValidation Loss: 0.667\tTime: 22.435073852539062\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.670\tValidation Loss: 0.664\tTime: 22.053611755371094\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.665\tValidation Loss: 0.661\tTime: 22.026111125946045\n",
            "Saving model 11\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 12 / 30\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.679\tValidation Loss: 0.661\tTime: 7.5084545612335205\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.645\tValidation Loss: 0.629\tTime: 7.07332181930542\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.620\tValidation Loss: 0.602\tTime: 6.918066501617432\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.593\tValidation Loss: 0.581\tTime: 6.754667282104492\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.572\tValidation Loss: 0.554\tTime: 6.970634937286377\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.551\tValidation Loss: 0.535\tTime: 6.819567441940308\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.530\tValidation Loss: 0.516\tTime: 6.9034929275512695\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.512\tValidation Loss: 0.495\tTime: 6.953694581985474\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.494\tValidation Loss: 0.481\tTime: 6.771181106567383\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.479\tValidation Loss: 0.463\tTime: 6.600666522979736\n",
            "Saving model 12\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment: 13 / 30\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.690\tValidation Loss: 0.683\tTime: 7.076603889465332\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.681\tValidation Loss: 0.675\tTime: 6.993559122085571\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.669\tValidation Loss: 0.666\tTime: 6.928287982940674\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.656\tValidation Loss: 0.650\tTime: 7.148546934127808\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.637\tValidation Loss: 0.629\tTime: 7.013213396072388\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.612\tValidation Loss: 0.601\tTime: 7.08056116104126\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.581\tValidation Loss: 0.569\tTime: 6.866837978363037\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.543\tValidation Loss: 0.530\tTime: 6.884841680526733\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.498\tValidation Loss: 0.483\tTime: 6.97974157333374\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.450\tValidation Loss: 0.436\tTime: 6.922553300857544\n",
            "Saving model 13\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment: 14 / 30\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.686\tValidation Loss: 0.683\tTime: 19.08712601661682\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.671\tValidation Loss: 0.664\tTime: 19.28941512107849\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.656\tValidation Loss: 0.650\tTime: 19.446338415145874\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.630\tValidation Loss: 0.624\tTime: 19.410478591918945\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.602\tValidation Loss: 0.595\tTime: 19.74178647994995\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.566\tValidation Loss: 0.564\tTime: 19.592108964920044\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.529\tValidation Loss: 0.521\tTime: 19.650972843170166\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.482\tValidation Loss: 0.477\tTime: 19.558578968048096\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.431\tValidation Loss: 0.433\tTime: 19.39925456047058\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.392\tValidation Loss: 0.396\tTime: 19.38790249824524\n",
            "Saving model 14\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 15 / 30\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.689\tValidation Loss: 0.685\tTime: 8.818409204483032\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.685\tValidation Loss: 0.689\tTime: 6.071134805679321\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.683\tValidation Loss: 0.681\tTime: 8.55595588684082\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.684\tValidation Loss: 0.678\tTime: 8.465004920959473\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.683\tValidation Loss: 0.678\tTime: 8.823824405670166\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.677\tValidation Loss: 0.673\tTime: 8.477484464645386\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.675\tValidation Loss: 0.671\tTime: 8.549644470214844\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.674\tValidation Loss: 0.677\tTime: 6.014814138412476\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.672\tValidation Loss: 0.674\tTime: 6.1699607372283936\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.670\tValidation Loss: 0.667\tTime: 9.410807847976685\n",
            "Saving model 15\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 16 / 30\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.692\tValidation Loss: 0.687\tTime: 22.174513578414917\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.689\tValidation Loss: 0.686\tTime: 22.49984622001648\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.686\tValidation Loss: 0.681\tTime: 22.022403717041016\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.681\tValidation Loss: 0.679\tTime: 22.14875817298889\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.680\tValidation Loss: 0.675\tTime: 21.420822143554688\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.677\tValidation Loss: 0.674\tTime: 21.424134016036987\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.676\tValidation Loss: 0.675\tTime: 17.390335083007812\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.672\tValidation Loss: 0.667\tTime: 21.05154800415039\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.669\tValidation Loss: 0.663\tTime: 21.622837781906128\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.670\tValidation Loss: 0.663\tTime: 21.00679326057434\n",
            "Saving model 16\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 17 / 30\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.677\tValidation Loss: 0.654\tTime: 7.397042512893677\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.643\tValidation Loss: 0.627\tTime: 7.017965078353882\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.618\tValidation Loss: 0.601\tTime: 6.868394613265991\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.592\tValidation Loss: 0.574\tTime: 7.022149562835693\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.567\tValidation Loss: 0.565\tTime: 6.9076855182647705\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.556\tValidation Loss: 0.532\tTime: 6.8176538944244385\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.535\tValidation Loss: 0.514\tTime: 6.725397348403931\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.525\tValidation Loss: 0.499\tTime: 6.779222249984741\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.500\tValidation Loss: 0.487\tTime: 7.0862791538238525\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.481\tValidation Loss: 0.465\tTime: 6.920663118362427\n",
            "Saving model 17\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment: 18 / 30\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.689\tValidation Loss: 0.685\tTime: 7.2161126136779785\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.681\tValidation Loss: 0.674\tTime: 7.128198862075806\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.667\tValidation Loss: 0.662\tTime: 7.159512519836426\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.652\tValidation Loss: 0.645\tTime: 7.181783676147461\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.629\tValidation Loss: 0.625\tTime: 7.15921688079834\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.606\tValidation Loss: 0.596\tTime: 7.013851642608643\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.573\tValidation Loss: 0.562\tTime: 7.061814069747925\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.535\tValidation Loss: 0.523\tTime: 7.137240648269653\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.490\tValidation Loss: 0.481\tTime: 6.965268611907959\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.448\tValidation Loss: 0.436\tTime: 6.9703593254089355\n",
            "Saving model 18\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment: 19 / 30\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.687\tValidation Loss: 0.680\tTime: 19.474644422531128\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.674\tValidation Loss: 0.663\tTime: 19.42271637916565\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.656\tValidation Loss: 0.647\tTime: 19.258745670318604\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.630\tValidation Loss: 0.632\tTime: 18.968650579452515\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.603\tValidation Loss: 0.599\tTime: 19.153011560440063\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.566\tValidation Loss: 0.566\tTime: 19.364484548568726\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.533\tValidation Loss: 0.525\tTime: 19.59523868560791\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.481\tValidation Loss: 0.487\tTime: 19.406338691711426\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.441\tValidation Loss: 0.437\tTime: 19.855223894119263\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.396\tValidation Loss: 0.395\tTime: 19.796831846237183\n",
            "Saving model 19\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 20 / 30\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.688\tValidation Loss: 0.682\tTime: 8.489060401916504\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.684\tValidation Loss: 0.681\tTime: 8.840106725692749\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.683\tValidation Loss: 0.678\tTime: 8.689374923706055\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.680\tValidation Loss: 0.675\tTime: 8.507813930511475\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.678\tValidation Loss: 0.679\tTime: 6.18670392036438\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.682\tValidation Loss: 0.673\tTime: 8.616422414779663\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.674\tValidation Loss: 0.670\tTime: 8.720003843307495\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.673\tValidation Loss: 0.670\tTime: 8.5767502784729\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.672\tValidation Loss: 0.668\tTime: 8.722817659378052\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.672\tValidation Loss: 0.665\tTime: 8.749654531478882\n",
            "Saving model 20\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 21 / 30\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.686\tValidation Loss: 0.682\tTime: 20.633270263671875\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.685\tValidation Loss: 0.690\tTime: 17.0772705078125\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.682\tValidation Loss: 0.680\tTime: 21.302461624145508\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.683\tValidation Loss: 0.678\tTime: 21.97935724258423\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.681\tValidation Loss: 0.675\tTime: 22.054893255233765\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.677\tValidation Loss: 0.671\tTime: 22.208449363708496\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.675\tValidation Loss: 0.670\tTime: 22.30331587791443\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.671\tValidation Loss: 0.668\tTime: 21.562274932861328\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.673\tValidation Loss: 0.666\tTime: 21.031184911727905\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.667\tValidation Loss: 0.663\tTime: 21.323428630828857\n",
            "Saving model 21\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 22 / 30\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.644\tValidation Loss: 0.626\tTime: 6.982387542724609\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.612\tValidation Loss: 0.594\tTime: 6.568299770355225\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.593\tValidation Loss: 0.577\tTime: 6.678285598754883\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.566\tValidation Loss: 0.549\tTime: 6.581034183502197\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.546\tValidation Loss: 0.547\tTime: 6.573323488235474\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.529\tValidation Loss: 0.512\tTime: 6.4540228843688965\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.517\tValidation Loss: 0.498\tTime: 6.422543287277222\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.505\tValidation Loss: 0.480\tTime: 6.6607115268707275\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.479\tValidation Loss: 0.464\tTime: 7.006305456161499\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.473\tValidation Loss: 0.456\tTime: 6.9671311378479\n",
            "Saving model 22\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment: 23 / 30\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.686\tValidation Loss: 0.678\tTime: 7.3284101486206055\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.673\tValidation Loss: 0.671\tTime: 7.281965255737305\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.662\tValidation Loss: 0.656\tTime: 7.234597206115723\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.644\tValidation Loss: 0.641\tTime: 7.169467926025391\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.622\tValidation Loss: 0.618\tTime: 7.1799702644348145\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.601\tValidation Loss: 0.591\tTime: 7.36806058883667\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.569\tValidation Loss: 0.558\tTime: 7.389988660812378\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.536\tValidation Loss: 0.524\tTime: 7.354619026184082\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.493\tValidation Loss: 0.485\tTime: 7.2438671588897705\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.456\tValidation Loss: 0.444\tTime: 7.277097225189209\n",
            "Saving model 23\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment: 24 / 30\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.680\tValidation Loss: 0.666\tTime: 19.351480722427368\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.658\tValidation Loss: 0.653\tTime: 19.45218825340271\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.638\tValidation Loss: 0.630\tTime: 19.14170241355896\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.608\tValidation Loss: 0.603\tTime: 19.491692304611206\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.574\tValidation Loss: 0.579\tTime: 19.237613916397095\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.538\tValidation Loss: 0.535\tTime: 19.06576132774353\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.495\tValidation Loss: 0.499\tTime: 18.950185775756836\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.456\tValidation Loss: 0.454\tTime: 19.231481313705444\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.418\tValidation Loss: 0.421\tTime: 19.406554460525513\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.367\tValidation Loss: 0.370\tTime: 19.229071140289307\n",
            "Saving model 24\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 25 / 30\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.694\tValidation Loss: 0.692\tTime: 9.230631113052368\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.692\tValidation Loss: 0.690\tTime: 9.527332782745361\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.689\tValidation Loss: 0.687\tTime: 9.413230419158936\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.689\tValidation Loss: 0.685\tTime: 9.412335634231567\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.686\tValidation Loss: 0.684\tTime: 8.857020139694214\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.688\tValidation Loss: 0.684\tTime: 9.3740553855896\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.687\tValidation Loss: 0.684\tTime: 9.260775566101074\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.686\tValidation Loss: 0.682\tTime: 8.77151346206665\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.683\tValidation Loss: 0.682\tTime: 8.976300954818726\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.681\tValidation Loss: 0.681\tTime: 8.67374324798584\n",
            "Saving model 25\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 26 / 30\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.692\tValidation Loss: 0.686\tTime: 21.552122354507446\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.689\tValidation Loss: 0.687\tTime: 17.33138108253479\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.687\tValidation Loss: 0.685\tTime: 21.119621753692627\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.687\tValidation Loss: 0.684\tTime: 20.99765920639038\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.687\tValidation Loss: 0.681\tTime: 22.18208885192871\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.686\tValidation Loss: 0.681\tTime: 22.235640048980713\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.685\tValidation Loss: 0.681\tTime: 21.701772928237915\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.684\tValidation Loss: 0.681\tTime: 17.803433418273926\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.682\tValidation Loss: 0.678\tTime: 22.49164319038391\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.682\tValidation Loss: 0.678\tTime: 22.065696477890015\n",
            "Saving model 26\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 27 / 30\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.684\tValidation Loss: 0.673\tTime: 7.325077056884766\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.664\tValidation Loss: 0.658\tTime: 6.726152658462524\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.654\tValidation Loss: 0.644\tTime: 6.883194923400879\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.642\tValidation Loss: 0.632\tTime: 6.920802354812622\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.631\tValidation Loss: 0.625\tTime: 6.892047643661499\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.623\tValidation Loss: 0.613\tTime: 6.793457508087158\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.608\tValidation Loss: 0.601\tTime: 6.806369066238403\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.602\tValidation Loss: 0.594\tTime: 6.70089316368103\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.591\tValidation Loss: 0.584\tTime: 6.684722900390625\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.580\tValidation Loss: 0.575\tTime: 6.711811065673828\n",
            "Saving model 27\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment: 28 / 30\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.691\tValidation Loss: 0.687\tTime: 7.2171642780303955\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.688\tValidation Loss: 0.685\tTime: 6.794962406158447\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.684\tValidation Loss: 0.683\tTime: 6.85439920425415\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.682\tValidation Loss: 0.680\tTime: 6.923794746398926\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.679\tValidation Loss: 0.677\tTime: 6.87891697883606\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.676\tValidation Loss: 0.675\tTime: 7.337771415710449\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.672\tValidation Loss: 0.671\tTime: 7.440266847610474\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.671\tValidation Loss: 0.669\tTime: 7.458268642425537\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.666\tValidation Loss: 0.665\tTime: 7.502887725830078\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.663\tValidation Loss: 0.661\tTime: 7.334418535232544\n",
            "Saving model 28\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment: 29 / 30\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.690\tValidation Loss: 0.688\tTime: 19.734837293624878\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.686\tValidation Loss: 0.682\tTime: 19.77895760536194\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.680\tValidation Loss: 0.679\tTime: 19.884761095046997\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.677\tValidation Loss: 0.674\tTime: 19.959280252456665\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.672\tValidation Loss: 0.669\tTime: 19.623947858810425\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.667\tValidation Loss: 0.669\tTime: 19.516502380371094\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.663\tValidation Loss: 0.667\tTime: 19.77641201019287\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.658\tValidation Loss: 0.661\tTime: 19.46148133277893\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.653\tValidation Loss: 0.658\tTime: 19.31468391418457\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.646\tValidation Loss: 0.645\tTime: 19.38123393058777\n",
            "Saving model 29\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n"
          ]
        }
      ],
      "source": [
        "optimizer_types = [torch.optim.Adam, torch.optim.RAdam, torch.optim.NAdam, torch.optim.AdamW, torch.optim.RMSprop, torch.optim.Adamax]\n",
        "batch_size = 32\n",
        "lr = 0.00001\n",
        "exp_title = 'test_optimizer'\n",
        "# xlm-roberta-base xlm-roberta-large bert-base-uncased roberta-base roberta-large\n",
        "pretrained_model_names = ['xlm-roberta-base', 'xlm-roberta-large', 'bert-base-uncased', 'roberta-base', 'roberta-large']\n",
        "model_type = Ext_Arch\n",
        "loss_func_type = nn.NLLLoss\n",
        "max_length = 25 \n",
        "padding = 'max_length'\n",
        "truncation=True\n",
        "epochs=10\n",
        "\n",
        "exp_index = 0\n",
        "total_exp = len(optimizer_types) * len(pretrained_model_names)\n",
        "for optimizer_type in optimizer_types:\n",
        "    for pretrained_model_name in pretrained_model_names:\n",
        "        model = get_model(pretrained_model_name, model_type, device)\n",
        "        train_dataloader, val_dataloader, test_data, weights = tokenize_data(pretrained_model_name, max_length, padding, truncation, batch_size, device)\n",
        "        loss_func  = loss_func_type(weight=weights)\n",
        "\n",
        "        print(f'Experiment: {exp_index} / {total_exp}')\n",
        "        optimizer = optimizer_type(model.parameters(), lr=lr)\n",
        "        do_exp(model, train_dataloader, val_dataloader, test_data, device, loss_func, optimizer, epochs, lr, \n",
        "            batch_size, max_length, padding, truncation, pretrained_model_name, exp_title, exp_index, root)\n",
        "        exp_index += 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-ZBZKjY76jQ"
      },
      "source": [
        "# Test batch_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "PtqMz_D-76jQ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment: 0 / 25\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.679\tValidation Loss: 0.665\tTime: 23.98838973045349\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.666\tValidation Loss: 0.656\tTime: 21.09397864341736\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.665\tValidation Loss: 0.662\tTime: 19.11738133430481\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.667\tValidation Loss: 0.673\tTime: 20.244322299957275\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.658\tValidation Loss: 0.656\tTime: 22.199096202850342\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.655\tValidation Loss: 0.659\tTime: 18.514081239700317\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.651\tValidation Loss: 0.644\tTime: 20.098565816879272\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.649\tValidation Loss: 0.652\tTime: 17.461560249328613\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.644\tValidation Loss: 0.632\tTime: 19.899435997009277\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.635\tValidation Loss: 0.645\tTime: 17.37855839729309\n",
            "Saving model 0\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 1 / 25\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.678\tValidation Loss: 0.694\tTime: 33.63002300262451\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.678\tValidation Loss: 0.683\tTime: 40.81469011306763\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.664\tValidation Loss: 0.661\tTime: 41.27566957473755\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.667\tValidation Loss: 0.662\tTime: 36.24866223335266\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.657\tValidation Loss: 0.657\tTime: 37.221388816833496\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.651\tValidation Loss: 0.653\tTime: 35.956632137298584\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.644\tValidation Loss: 0.642\tTime: 34.4104540348053\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.641\tValidation Loss: 0.636\tTime: 39.83567190170288\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.636\tValidation Loss: 0.629\tTime: 39.95674443244934\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.632\tValidation Loss: 0.627\tTime: 42.40950655937195\n",
            "Saving model 1\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 2 / 25\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.646\tValidation Loss: 0.609\tTime: 18.515959978103638\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.581\tValidation Loss: 0.553\tTime: 18.143500089645386\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.543\tValidation Loss: 0.509\tTime: 17.991856336593628\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.496\tValidation Loss: 0.486\tTime: 17.693729400634766\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.469\tValidation Loss: 0.436\tTime: 17.296090126037598\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.436\tValidation Loss: 0.409\tTime: 16.08825945854187\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.414\tValidation Loss: 0.389\tTime: 18.02656316757202\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.395\tValidation Loss: 0.373\tTime: 20.308221101760864\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.376\tValidation Loss: 0.357\tTime: 19.41111183166504\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.363\tValidation Loss: 0.375\tTime: 18.92697048187256\n",
            "Saving model 2\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment: 3 / 25\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.670\tValidation Loss: 0.654\tTime: 21.75332474708557\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.635\tValidation Loss: 0.617\tTime: 20.6098370552063\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.574\tValidation Loss: 0.524\tTime: 19.25197410583496\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.477\tValidation Loss: 0.412\tTime: 18.433223009109497\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.371\tValidation Loss: 0.311\tTime: 18.87675642967224\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.296\tValidation Loss: 0.248\tTime: 18.158806324005127\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.239\tValidation Loss: 0.229\tTime: 17.889078617095947\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.211\tValidation Loss: 0.194\tTime: 16.779710054397583\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.224\tValidation Loss: 0.217\tTime: 15.563987970352173\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.205\tValidation Loss: 0.215\tTime: 19.175097942352295\n",
            "Saving model 3\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment: 4 / 25\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.670\tValidation Loss: 0.670\tTime: 38.05182194709778\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.636\tValidation Loss: 0.607\tTime: 41.74182486534119\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.569\tValidation Loss: 0.544\tTime: 37.64458632469177\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.479\tValidation Loss: 0.447\tTime: 35.25330686569214\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.394\tValidation Loss: 0.361\tTime: 33.89645600318909\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.310\tValidation Loss: 0.280\tTime: 31.767972946166992\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.268\tValidation Loss: 0.235\tTime: 39.637451171875\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.223\tValidation Loss: 0.206\tTime: 37.88872170448303\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.199\tValidation Loss: 0.184\tTime: 39.93929696083069\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.199\tValidation Loss: 0.288\tTime: 32.70516848564148\n",
            "Saving model 4\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 5 / 25\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.689\tValidation Loss: 0.689\tTime: 11.921684741973877\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.685\tValidation Loss: 0.676\tTime: 11.802718162536621\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.674\tValidation Loss: 0.671\tTime: 11.825105667114258\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.678\tValidation Loss: 0.670\tTime: 11.847075700759888\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.668\tValidation Loss: 0.676\tTime: 8.744045972824097\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.672\tValidation Loss: 0.666\tTime: 10.938055515289307\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.667\tValidation Loss: 0.663\tTime: 11.231729507446289\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.666\tValidation Loss: 0.663\tTime: 10.27015495300293\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.664\tValidation Loss: 0.656\tTime: 13.396566390991211\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.654\tValidation Loss: 0.650\tTime: 13.12188172340393\n",
            "Saving model 5\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 6 / 25\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.691\tValidation Loss: 0.688\tTime: 25.701112508773804\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.687\tValidation Loss: 0.681\tTime: 26.862378120422363\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.685\tValidation Loss: 0.680\tTime: 26.050567865371704\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.680\tValidation Loss: 0.680\tTime: 24.75468397140503\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.675\tValidation Loss: 0.669\tTime: 24.91399073600769\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.674\tValidation Loss: 0.668\tTime: 24.274856567382812\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.667\tValidation Loss: 0.661\tTime: 24.1332585811615\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.662\tValidation Loss: 0.660\tTime: 24.467950344085693\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.660\tValidation Loss: 0.655\tTime: 26.050262928009033\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.659\tValidation Loss: 0.656\tTime: 21.018518686294556\n",
            "Saving model 6\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 7 / 25\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.664\tValidation Loss: 0.641\tTime: 12.005214214324951\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.623\tValidation Loss: 0.609\tTime: 11.168763160705566\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.582\tValidation Loss: 0.559\tTime: 11.161170959472656\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.552\tValidation Loss: 0.526\tTime: 11.284906387329102\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.514\tValidation Loss: 0.497\tTime: 11.129113674163818\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.498\tValidation Loss: 0.471\tTime: 10.5163893699646\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.469\tValidation Loss: 0.448\tTime: 10.223817348480225\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.445\tValidation Loss: 0.431\tTime: 10.048004865646362\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.434\tValidation Loss: 0.419\tTime: 10.010257720947266\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.415\tValidation Loss: 0.396\tTime: 10.021325826644897\n",
            "Saving model 7\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment: 8 / 25\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.685\tValidation Loss: 0.678\tTime: 10.354317426681519\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.669\tValidation Loss: 0.659\tTime: 9.979732990264893\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.646\tValidation Loss: 0.639\tTime: 9.946695327758789\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.613\tValidation Loss: 0.596\tTime: 10.15288758277893\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.563\tValidation Loss: 0.544\tTime: 9.486114740371704\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.498\tValidation Loss: 0.470\tTime: 9.738135814666748\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.422\tValidation Loss: 0.403\tTime: 9.202181100845337\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.367\tValidation Loss: 0.345\tTime: 11.431230068206787\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.324\tValidation Loss: 0.290\tTime: 11.453316926956177\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.269\tValidation Loss: 0.264\tTime: 11.435560941696167\n",
            "Saving model 8\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment: 9 / 25\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.684\tValidation Loss: 0.675\tTime: 23.603644132614136\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.656\tValidation Loss: 0.651\tTime: 24.11484718322754\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.624\tValidation Loss: 0.617\tTime: 23.835522174835205\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.577\tValidation Loss: 0.564\tTime: 23.05897355079651\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.518\tValidation Loss: 0.506\tTime: 22.863340139389038\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.449\tValidation Loss: 0.442\tTime: 22.555540561676025\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.397\tValidation Loss: 0.376\tTime: 22.76352596282959\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.338\tValidation Loss: 0.321\tTime: 22.317342519760132\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.296\tValidation Loss: 0.280\tTime: 22.622762441635132\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.257\tValidation Loss: 0.252\tTime: 23.891093492507935\n",
            "Saving model 9\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 10 / 25\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.687\tValidation Loss: 0.686\tTime: 9.043885231018066\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.685\tValidation Loss: 0.681\tTime: 8.610185384750366\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.682\tValidation Loss: 0.682\tTime: 6.315745830535889\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.680\tValidation Loss: 0.676\tTime: 9.164732694625854\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.677\tValidation Loss: 0.674\tTime: 9.051925420761108\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.675\tValidation Loss: 0.671\tTime: 9.123213768005371\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.669\tValidation Loss: 0.669\tTime: 9.22821855545044\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.671\tValidation Loss: 0.667\tTime: 8.968701362609863\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.670\tValidation Loss: 0.665\tTime: 9.015359878540039\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.665\tValidation Loss: 0.668\tTime: 6.3105103969573975\n",
            "Saving model 10\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 11 / 25\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.692\tValidation Loss: 0.691\tTime: 21.595371961593628\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.690\tValidation Loss: 0.685\tTime: 21.691043615341187\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.686\tValidation Loss: 0.682\tTime: 21.29072093963623\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.686\tValidation Loss: 0.681\tTime: 21.486388444900513\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.681\tValidation Loss: 0.680\tTime: 21.471702575683594\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.681\tValidation Loss: 0.674\tTime: 22.263978004455566\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.674\tValidation Loss: 0.671\tTime: 22.166958808898926\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.673\tValidation Loss: 0.670\tTime: 22.08313298225403\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.673\tValidation Loss: 0.668\tTime: 22.213752031326294\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.670\tValidation Loss: 0.671\tTime: 17.674452781677246\n",
            "Saving model 11\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 12 / 25\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.682\tValidation Loss: 0.664\tTime: 7.420500755310059\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.653\tValidation Loss: 0.632\tTime: 6.780436038970947\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.627\tValidation Loss: 0.605\tTime: 6.646864652633667\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.604\tValidation Loss: 0.584\tTime: 6.6981635093688965\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.580\tValidation Loss: 0.559\tTime: 6.8641674518585205\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.559\tValidation Loss: 0.544\tTime: 6.7579262256622314\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.540\tValidation Loss: 0.527\tTime: 6.614099502563477\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.523\tValidation Loss: 0.504\tTime: 6.5549538135528564\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.501\tValidation Loss: 0.485\tTime: 6.714276313781738\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.492\tValidation Loss: 0.468\tTime: 6.6679980754852295\n",
            "Saving model 12\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment: 13 / 25\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.689\tValidation Loss: 0.684\tTime: 7.124098539352417\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.679\tValidation Loss: 0.675\tTime: 7.030749797821045\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.669\tValidation Loss: 0.663\tTime: 6.868197917938232\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.653\tValidation Loss: 0.647\tTime: 6.763813257217407\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.636\tValidation Loss: 0.628\tTime: 6.82665228843689\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.612\tValidation Loss: 0.600\tTime: 6.7846455574035645\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.577\tValidation Loss: 0.564\tTime: 6.713482856750488\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.538\tValidation Loss: 0.526\tTime: 6.9053733348846436\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.496\tValidation Loss: 0.484\tTime: 7.364561080932617\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.453\tValidation Loss: 0.437\tTime: 7.224550485610962\n",
            "Saving model 13\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment: 14 / 25\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.686\tValidation Loss: 0.685\tTime: 19.7337486743927\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.671\tValidation Loss: 0.661\tTime: 19.791024446487427\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.650\tValidation Loss: 0.643\tTime: 20.26658535003662\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.625\tValidation Loss: 0.623\tTime: 20.013923406600952\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.601\tValidation Loss: 0.593\tTime: 19.708624124526978\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.569\tValidation Loss: 0.563\tTime: 19.606945514678955\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.524\tValidation Loss: 0.519\tTime: 19.47452974319458\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.478\tValidation Loss: 0.483\tTime: 19.35144877433777\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.436\tValidation Loss: 0.444\tTime: 19.23615574836731\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.401\tValidation Loss: 0.397\tTime: 19.255058526992798\n",
            "Saving model 14\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 15 / 25\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.693\tValidation Loss: 0.691\tTime: 7.406397819519043\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.690\tValidation Loss: 0.688\tTime: 7.4895713329315186\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.686\tValidation Loss: 0.686\tTime: 7.704280138015747\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.684\tValidation Loss: 0.681\tTime: 7.730759620666504\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.683\tValidation Loss: 0.679\tTime: 8.044843673706055\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.685\tValidation Loss: 0.679\tTime: 8.176687717437744\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.682\tValidation Loss: 0.677\tTime: 7.697906970977783\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.680\tValidation Loss: 0.680\tTime: 5.243627071380615\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.679\tValidation Loss: 0.676\tTime: 7.620886325836182\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.678\tValidation Loss: 0.675\tTime: 7.5558922290802\n",
            "Saving model 15\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 16 / 25\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.695\tValidation Loss: 0.689\tTime: 18.88199758529663\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.689\tValidation Loss: 0.683\tTime: 18.62480592727661\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.686\tValidation Loss: 0.682\tTime: 17.73676586151123\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.688\tValidation Loss: 0.683\tTime: 13.362841844558716\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.685\tValidation Loss: 0.686\tTime: 13.046334743499756\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.681\tValidation Loss: 0.679\tTime: 17.591274976730347\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.681\tValidation Loss: 0.674\tTime: 17.90702748298645\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.679\tValidation Loss: 0.671\tTime: 16.951937913894653\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.675\tValidation Loss: 0.678\tTime: 13.04943585395813\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.677\tValidation Loss: 0.672\tTime: 12.026671648025513\n",
            "Saving model 16\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 17 / 25\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.684\tValidation Loss: 0.666\tTime: 6.2796101570129395\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.664\tValidation Loss: 0.647\tTime: 5.772010326385498\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.647\tValidation Loss: 0.631\tTime: 5.884489059448242\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.625\tValidation Loss: 0.619\tTime: 5.773471832275391\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.611\tValidation Loss: 0.601\tTime: 5.8541858196258545\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.598\tValidation Loss: 0.582\tTime: 5.797602891921997\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.583\tValidation Loss: 0.573\tTime: 5.791057109832764\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.567\tValidation Loss: 0.560\tTime: 5.742867946624756\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.560\tValidation Loss: 0.541\tTime: 5.770049571990967\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.548\tValidation Loss: 0.529\tTime: 5.929220199584961\n",
            "Saving model 17\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment: 18 / 25\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.690\tValidation Loss: 0.686\tTime: 5.990937232971191\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.684\tValidation Loss: 0.681\tTime: 5.970773696899414\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.679\tValidation Loss: 0.675\tTime: 6.064346790313721\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.673\tValidation Loss: 0.668\tTime: 6.276692152023315\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.664\tValidation Loss: 0.660\tTime: 6.046563625335693\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.653\tValidation Loss: 0.651\tTime: 6.045080900192261\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.642\tValidation Loss: 0.639\tTime: 5.950970649719238\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.628\tValidation Loss: 0.624\tTime: 6.133475065231323\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.612\tValidation Loss: 0.607\tTime: 6.112462282180786\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.591\tValidation Loss: 0.588\tTime: 5.903470039367676\n",
            "Saving model 18\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment: 19 / 25\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.687\tValidation Loss: 0.681\tTime: 15.009151935577393\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.679\tValidation Loss: 0.674\tTime: 15.038951635360718\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.670\tValidation Loss: 0.663\tTime: 15.45943307876587\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.655\tValidation Loss: 0.657\tTime: 15.667170524597168\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.643\tValidation Loss: 0.641\tTime: 15.107288837432861\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.624\tValidation Loss: 0.626\tTime: 15.22336483001709\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.606\tValidation Loss: 0.608\tTime: 14.69206714630127\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.584\tValidation Loss: 0.589\tTime: 14.971213579177856\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.555\tValidation Loss: 0.562\tTime: 16.766064643859863\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.521\tValidation Loss: 0.537\tTime: 16.036067247390747\n",
            "Saving model 19\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 20 / 25\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.695\tValidation Loss: 0.691\tTime: 7.020407438278198\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.691\tValidation Loss: 0.690\tTime: 7.199737787246704\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.690\tValidation Loss: 0.688\tTime: 7.451333522796631\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.689\tValidation Loss: 0.687\tTime: 7.3570396900177\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.687\tValidation Loss: 0.685\tTime: 7.236110210418701\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.687\tValidation Loss: 0.685\tTime: 7.600670576095581\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.687\tValidation Loss: 0.684\tTime: 7.787938594818115\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.684\tValidation Loss: 0.683\tTime: 7.671349048614502\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.682\tValidation Loss: 0.681\tTime: 7.78887414932251\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.682\tValidation Loss: 0.680\tTime: 7.2207112312316895\n",
            "Saving model 20\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 21 / 25\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.694\tValidation Loss: 0.690\tTime: 15.675187826156616\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.690\tValidation Loss: 0.688\tTime: 15.850068092346191\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.687\tValidation Loss: 0.687\tTime: 15.51608395576477\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.686\tValidation Loss: 0.685\tTime: 15.65383005142212\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.686\tValidation Loss: 0.684\tTime: 15.54559588432312\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.686\tValidation Loss: 0.683\tTime: 15.588791131973267\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.685\tValidation Loss: 0.682\tTime: 16.00121808052063\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.685\tValidation Loss: 0.681\tTime: 15.860946416854858\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.682\tValidation Loss: 0.680\tTime: 15.927402973175049\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.681\tValidation Loss: 0.679\tTime: 15.977258920669556\n",
            "Saving model 21\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 22 / 25\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.688\tValidation Loss: 0.679\tTime: 5.829899787902832\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.673\tValidation Loss: 0.666\tTime: 5.420332431793213\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.658\tValidation Loss: 0.653\tTime: 5.386577844619751\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.648\tValidation Loss: 0.642\tTime: 5.381049394607544\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.636\tValidation Loss: 0.632\tTime: 5.315939903259277\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.627\tValidation Loss: 0.623\tTime: 5.331041574478149\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.616\tValidation Loss: 0.613\tTime: 5.395085573196411\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.607\tValidation Loss: 0.604\tTime: 5.280910491943359\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.600\tValidation Loss: 0.594\tTime: 5.348975896835327\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.592\tValidation Loss: 0.586\tTime: 5.33595871925354\n",
            "Saving model 22\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment: 23 / 25\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.691\tValidation Loss: 0.690\tTime: 5.682338237762451\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.688\tValidation Loss: 0.687\tTime: 5.714782476425171\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.685\tValidation Loss: 0.684\tTime: 5.4158549308776855\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.682\tValidation Loss: 0.680\tTime: 5.491091728210449\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.677\tValidation Loss: 0.677\tTime: 5.462161540985107\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.673\tValidation Loss: 0.672\tTime: 5.545737028121948\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.668\tValidation Loss: 0.667\tTime: 5.450448513031006\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.663\tValidation Loss: 0.662\tTime: 5.469615459442139\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.655\tValidation Loss: 0.655\tTime: 5.540325403213501\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.648\tValidation Loss: 0.648\tTime: 5.520331859588623\n",
            "Saving model 23\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment: 24 / 25\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.691\tValidation Loss: 0.687\tTime: 13.190217971801758\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.684\tValidation Loss: 0.681\tTime: 13.558694839477539\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.678\tValidation Loss: 0.674\tTime: 13.757916927337646\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.672\tValidation Loss: 0.668\tTime: 13.564280986785889\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.663\tValidation Loss: 0.662\tTime: 13.500276327133179\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.656\tValidation Loss: 0.654\tTime: 13.631559610366821\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.646\tValidation Loss: 0.647\tTime: 13.923434734344482\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.637\tValidation Loss: 0.637\tTime: 14.033616065979004\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.625\tValidation Loss: 0.627\tTime: 13.270806312561035\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.610\tValidation Loss: 0.616\tTime: 13.840985774993896\n",
            "Saving model 24\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n"
          ]
        }
      ],
      "source": [
        "batch_sizes = [8, 16, 32, 64, 128]\n",
        "lr = 0.00001\n",
        "exp_title = 'test_batch'\n",
        "# xlm-roberta-base xlm-roberta-large bert-base-uncased roberta-base roberta-large\n",
        "pretrained_model_names = ['xlm-roberta-base', 'xlm-roberta-large', 'bert-base-uncased', 'roberta-base', 'roberta-large']\n",
        "model_type = Ext_Arch\n",
        "optimizer_type = torch.optim.AdamW\n",
        "loss_func_type = nn.NLLLoss\n",
        "max_length = 25 \n",
        "padding = 'max_length'\n",
        "truncation=True\n",
        "epochs=10\n",
        "\n",
        "exp_index = 0\n",
        "total_exp = len(batch_sizes) * len(pretrained_model_names)\n",
        "for batch_size in batch_sizes:\n",
        "    for pretrained_model_name in pretrained_model_names:\n",
        "        model = get_model(pretrained_model_name, model_type, device)\n",
        "        train_dataloader, val_dataloader, test_data, weights = tokenize_data(pretrained_model_name, max_length, padding, truncation, batch_size, device)\n",
        "        loss_func  = loss_func_type(weight=weights)\n",
        "\n",
        "        print(f'Experiment: {exp_index} / {total_exp}')\n",
        "        optimizer = optimizer_type(model.parameters(), lr=lr)\n",
        "        do_exp(model, train_dataloader, val_dataloader, test_data, device, loss_func, optimizer, epochs, lr, \n",
        "            batch_size, max_length, padding, truncation, pretrained_model_name, exp_title, exp_index, root)\n",
        "        exp_index += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vh6Sxuy076jU"
      },
      "source": [
        "# Test epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "7d17e66b0d2c4b179f6eb10703a8f103",
            "0956d50bb8bb4a96a5839096c02a1396",
            "95e0dacbf7624768936cfc9761315052",
            "0d5efedfb9334fda99c4d6a3c442ea5a",
            "509362e2f4e74751a503b749069b7c52",
            "75e2468acf4d4291a88507128bee5659",
            "8f7fa846ce8340adbb09bd64e13645f0",
            "7b339b0ca3cd4315b3c6ff4c29994311",
            "4ed08d7e20da4b28bbb0d4999b72607f",
            "18ebd228a00843258b30e331c994866b",
            "7bf75335b5014e9b819e8a3c0104b4cf",
            "d3a598d8453942deb6a88a93f04890e3",
            "f1a06f763c1c459683c16bef52a1c625",
            "5bc617eee02b4b51a4381ddd1cafcfca",
            "92d5b5aa4bb34104a8e621d3a535d38b",
            "96cc903d307f485f9539b64dcb2dc6e2",
            "cf9c25bb737d4fc98827eedaa8697437",
            "589f1cf485f545e78ffd32b36989c890",
            "48cd5de792aa41c7943dce135633c1af",
            "08252b60503a48ff83eb09cbef145008",
            "da7d4a0db77b4887b65d69e5f4f07f24",
            "9dfdb94fceaf417d9395cb6d27be6d7a",
            "c8573da774bd4354b53397f290fa7027",
            "ce579f849d9543bbb4885817c5d7c868",
            "ffb3e7b234214b31b380cfe32ae3efbf",
            "9f76a1d2473c460d9dcb8b14b7cee41f",
            "6413e70cc9414dc580660cb6d63b921c",
            "996a74b459a249f0bbde8078e2c8fc13",
            "f97e3e28cdea468fbd6a3a083560f0d7",
            "17488c3957d74bfc97d2107cde6a81d5",
            "19ee7f62678740ceb424d286d7aa5ab8",
            "78a53ca1a88147bea7aacdd03dbbcb9f",
            "b12dc328ef254cd382c56fb896427206",
            "5fd3aa86a3b94c7db7f81e8d89fee6ab",
            "5bf481737e944c9f8bb8c825bfa04770",
            "d8f0497100314a008b7acdbd949e152d",
            "38e7355a03394d828f8c456fb3c643ca",
            "921a0c380c184d728a2b6ecb4f468965",
            "417e6d002d79481d9d95f7a49f8ce743",
            "82a686ed0f574dfca2263869739624ba",
            "ead0b4791e8f4cfdbda9531355c5df77",
            "7de6c639f9e14044a1916ed80b0f5d65",
            "acd70587d99947ce8881e2cb3aa843db",
            "181351edbd1d4812beab0cb19db53b95",
            "4058eaba28ce438dafdb08d5c9dbc019",
            "8052bd807aaa4d3596d7902a00016c1d",
            "99f3590f696441d89af03e0047aac3d9",
            "0a035b64a4e1468fb7707e6d50a0b94d",
            "7bc3458ecf5040cf92137e1b9775351b",
            "22ce42b26c1649c2b4c2227e9a5da007",
            "448decff4b1a430b8d914957c987c83a",
            "e1a7d093d8bb4f28806d90f8f45a8bee",
            "2a8ddccfc7c34abfb7a4979457a5f7a3",
            "6c669e758bf24354931837f267521295",
            "cdbe6a8c633b4327871ae2da3536ca32",
            "694045217454412fbc360687a58bf45e",
            "9bc05b200c0e427cb8160c6d0e759d19",
            "2e050b7ac70d4864af095712c276fb14",
            "a54fc3fda1854726b4b12d4ba8b236a5",
            "264558bb349e471689fee9d52e8fdd87",
            "5f7f43db8be44e4eb0eb407807aa179a",
            "d221704dead9424db87799577af23ca5",
            "c0096937868042a79c5bf2850998f40f",
            "ae119e6513b043dcb24e311402175226",
            "f8b79b63acb24d648c53897d426c255d",
            "22c7785136854defb6606cb0b6c17e4f",
            "3c51c98758f6466f94ae99b2e171708a",
            "052d883916bb4ac986ff38f3cab495ad",
            "1012ab71d77343faa8fac411407531e6",
            "4136a420f952478080ecd24e970ce25d",
            "ee9aacfa8b8a4df2b6e546f6261ebee2",
            "a4ed2246098847a2a85c8bd93540c654",
            "569e0f56568c40839449c5f334dd5961",
            "df24cde7cc614e40b17a5f6f30674860",
            "034f900b20084530a494765a5aaab0e3",
            "1aef4eba6df14726bdc0ce2140d17012",
            "56b1479b512748c5a2a624c02b5ae417",
            "6ce00ecc713f4b8ba93572b8148f9001",
            "20c90a9b70914ff0800325722138f757",
            "5d34d1cce81c4f92aba606cbefdac8ef",
            "b06fe147c9374d578412e6cf510de7dc"
          ]
        },
        "id": "prsz_qOU76jW",
        "outputId": "4f50784a-d846-433c-db93-d4551b2b1930"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment: 0 / 25\n",
            "  Epoch 1 / 2\n",
            "    Training Loss: 0.690\tValidation Loss: 0.686\tTime: 8.861162424087524\n",
            "  Epoch 2 / 2\n",
            "    Training Loss: 0.687\tValidation Loss: 0.683\tTime: 8.766016483306885\n",
            "Saving model 0\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 1 / 25\n",
            "  Epoch 1 / 2\n",
            "    Training Loss: 0.694\tValidation Loss: 0.686\tTime: 22.153784036636353\n",
            "  Epoch 2 / 2\n",
            "    Training Loss: 0.685\tValidation Loss: 0.682\tTime: 21.05605936050415\n",
            "Saving model 1\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 2 / 25\n",
            "  Epoch 1 / 2\n",
            "    Training Loss: 0.672\tValidation Loss: 0.652\tTime: 7.019873142242432\n",
            "  Epoch 2 / 2\n",
            "    Training Loss: 0.638\tValidation Loss: 0.619\tTime: 6.679035902023315\n",
            "Saving model 2\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment: 3 / 25\n",
            "  Epoch 1 / 2\n",
            "    Training Loss: 0.690\tValidation Loss: 0.685\tTime: 7.059591293334961\n",
            "  Epoch 2 / 2\n",
            "    Training Loss: 0.681\tValidation Loss: 0.676\tTime: 7.02262282371521\n",
            "Saving model 3\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment: 4 / 25\n",
            "  Epoch 1 / 2\n",
            "    Training Loss: 0.688\tValidation Loss: 0.675\tTime: 18.913079261779785\n",
            "  Epoch 2 / 2\n",
            "    Training Loss: 0.668\tValidation Loss: 0.661\tTime: 18.94669818878174\n",
            "Saving model 4\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 5 / 25\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.691\tValidation Loss: 0.686\tTime: 8.470205783843994\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.685\tValidation Loss: 0.683\tTime: 9.207899808883667\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.681\tValidation Loss: 0.680\tTime: 9.137566566467285\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.682\tValidation Loss: 0.676\tTime: 8.783324480056763\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.682\tValidation Loss: 0.677\tTime: 6.33894157409668\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.676\tValidation Loss: 0.672\tTime: 8.776455402374268\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.676\tValidation Loss: 0.671\tTime: 8.747791528701782\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.673\tValidation Loss: 0.668\tTime: 9.019104480743408\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.670\tValidation Loss: 0.667\tTime: 9.93942403793335\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.670\tValidation Loss: 0.666\tTime: 9.006928443908691\n",
            "Saving model 5\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 6 / 25\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.693\tValidation Loss: 0.686\tTime: 22.397684335708618\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.688\tValidation Loss: 0.684\tTime: 21.708515167236328\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.685\tValidation Loss: 0.680\tTime: 21.44677710533142\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.688\tValidation Loss: 0.679\tTime: 21.21941351890564\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.680\tValidation Loss: 0.675\tTime: 21.511581659317017\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.679\tValidation Loss: 0.673\tTime: 21.675630569458008\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.679\tValidation Loss: 0.673\tTime: 21.69155263900757\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.674\tValidation Loss: 0.674\tTime: 17.993377923965454\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.672\tValidation Loss: 0.670\tTime: 22.166805505752563\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.669\tValidation Loss: 0.664\tTime: 22.516568660736084\n",
            "Saving model 6\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 7 / 25\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.670\tValidation Loss: 0.646\tTime: 7.591184854507446\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.640\tValidation Loss: 0.615\tTime: 7.173248052597046\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.611\tValidation Loss: 0.592\tTime: 7.031142950057983\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.581\tValidation Loss: 0.569\tTime: 7.093734264373779\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.560\tValidation Loss: 0.545\tTime: 6.907941102981567\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.538\tValidation Loss: 0.520\tTime: 6.939697027206421\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.519\tValidation Loss: 0.509\tTime: 6.847618818283081\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.500\tValidation Loss: 0.490\tTime: 6.806779623031616\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.484\tValidation Loss: 0.473\tTime: 6.755766153335571\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.467\tValidation Loss: 0.456\tTime: 6.787926435470581\n",
            "Saving model 7\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment: 8 / 25\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.690\tValidation Loss: 0.683\tTime: 7.195374250411987\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.681\tValidation Loss: 0.674\tTime: 7.042504072189331\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.668\tValidation Loss: 0.663\tTime: 6.971092224121094\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.654\tValidation Loss: 0.646\tTime: 7.004769325256348\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.632\tValidation Loss: 0.623\tTime: 6.984473943710327\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.604\tValidation Loss: 0.594\tTime: 7.114091873168945\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.569\tValidation Loss: 0.559\tTime: 6.977406740188599\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.527\tValidation Loss: 0.516\tTime: 6.975372552871704\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.485\tValidation Loss: 0.469\tTime: 6.910749197006226\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.443\tValidation Loss: 0.426\tTime: 7.056468725204468\n",
            "Saving model 8\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment: 9 / 25\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.688\tValidation Loss: 0.682\tTime: 18.899912118911743\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.672\tValidation Loss: 0.667\tTime: 19.754177570343018\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.656\tValidation Loss: 0.655\tTime: 20.140976428985596\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.635\tValidation Loss: 0.630\tTime: 19.928184270858765\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.604\tValidation Loss: 0.599\tTime: 19.78309965133667\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.573\tValidation Loss: 0.563\tTime: 19.919211864471436\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.532\tValidation Loss: 0.526\tTime: 19.77229332923889\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.481\tValidation Loss: 0.490\tTime: 19.579654455184937\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.441\tValidation Loss: 0.444\tTime: 19.486360549926758\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.405\tValidation Loss: 0.408\tTime: 19.591017484664917\n",
            "Saving model 9\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 10 / 25\n",
            "  Epoch 1 / 20\n",
            "    Training Loss: 0.692\tValidation Loss: 0.685\tTime: 8.660951375961304\n",
            "  Epoch 2 / 20\n",
            "    Training Loss: 0.686\tValidation Loss: 0.682\tTime: 8.52806568145752\n",
            "  Epoch 3 / 20\n",
            "    Training Loss: 0.685\tValidation Loss: 0.680\tTime: 8.699077844619751\n",
            "  Epoch 4 / 20\n",
            "    Training Loss: 0.683\tValidation Loss: 0.676\tTime: 8.350282192230225\n",
            "  Epoch 5 / 20\n",
            "    Training Loss: 0.679\tValidation Loss: 0.675\tTime: 8.46064567565918\n",
            "  Epoch 6 / 20\n",
            "    Training Loss: 0.677\tValidation Loss: 0.673\tTime: 8.504305839538574\n",
            "  Epoch 7 / 20\n",
            "    Training Loss: 0.673\tValidation Loss: 0.669\tTime: 8.517787218093872\n",
            "  Epoch 8 / 20\n",
            "    Training Loss: 0.672\tValidation Loss: 0.672\tTime: 6.2970969676971436\n",
            "  Epoch 9 / 20\n",
            "    Training Loss: 0.671\tValidation Loss: 0.668\tTime: 9.388917207717896\n",
            "  Epoch 10 / 20\n",
            "    Training Loss: 0.670\tValidation Loss: 0.664\tTime: 9.15772819519043\n",
            "  Epoch 11 / 20\n",
            "    Training Loss: 0.669\tValidation Loss: 0.662\tTime: 8.8621084690094\n",
            "  Epoch 12 / 20\n",
            "    Training Loss: 0.666\tValidation Loss: 0.666\tTime: 6.31548547744751\n",
            "  Epoch 13 / 20\n",
            "    Training Loss: 0.664\tValidation Loss: 0.658\tTime: 8.972297430038452\n",
            "  Epoch 14 / 20\n",
            "    Training Loss: 0.663\tValidation Loss: 0.657\tTime: 9.325387239456177\n",
            "  Epoch 15 / 20\n",
            "    Training Loss: 0.658\tValidation Loss: 0.654\tTime: 9.401001214981079\n",
            "  Epoch 16 / 20\n",
            "    Training Loss: 0.654\tValidation Loss: 0.654\tTime: 6.369373798370361\n",
            "  Epoch 17 / 20\n",
            "    Training Loss: 0.652\tValidation Loss: 0.648\tTime: 8.903038024902344\n",
            "  Epoch 18 / 20\n",
            "    Training Loss: 0.651\tValidation Loss: 0.646\tTime: 9.102917671203613\n",
            "  Epoch 19 / 20\n",
            "    Training Loss: 0.647\tValidation Loss: 0.648\tTime: 6.538273334503174\n",
            "  Epoch 20 / 20\n",
            "    Training Loss: 0.651\tValidation Loss: 0.642\tTime: 9.019500732421875\n",
            "Saving model 10\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 11 / 25\n",
            "  Epoch 1 / 20\n",
            "    Training Loss: 0.689\tValidation Loss: 0.688\tTime: 21.39113736152649\n",
            "  Epoch 2 / 20\n",
            "    Training Loss: 0.688\tValidation Loss: 0.683\tTime: 21.24049711227417\n",
            "  Epoch 3 / 20\n",
            "    Training Loss: 0.685\tValidation Loss: 0.682\tTime: 21.927545309066772\n",
            "  Epoch 4 / 20\n",
            "    Training Loss: 0.683\tValidation Loss: 0.678\tTime: 21.420313358306885\n",
            "  Epoch 5 / 20\n",
            "    Training Loss: 0.679\tValidation Loss: 0.674\tTime: 21.163392543792725\n",
            "  Epoch 6 / 20\n",
            "    Training Loss: 0.679\tValidation Loss: 0.672\tTime: 21.622281789779663\n",
            "  Epoch 7 / 20\n",
            "    Training Loss: 0.676\tValidation Loss: 0.672\tTime: 21.836736917495728\n",
            "  Epoch 8 / 20\n",
            "    Training Loss: 0.676\tValidation Loss: 0.671\tTime: 21.54808259010315\n",
            "  Epoch 9 / 20\n",
            "    Training Loss: 0.668\tValidation Loss: 0.666\tTime: 22.273963451385498\n",
            "  Epoch 10 / 20\n",
            "    Training Loss: 0.671\tValidation Loss: 0.663\tTime: 21.680235862731934\n",
            "  Epoch 11 / 20\n",
            "    Training Loss: 0.667\tValidation Loss: 0.665\tTime: 17.638585329055786\n",
            "  Epoch 12 / 20\n",
            "    Training Loss: 0.664\tValidation Loss: 0.663\tTime: 21.42605447769165\n",
            "  Epoch 13 / 20\n",
            "    Training Loss: 0.659\tValidation Loss: 0.655\tTime: 21.80486273765564\n",
            "  Epoch 14 / 20\n",
            "    Training Loss: 0.655\tValidation Loss: 0.653\tTime: 21.86941409111023\n",
            "  Epoch 15 / 20\n",
            "    Training Loss: 0.657\tValidation Loss: 0.651\tTime: 21.655343770980835\n",
            "  Epoch 16 / 20\n",
            "    Training Loss: 0.656\tValidation Loss: 0.650\tTime: 21.479774951934814\n",
            "  Epoch 17 / 20\n",
            "    Training Loss: 0.652\tValidation Loss: 0.646\tTime: 21.561782836914062\n",
            "  Epoch 18 / 20\n",
            "    Training Loss: 0.648\tValidation Loss: 0.644\tTime: 22.108956336975098\n",
            "  Epoch 19 / 20\n",
            "    Training Loss: 0.648\tValidation Loss: 0.646\tTime: 17.671931266784668\n",
            "  Epoch 20 / 20\n",
            "    Training Loss: 0.642\tValidation Loss: 0.638\tTime: 22.31116819381714\n",
            "Saving model 11\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 12 / 25\n",
            "  Epoch 1 / 20\n",
            "    Training Loss: 0.671\tValidation Loss: 0.650\tTime: 7.53550124168396\n",
            "  Epoch 2 / 20\n",
            "    Training Loss: 0.643\tValidation Loss: 0.622\tTime: 7.095573902130127\n",
            "  Epoch 3 / 20\n",
            "    Training Loss: 0.615\tValidation Loss: 0.609\tTime: 6.936500072479248\n",
            "  Epoch 4 / 20\n",
            "    Training Loss: 0.593\tValidation Loss: 0.571\tTime: 7.045692443847656\n",
            "  Epoch 5 / 20\n",
            "    Training Loss: 0.569\tValidation Loss: 0.549\tTime: 6.779224157333374\n",
            "  Epoch 6 / 20\n",
            "    Training Loss: 0.545\tValidation Loss: 0.538\tTime: 7.118973255157471\n",
            "  Epoch 7 / 20\n",
            "    Training Loss: 0.528\tValidation Loss: 0.512\tTime: 6.7849531173706055\n",
            "  Epoch 8 / 20\n",
            "    Training Loss: 0.503\tValidation Loss: 0.490\tTime: 6.656920433044434\n",
            "  Epoch 9 / 20\n",
            "    Training Loss: 0.491\tValidation Loss: 0.472\tTime: 6.691697597503662\n",
            "  Epoch 10 / 20\n",
            "    Training Loss: 0.473\tValidation Loss: 0.467\tTime: 6.696234941482544\n",
            "  Epoch 11 / 20\n",
            "    Training Loss: 0.456\tValidation Loss: 0.444\tTime: 6.8536951541900635\n",
            "  Epoch 12 / 20\n",
            "    Training Loss: 0.445\tValidation Loss: 0.429\tTime: 6.760172128677368\n",
            "  Epoch 13 / 20\n",
            "    Training Loss: 0.440\tValidation Loss: 0.417\tTime: 6.821599960327148\n",
            "  Epoch 14 / 20\n",
            "    Training Loss: 0.427\tValidation Loss: 0.407\tTime: 6.827200889587402\n",
            "  Epoch 15 / 20\n",
            "    Training Loss: 0.414\tValidation Loss: 0.395\tTime: 6.865864276885986\n",
            "  Epoch 16 / 20\n",
            "    Training Loss: 0.396\tValidation Loss: 0.384\tTime: 6.78301739692688\n",
            "  Epoch 17 / 20\n",
            "    Training Loss: 0.390\tValidation Loss: 0.375\tTime: 6.623539924621582\n",
            "  Epoch 18 / 20\n",
            "    Training Loss: 0.371\tValidation Loss: 0.373\tTime: 6.797580718994141\n",
            "  Epoch 19 / 20\n",
            "    Training Loss: 0.374\tValidation Loss: 0.359\tTime: 6.639119625091553\n",
            "  Epoch 20 / 20\n",
            "    Training Loss: 0.361\tValidation Loss: 0.349\tTime: 6.616823196411133\n",
            "Saving model 12\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment: 13 / 25\n",
            "  Epoch 1 / 20\n",
            "    Training Loss: 0.689\tValidation Loss: 0.684\tTime: 6.80451512336731\n",
            "  Epoch 2 / 20\n",
            "    Training Loss: 0.680\tValidation Loss: 0.676\tTime: 6.7667906284332275\n",
            "  Epoch 3 / 20\n",
            "    Training Loss: 0.670\tValidation Loss: 0.665\tTime: 6.689858913421631\n",
            "  Epoch 4 / 20\n",
            "    Training Loss: 0.653\tValidation Loss: 0.648\tTime: 7.011199951171875\n",
            "  Epoch 5 / 20\n",
            "    Training Loss: 0.634\tValidation Loss: 0.628\tTime: 7.2911155223846436\n",
            "  Epoch 6 / 20\n",
            "    Training Loss: 0.609\tValidation Loss: 0.597\tTime: 7.231879472732544\n",
            "  Epoch 7 / 20\n",
            "    Training Loss: 0.574\tValidation Loss: 0.561\tTime: 7.18322229385376\n",
            "  Epoch 8 / 20\n",
            "    Training Loss: 0.532\tValidation Loss: 0.518\tTime: 7.209816932678223\n",
            "  Epoch 9 / 20\n",
            "    Training Loss: 0.490\tValidation Loss: 0.474\tTime: 7.16950535774231\n",
            "  Epoch 10 / 20\n",
            "    Training Loss: 0.449\tValidation Loss: 0.432\tTime: 7.0802903175354\n",
            "  Epoch 11 / 20\n",
            "    Training Loss: 0.404\tValidation Loss: 0.390\tTime: 6.985238313674927\n",
            "  Epoch 12 / 20\n",
            "    Training Loss: 0.365\tValidation Loss: 0.352\tTime: 7.084964036941528\n",
            "  Epoch 13 / 20\n",
            "    Training Loss: 0.331\tValidation Loss: 0.317\tTime: 7.302319288253784\n",
            "  Epoch 14 / 20\n",
            "    Training Loss: 0.309\tValidation Loss: 0.289\tTime: 7.481375694274902\n",
            "  Epoch 15 / 20\n",
            "    Training Loss: 0.273\tValidation Loss: 0.270\tTime: 7.417805194854736\n",
            "  Epoch 16 / 20\n",
            "    Training Loss: 0.251\tValidation Loss: 0.246\tTime: 7.330837965011597\n",
            "  Epoch 17 / 20\n",
            "    Training Loss: 0.243\tValidation Loss: 0.232\tTime: 7.263888597488403\n",
            "  Epoch 18 / 20\n",
            "    Training Loss: 0.221\tValidation Loss: 0.217\tTime: 7.276565313339233\n",
            "  Epoch 19 / 20\n",
            "    Training Loss: 0.220\tValidation Loss: 0.204\tTime: 7.3908185958862305\n",
            "  Epoch 20 / 20\n",
            "    Training Loss: 0.197\tValidation Loss: 0.196\tTime: 7.352125406265259\n",
            "Saving model 13\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment: 14 / 25\n",
            "  Epoch 1 / 20\n",
            "    Training Loss: 0.687\tValidation Loss: 0.676\tTime: 19.335097074508667\n",
            "  Epoch 2 / 20\n",
            "    Training Loss: 0.668\tValidation Loss: 0.664\tTime: 19.32908320426941\n",
            "  Epoch 3 / 20\n",
            "    Training Loss: 0.651\tValidation Loss: 0.642\tTime: 19.409101486206055\n",
            "  Epoch 4 / 20\n",
            "    Training Loss: 0.624\tValidation Loss: 0.620\tTime: 19.298245429992676\n",
            "  Epoch 5 / 20\n",
            "    Training Loss: 0.600\tValidation Loss: 0.592\tTime: 19.27995252609253\n",
            "  Epoch 6 / 20\n",
            "    Training Loss: 0.563\tValidation Loss: 0.560\tTime: 19.17588496208191\n",
            "  Epoch 7 / 20\n",
            "    Training Loss: 0.524\tValidation Loss: 0.528\tTime: 19.73770022392273\n",
            "  Epoch 8 / 20\n",
            "    Training Loss: 0.486\tValidation Loss: 0.480\tTime: 19.83469820022583\n",
            "  Epoch 9 / 20\n",
            "    Training Loss: 0.438\tValidation Loss: 0.441\tTime: 19.719359874725342\n",
            "  Epoch 10 / 20\n",
            "    Training Loss: 0.404\tValidation Loss: 0.393\tTime: 19.80029821395874\n",
            "  Epoch 11 / 20\n",
            "    Training Loss: 0.356\tValidation Loss: 0.370\tTime: 19.811071157455444\n",
            "  Epoch 12 / 20\n",
            "    Training Loss: 0.328\tValidation Loss: 0.326\tTime: 19.72534441947937\n",
            "  Epoch 13 / 20\n",
            "    Training Loss: 0.303\tValidation Loss: 0.313\tTime: 19.488311052322388\n",
            "  Epoch 14 / 20\n",
            "    Training Loss: 0.278\tValidation Loss: 0.277\tTime: 19.69755220413208\n",
            "  Epoch 15 / 20\n",
            "    Training Loss: 0.251\tValidation Loss: 0.263\tTime: 19.36877465248108\n",
            "  Epoch 16 / 20\n",
            "    Training Loss: 0.237\tValidation Loss: 0.243\tTime: 19.363749980926514\n",
            "  Epoch 17 / 20\n",
            "    Training Loss: 0.228\tValidation Loss: 0.230\tTime: 19.25164222717285\n",
            "  Epoch 18 / 20\n",
            "    Training Loss: 0.217\tValidation Loss: 0.221\tTime: 19.042582273483276\n",
            "  Epoch 19 / 20\n",
            "    Training Loss: 0.216\tValidation Loss: 0.208\tTime: 19.890692710876465\n",
            "  Epoch 20 / 20\n",
            "    Training Loss: 0.194\tValidation Loss: 0.199\tTime: 19.873405694961548\n",
            "Saving model 14\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 15 / 25\n",
            "  Epoch 1 / 50\n",
            "    Training Loss: 0.691\tValidation Loss: 0.685\tTime: 9.26445984840393\n",
            "  Epoch 2 / 50\n",
            "    Training Loss: 0.686\tValidation Loss: 0.685\tTime: 6.569987773895264\n",
            "  Epoch 3 / 50\n",
            "    Training Loss: 0.687\tValidation Loss: 0.679\tTime: 9.507031679153442\n",
            "  Epoch 4 / 50\n",
            "    Training Loss: 0.681\tValidation Loss: 0.677\tTime: 9.425459384918213\n",
            "  Epoch 5 / 50\n",
            "    Training Loss: 0.680\tValidation Loss: 0.677\tTime: 8.921682596206665\n",
            "  Epoch 6 / 50\n",
            "    Training Loss: 0.678\tValidation Loss: 0.676\tTime: 9.075105667114258\n",
            "  Epoch 7 / 50\n",
            "    Training Loss: 0.676\tValidation Loss: 0.673\tTime: 9.136417150497437\n",
            "  Epoch 8 / 50\n",
            "    Training Loss: 0.674\tValidation Loss: 0.670\tTime: 8.97774362564087\n",
            "  Epoch 9 / 50\n",
            "    Training Loss: 0.674\tValidation Loss: 0.669\tTime: 8.94765567779541\n",
            "  Epoch 10 / 50\n",
            "    Training Loss: 0.671\tValidation Loss: 0.667\tTime: 8.60437560081482\n",
            "  Epoch 11 / 50\n",
            "    Training Loss: 0.670\tValidation Loss: 0.664\tTime: 8.760822296142578\n",
            "  Epoch 12 / 50\n",
            "    Training Loss: 0.667\tValidation Loss: 0.663\tTime: 8.539199590682983\n",
            "  Epoch 13 / 50\n",
            "    Training Loss: 0.661\tValidation Loss: 0.660\tTime: 8.448822259902954\n",
            "  Epoch 14 / 50\n",
            "    Training Loss: 0.661\tValidation Loss: 0.658\tTime: 8.652814388275146\n",
            "  Epoch 15 / 50\n",
            "    Training Loss: 0.659\tValidation Loss: 0.656\tTime: 8.87192440032959\n",
            "  Epoch 16 / 50\n",
            "    Training Loss: 0.659\tValidation Loss: 0.654\tTime: 8.558831691741943\n",
            "  Epoch 17 / 50\n",
            "    Training Loss: 0.656\tValidation Loss: 0.652\tTime: 8.473329544067383\n",
            "  Epoch 18 / 50\n",
            "    Training Loss: 0.651\tValidation Loss: 0.650\tTime: 8.45198130607605\n",
            "  Epoch 19 / 50\n",
            "    Training Loss: 0.653\tValidation Loss: 0.647\tTime: 8.644300937652588\n",
            "  Epoch 20 / 50\n",
            "    Training Loss: 0.646\tValidation Loss: 0.646\tTime: 8.44179081916809\n",
            "  Epoch 21 / 50\n",
            "    Training Loss: 0.647\tValidation Loss: 0.643\tTime: 8.45533299446106\n",
            "  Epoch 22 / 50\n",
            "    Training Loss: 0.641\tValidation Loss: 0.640\tTime: 8.440157651901245\n",
            "  Epoch 23 / 50\n",
            "    Training Loss: 0.646\tValidation Loss: 0.640\tTime: 6.061263799667358\n",
            "  Epoch 24 / 50\n",
            "    Training Loss: 0.636\tValidation Loss: 0.637\tTime: 8.89977478981018\n",
            "  Epoch 25 / 50\n",
            "    Training Loss: 0.634\tValidation Loss: 0.632\tTime: 9.09059739112854\n",
            "  Epoch 26 / 50\n",
            "    Training Loss: 0.633\tValidation Loss: 0.630\tTime: 9.00130558013916\n",
            "  Epoch 27 / 50\n",
            "    Training Loss: 0.631\tValidation Loss: 0.634\tTime: 6.297577619552612\n",
            "  Epoch 28 / 50\n",
            "    Training Loss: 0.623\tValidation Loss: 0.628\tTime: 9.037415742874146\n",
            "  Epoch 29 / 50\n",
            "    Training Loss: 0.621\tValidation Loss: 0.623\tTime: 9.33297848701477\n",
            "  Epoch 30 / 50\n",
            "    Training Loss: 0.621\tValidation Loss: 0.619\tTime: 9.147388696670532\n",
            "  Epoch 31 / 50\n",
            "    Training Loss: 0.619\tValidation Loss: 0.617\tTime: 9.227203845977783\n",
            "  Epoch 32 / 50\n",
            "    Training Loss: 0.619\tValidation Loss: 0.612\tTime: 9.2609543800354\n",
            "  Epoch 33 / 50\n",
            "    Training Loss: 0.618\tValidation Loss: 0.612\tTime: 9.240372896194458\n",
            "  Epoch 34 / 50\n",
            "    Training Loss: 0.614\tValidation Loss: 0.609\tTime: 9.017644166946411\n",
            "  Epoch 35 / 50\n",
            "    Training Loss: 0.613\tValidation Loss: 0.607\tTime: 9.225027799606323\n",
            "  Epoch 36 / 50\n",
            "    Training Loss: 0.609\tValidation Loss: 0.601\tTime: 8.945260763168335\n",
            "  Epoch 37 / 50\n",
            "    Training Loss: 0.604\tValidation Loss: 0.599\tTime: 8.707176923751831\n",
            "  Epoch 38 / 50\n",
            "    Training Loss: 0.601\tValidation Loss: 0.596\tTime: 8.822067022323608\n",
            "  Epoch 39 / 50\n",
            "    Training Loss: 0.597\tValidation Loss: 0.605\tTime: 6.319161891937256\n",
            "  Epoch 40 / 50\n",
            "    Training Loss: 0.594\tValidation Loss: 0.592\tTime: 8.675277709960938\n",
            "  Epoch 41 / 50\n",
            "    Training Loss: 0.596\tValidation Loss: 0.587\tTime: 8.870376348495483\n",
            "  Epoch 42 / 50\n",
            "    Training Loss: 0.587\tValidation Loss: 0.584\tTime: 8.895925998687744\n",
            "  Epoch 43 / 50\n",
            "    Training Loss: 0.587\tValidation Loss: 0.588\tTime: 6.146770000457764\n",
            "  Epoch 44 / 50\n",
            "    Training Loss: 0.581\tValidation Loss: 0.579\tTime: 8.767337560653687\n",
            "  Epoch 45 / 50\n",
            "    Training Loss: 0.584\tValidation Loss: 0.576\tTime: 8.682386875152588\n",
            "  Epoch 46 / 50\n",
            "    Training Loss: 0.577\tValidation Loss: 0.576\tTime: 8.627701044082642\n",
            "  Epoch 47 / 50\n",
            "    Training Loss: 0.578\tValidation Loss: 0.570\tTime: 8.629688739776611\n",
            "  Epoch 48 / 50\n",
            "    Training Loss: 0.565\tValidation Loss: 0.569\tTime: 8.667803049087524\n",
            "  Epoch 49 / 50\n",
            "    Training Loss: 0.572\tValidation Loss: 0.564\tTime: 8.529785871505737\n",
            "  Epoch 50 / 50\n",
            "    Training Loss: 0.565\tValidation Loss: 0.562\tTime: 8.436175346374512\n",
            "Saving model 15\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 16 / 25\n",
            "  Epoch 1 / 50\n",
            "    Training Loss: 0.692\tValidation Loss: 0.688\tTime: 21.782610177993774\n",
            "  Epoch 2 / 50\n",
            "    Training Loss: 0.689\tValidation Loss: 0.684\tTime: 21.691760540008545\n",
            "  Epoch 3 / 50\n",
            "    Training Loss: 0.685\tValidation Loss: 0.680\tTime: 22.08915424346924\n",
            "  Epoch 4 / 50\n",
            "    Training Loss: 0.684\tValidation Loss: 0.678\tTime: 22.06125569343567\n",
            "  Epoch 5 / 50\n",
            "    Training Loss: 0.681\tValidation Loss: 0.679\tTime: 17.64996337890625\n",
            "  Epoch 6 / 50\n",
            "    Training Loss: 0.678\tValidation Loss: 0.672\tTime: 21.872788906097412\n",
            "  Epoch 7 / 50\n",
            "    Training Loss: 0.676\tValidation Loss: 0.669\tTime: 21.909536123275757\n",
            "  Epoch 8 / 50\n",
            "    Training Loss: 0.673\tValidation Loss: 0.668\tTime: 21.88409996032715\n",
            "  Epoch 9 / 50\n",
            "    Training Loss: 0.670\tValidation Loss: 0.664\tTime: 21.715547561645508\n",
            "  Epoch 10 / 50\n",
            "    Training Loss: 0.664\tValidation Loss: 0.664\tTime: 21.456770181655884\n",
            "  Epoch 11 / 50\n",
            "    Training Loss: 0.667\tValidation Loss: 0.661\tTime: 21.383737325668335\n",
            "  Epoch 12 / 50\n",
            "    Training Loss: 0.667\tValidation Loss: 0.659\tTime: 21.788596153259277\n",
            "  Epoch 13 / 50\n",
            "    Training Loss: 0.662\tValidation Loss: 0.656\tTime: 22.228750467300415\n",
            "  Epoch 14 / 50\n",
            "    Training Loss: 0.660\tValidation Loss: 0.659\tTime: 17.52486753463745\n",
            "  Epoch 15 / 50\n",
            "    Training Loss: 0.654\tValidation Loss: 0.656\tTime: 22.098352670669556\n",
            "  Epoch 16 / 50\n",
            "    Training Loss: 0.651\tValidation Loss: 0.646\tTime: 22.19236969947815\n",
            "  Epoch 17 / 50\n",
            "    Training Loss: 0.653\tValidation Loss: 0.648\tTime: 17.813727855682373\n",
            "  Epoch 18 / 50\n",
            "    Training Loss: 0.650\tValidation Loss: 0.650\tTime: 17.192625999450684\n",
            "  Epoch 19 / 50\n",
            "    Training Loss: 0.647\tValidation Loss: 0.646\tTime: 21.2341730594635\n",
            "  Epoch 20 / 50\n",
            "    Training Loss: 0.641\tValidation Loss: 0.638\tTime: 21.482969999313354\n",
            "  Epoch 21 / 50\n",
            "    Training Loss: 0.644\tValidation Loss: 0.636\tTime: 21.52717161178589\n",
            "  Epoch 22 / 50\n",
            "    Training Loss: 0.637\tValidation Loss: 0.642\tTime: 16.947367668151855\n",
            "  Epoch 23 / 50\n",
            "    Training Loss: 0.631\tValidation Loss: 0.634\tTime: 20.844635248184204\n",
            "  Epoch 24 / 50\n",
            "    Training Loss: 0.631\tValidation Loss: 0.633\tTime: 22.210137605667114\n",
            "  Epoch 25 / 50\n",
            "    Training Loss: 0.631\tValidation Loss: 0.629\tTime: 21.99091672897339\n",
            "  Epoch 26 / 50\n",
            "    Training Loss: 0.626\tValidation Loss: 0.623\tTime: 21.756457090377808\n",
            "  Epoch 27 / 50\n",
            "    Training Loss: 0.633\tValidation Loss: 0.633\tTime: 17.794456958770752\n",
            "  Epoch 28 / 50\n",
            "    Training Loss: 0.617\tValidation Loss: 0.616\tTime: 22.008185386657715\n",
            "  Epoch 29 / 50\n",
            "    Training Loss: 0.619\tValidation Loss: 0.621\tTime: 17.778080463409424\n",
            "  Epoch 30 / 50\n",
            "    Training Loss: 0.611\tValidation Loss: 0.612\tTime: 21.41123628616333\n",
            "  Epoch 31 / 50\n",
            "    Training Loss: 0.615\tValidation Loss: 0.610\tTime: 21.468975067138672\n",
            "  Epoch 32 / 50\n",
            "    Training Loss: 0.612\tValidation Loss: 0.613\tTime: 17.33201789855957\n",
            "  Epoch 33 / 50\n",
            "    Training Loss: 0.612\tValidation Loss: 0.609\tTime: 21.162222623825073\n",
            "  Epoch 34 / 50\n",
            "    Training Loss: 0.608\tValidation Loss: 0.598\tTime: 21.134441137313843\n",
            "  Epoch 35 / 50\n",
            "    Training Loss: 0.608\tValidation Loss: 0.611\tTime: 17.213120698928833\n",
            "  Epoch 36 / 50\n",
            "    Training Loss: 0.602\tValidation Loss: 0.594\tTime: 22.30765962600708\n",
            "  Epoch 37 / 50\n",
            "    Training Loss: 0.599\tValidation Loss: 0.592\tTime: 22.036781549453735\n",
            "  Epoch 38 / 50\n",
            "    Training Loss: 0.594\tValidation Loss: 0.587\tTime: 22.118353843688965\n",
            "  Epoch 39 / 50\n",
            "    Training Loss: 0.598\tValidation Loss: 0.597\tTime: 17.505252838134766\n",
            "  Epoch 40 / 50\n",
            "    Training Loss: 0.588\tValidation Loss: 0.584\tTime: 21.924976110458374\n",
            "  Epoch 41 / 50\n",
            "    Training Loss: 0.585\tValidation Loss: 0.578\tTime: 21.871474742889404\n",
            "  Epoch 42 / 50\n",
            "    Training Loss: 0.582\tValidation Loss: 0.576\tTime: 21.868188858032227\n",
            "  Epoch 43 / 50\n",
            "    Training Loss: 0.584\tValidation Loss: 0.570\tTime: 21.083640575408936\n",
            "  Epoch 44 / 50\n",
            "    Training Loss: 0.583\tValidation Loss: 0.583\tTime: 17.434702396392822\n",
            "  Epoch 45 / 50\n",
            "    Training Loss: 0.576\tValidation Loss: 0.566\tTime: 21.677518844604492\n",
            "  Epoch 46 / 50\n",
            "    Training Loss: 0.574\tValidation Loss: 0.562\tTime: 21.169246673583984\n",
            "  Epoch 47 / 50\n",
            "    Training Loss: 0.572\tValidation Loss: 0.560\tTime: 21.815446853637695\n",
            "  Epoch 48 / 50\n",
            "    Training Loss: 0.569\tValidation Loss: 0.561\tTime: 17.60239028930664\n",
            "  Epoch 49 / 50\n",
            "    Training Loss: 0.568\tValidation Loss: 0.564\tTime: 17.319418907165527\n",
            "  Epoch 50 / 50\n",
            "    Training Loss: 0.559\tValidation Loss: 0.549\tTime: 21.977673053741455\n",
            "Saving model 16\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 17 / 25\n",
            "  Epoch 1 / 50\n",
            "    Training Loss: 0.669\tValidation Loss: 0.648\tTime: 7.385808229446411\n",
            "  Epoch 2 / 50\n",
            "    Training Loss: 0.637\tValidation Loss: 0.617\tTime: 7.091017961502075\n",
            "  Epoch 3 / 50\n",
            "    Training Loss: 0.613\tValidation Loss: 0.591\tTime: 6.975678443908691\n",
            "  Epoch 4 / 50\n",
            "    Training Loss: 0.590\tValidation Loss: 0.568\tTime: 6.888998746871948\n",
            "  Epoch 5 / 50\n",
            "    Training Loss: 0.566\tValidation Loss: 0.546\tTime: 7.087574243545532\n",
            "  Epoch 6 / 50\n",
            "    Training Loss: 0.539\tValidation Loss: 0.531\tTime: 6.6813719272613525\n",
            "  Epoch 7 / 50\n",
            "    Training Loss: 0.529\tValidation Loss: 0.515\tTime: 6.6653687953948975\n",
            "  Epoch 8 / 50\n",
            "    Training Loss: 0.502\tValidation Loss: 0.488\tTime: 6.720165491104126\n",
            "  Epoch 9 / 50\n",
            "    Training Loss: 0.488\tValidation Loss: 0.473\tTime: 6.660141468048096\n",
            "  Epoch 10 / 50\n",
            "    Training Loss: 0.475\tValidation Loss: 0.456\tTime: 6.8046770095825195\n",
            "  Epoch 11 / 50\n",
            "    Training Loss: 0.457\tValidation Loss: 0.448\tTime: 6.704731702804565\n",
            "  Epoch 12 / 50\n",
            "    Training Loss: 0.444\tValidation Loss: 0.435\tTime: 6.637927770614624\n",
            "  Epoch 13 / 50\n",
            "    Training Loss: 0.431\tValidation Loss: 0.415\tTime: 6.830469846725464\n",
            "  Epoch 14 / 50\n",
            "    Training Loss: 0.428\tValidation Loss: 0.409\tTime: 6.610087633132935\n",
            "  Epoch 15 / 50\n",
            "    Training Loss: 0.408\tValidation Loss: 0.394\tTime: 6.696924209594727\n",
            "  Epoch 16 / 50\n",
            "    Training Loss: 0.395\tValidation Loss: 0.384\tTime: 6.835158109664917\n",
            "  Epoch 17 / 50\n",
            "    Training Loss: 0.388\tValidation Loss: 0.373\tTime: 6.7373247146606445\n",
            "  Epoch 18 / 50\n",
            "    Training Loss: 0.385\tValidation Loss: 0.364\tTime: 6.663156509399414\n",
            "  Epoch 19 / 50\n",
            "    Training Loss: 0.360\tValidation Loss: 0.361\tTime: 6.558608531951904\n",
            "  Epoch 20 / 50\n",
            "    Training Loss: 0.360\tValidation Loss: 0.351\tTime: 6.4623003005981445\n",
            "  Epoch 21 / 50\n",
            "    Training Loss: 0.353\tValidation Loss: 0.343\tTime: 6.501144647598267\n",
            "  Epoch 22 / 50\n",
            "    Training Loss: 0.347\tValidation Loss: 0.332\tTime: 6.553473711013794\n",
            "  Epoch 23 / 50\n",
            "    Training Loss: 0.340\tValidation Loss: 0.325\tTime: 6.592307806015015\n",
            "  Epoch 24 / 50\n",
            "    Training Loss: 0.321\tValidation Loss: 0.319\tTime: 6.961692571640015\n",
            "  Epoch 25 / 50\n",
            "    Training Loss: 0.322\tValidation Loss: 0.313\tTime: 7.023428440093994\n",
            "  Epoch 26 / 50\n",
            "    Training Loss: 0.308\tValidation Loss: 0.308\tTime: 7.074868440628052\n",
            "  Epoch 27 / 50\n",
            "    Training Loss: 0.310\tValidation Loss: 0.301\tTime: 7.043245792388916\n",
            "  Epoch 28 / 50\n",
            "    Training Loss: 0.298\tValidation Loss: 0.295\tTime: 6.892398357391357\n",
            "  Epoch 29 / 50\n",
            "    Training Loss: 0.292\tValidation Loss: 0.294\tTime: 6.821418046951294\n",
            "  Epoch 30 / 50\n",
            "    Training Loss: 0.290\tValidation Loss: 0.286\tTime: 6.890168905258179\n",
            "  Epoch 31 / 50\n",
            "    Training Loss: 0.290\tValidation Loss: 0.282\tTime: 6.986644744873047\n",
            "  Epoch 32 / 50\n",
            "    Training Loss: 0.274\tValidation Loss: 0.277\tTime: 6.871899843215942\n",
            "  Epoch 33 / 50\n",
            "    Training Loss: 0.276\tValidation Loss: 0.273\tTime: 7.231703519821167\n",
            "  Epoch 34 / 50\n",
            "    Training Loss: 0.276\tValidation Loss: 0.267\tTime: 7.292834043502808\n",
            "  Epoch 35 / 50\n",
            "    Training Loss: 0.269\tValidation Loss: 0.265\tTime: 7.160846948623657\n",
            "  Epoch 36 / 50\n",
            "    Training Loss: 0.254\tValidation Loss: 0.260\tTime: 7.217370271682739\n",
            "  Epoch 37 / 50\n",
            "    Training Loss: 0.254\tValidation Loss: 0.263\tTime: 6.269105911254883\n",
            "  Epoch 38 / 50\n",
            "    Training Loss: 0.250\tValidation Loss: 0.255\tTime: 7.215379953384399\n",
            "  Epoch 39 / 50\n",
            "    Training Loss: 0.251\tValidation Loss: 0.249\tTime: 7.071056127548218\n",
            "  Epoch 40 / 50\n",
            "    Training Loss: 0.244\tValidation Loss: 0.246\tTime: 7.021809339523315\n",
            "  Epoch 41 / 50\n",
            "    Training Loss: 0.239\tValidation Loss: 0.244\tTime: 6.806837558746338\n",
            "  Epoch 42 / 50\n",
            "    Training Loss: 0.235\tValidation Loss: 0.241\tTime: 6.599430322647095\n",
            "  Epoch 43 / 50\n",
            "    Training Loss: 0.233\tValidation Loss: 0.239\tTime: 6.7442004680633545\n",
            "  Epoch 44 / 50\n",
            "    Training Loss: 0.229\tValidation Loss: 0.237\tTime: 6.7728540897369385\n",
            "  Epoch 45 / 50\n",
            "    Training Loss: 0.227\tValidation Loss: 0.232\tTime: 6.84062647819519\n",
            "  Epoch 46 / 50\n",
            "    Training Loss: 0.230\tValidation Loss: 0.230\tTime: 6.706622123718262\n",
            "  Epoch 47 / 50\n",
            "    Training Loss: 0.221\tValidation Loss: 0.229\tTime: 6.735799551010132\n",
            "  Epoch 48 / 50\n",
            "    Training Loss: 0.222\tValidation Loss: 0.241\tTime: 6.020012378692627\n",
            "  Epoch 49 / 50\n",
            "    Training Loss: 0.225\tValidation Loss: 0.223\tTime: -0.9227197170257568\n",
            "  Epoch 50 / 50\n",
            "    Training Loss: 0.224\tValidation Loss: 0.222\tTime: 6.080840826034546\n",
            "Saving model 17\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment: 18 / 25\n",
            "  Epoch 1 / 50\n",
            "    Training Loss: 0.690\tValidation Loss: 0.684\tTime: 6.995632648468018\n",
            "  Epoch 2 / 50\n",
            "    Training Loss: 0.681\tValidation Loss: 0.677\tTime: 6.93867564201355\n",
            "  Epoch 3 / 50\n",
            "    Training Loss: 0.669\tValidation Loss: 0.668\tTime: 6.9481682777404785\n",
            "  Epoch 4 / 50\n",
            "    Training Loss: 0.657\tValidation Loss: 0.650\tTime: 6.839345216751099\n",
            "  Epoch 5 / 50\n",
            "    Training Loss: 0.637\tValidation Loss: 0.629\tTime: 6.813249826431274\n",
            "  Epoch 6 / 50\n",
            "    Training Loss: 0.611\tValidation Loss: 0.600\tTime: 6.836756944656372\n",
            "  Epoch 7 / 50\n",
            "    Training Loss: 0.581\tValidation Loss: 0.566\tTime: 6.894240617752075\n",
            "  Epoch 8 / 50\n",
            "    Training Loss: 0.540\tValidation Loss: 0.525\tTime: 7.098673105239868\n",
            "  Epoch 9 / 50\n",
            "    Training Loss: 0.496\tValidation Loss: 0.483\tTime: 7.272745132446289\n",
            "  Epoch 10 / 50\n",
            "    Training Loss: 0.450\tValidation Loss: 0.442\tTime: 7.099940538406372\n",
            "  Epoch 11 / 50\n",
            "    Training Loss: 0.417\tValidation Loss: 0.400\tTime: 7.272067070007324\n",
            "  Epoch 12 / 50\n",
            "    Training Loss: 0.373\tValidation Loss: 0.358\tTime: 7.239888906478882\n",
            "  Epoch 13 / 50\n",
            "    Training Loss: 0.339\tValidation Loss: 0.324\tTime: 7.052207946777344\n",
            "  Epoch 14 / 50\n",
            "    Training Loss: 0.307\tValidation Loss: 0.297\tTime: 7.218174695968628\n",
            "  Epoch 15 / 50\n",
            "    Training Loss: 0.274\tValidation Loss: 0.271\tTime: 7.249993324279785\n",
            "  Epoch 16 / 50\n",
            "    Training Loss: 0.258\tValidation Loss: 0.250\tTime: 7.113763809204102\n",
            "  Epoch 17 / 50\n",
            "    Training Loss: 0.256\tValidation Loss: 0.233\tTime: 7.435104608535767\n",
            "  Epoch 18 / 50\n",
            "    Training Loss: 0.231\tValidation Loss: 0.216\tTime: 7.232738971710205\n",
            "  Epoch 19 / 50\n",
            "    Training Loss: 0.215\tValidation Loss: 0.206\tTime: 7.35098671913147\n",
            "  Epoch 20 / 50\n",
            "    Training Loss: 0.202\tValidation Loss: 0.196\tTime: 7.435877561569214\n",
            "  Epoch 21 / 50\n",
            "    Training Loss: 0.203\tValidation Loss: 0.191\tTime: 7.1502203941345215\n",
            "  Epoch 22 / 50\n",
            "    Training Loss: 0.193\tValidation Loss: 0.181\tTime: 7.238332986831665\n",
            "  Epoch 23 / 50\n",
            "    Training Loss: 0.188\tValidation Loss: 0.182\tTime: 6.305584907531738\n",
            "  Epoch 24 / 50\n",
            "    Training Loss: 0.180\tValidation Loss: 0.185\tTime: 6.496088981628418\n",
            "  Epoch 25 / 50\n",
            "    Training Loss: 0.189\tValidation Loss: 0.174\tTime: 7.007483959197998\n",
            "  Epoch 26 / 50\n",
            "    Training Loss: 0.169\tValidation Loss: 0.177\tTime: 6.024510622024536\n",
            "  Epoch 27 / 50\n",
            "    Training Loss: 0.186\tValidation Loss: 0.170\tTime: 7.089939832687378\n",
            "  Epoch 28 / 50\n",
            "    Training Loss: 0.173\tValidation Loss: 0.164\tTime: 6.913244009017944\n",
            "  Epoch 29 / 50\n",
            "    Training Loss: 0.170\tValidation Loss: 0.167\tTime: 6.145338296890259\n",
            "  Epoch 30 / 50\n",
            "    Training Loss: 0.169\tValidation Loss: 0.167\tTime: 6.207635164260864\n",
            "  Epoch 31 / 50\n",
            "    Training Loss: 0.168\tValidation Loss: 0.166\tTime: 6.0461647510528564\n",
            "  Epoch 32 / 50\n",
            "    Training Loss: 0.145\tValidation Loss: 0.165\tTime: 6.211497783660889\n",
            "  Epoch 33 / 50\n",
            "    Training Loss: 0.163\tValidation Loss: 0.164\tTime: 6.9102783203125\n",
            "  Epoch 34 / 50\n",
            "    Training Loss: 0.169\tValidation Loss: 0.152\tTime: 6.962664842605591\n",
            "  Epoch 35 / 50\n",
            "    Training Loss: 0.171\tValidation Loss: 0.153\tTime: 6.137481451034546\n",
            "  Epoch 36 / 50\n",
            "    Training Loss: 0.148\tValidation Loss: 0.178\tTime: 5.964558124542236\n",
            "  Epoch 37 / 50\n",
            "    Training Loss: 0.152\tValidation Loss: 0.162\tTime: 6.083695650100708\n",
            "  Epoch 38 / 50\n",
            "    Training Loss: 0.159\tValidation Loss: 0.168\tTime: 6.07009220123291\n",
            "  Epoch 39 / 50\n",
            "    Training Loss: 0.179\tValidation Loss: 0.187\tTime: 5.976399660110474\n",
            "  Epoch 40 / 50\n",
            "    Training Loss: 0.178\tValidation Loss: 0.151\tTime: 6.772993803024292\n",
            "  Epoch 41 / 50\n",
            "    Training Loss: 0.157\tValidation Loss: 0.166\tTime: 5.887695074081421\n",
            "  Epoch 42 / 50\n",
            "    Training Loss: 0.160\tValidation Loss: 0.146\tTime: 6.781905174255371\n",
            "  Epoch 43 / 50\n",
            "    Training Loss: 0.158\tValidation Loss: 0.202\tTime: 5.906234502792358\n",
            "  Epoch 44 / 50\n",
            "    Training Loss: 0.160\tValidation Loss: 0.176\tTime: 6.338341236114502\n",
            "  Epoch 45 / 50\n",
            "    Training Loss: 0.177\tValidation Loss: 0.168\tTime: 6.19505500793457\n",
            "  Epoch 46 / 50\n",
            "    Training Loss: 0.192\tValidation Loss: 0.167\tTime: 6.4012532234191895\n",
            "  Epoch 47 / 50\n",
            "    Training Loss: 0.145\tValidation Loss: 0.177\tTime: 6.461909294128418\n",
            "  Epoch 48 / 50\n",
            "    Training Loss: 0.159\tValidation Loss: 0.182\tTime: 6.201204061508179\n",
            "  Epoch 49 / 50\n",
            "    Training Loss: 0.160\tValidation Loss: 0.175\tTime: 6.393182754516602\n",
            "  Epoch 50 / 50\n",
            "    Training Loss: 0.157\tValidation Loss: 0.163\tTime: 6.209712982177734\n",
            "Saving model 18\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment: 19 / 25\n",
            "  Epoch 1 / 50\n",
            "    Training Loss: 0.688\tValidation Loss: 0.681\tTime: 19.464065074920654\n",
            "  Epoch 2 / 50\n",
            "    Training Loss: 0.674\tValidation Loss: 0.671\tTime: 19.81654930114746\n",
            "  Epoch 3 / 50\n",
            "    Training Loss: 0.656\tValidation Loss: 0.647\tTime: 19.455076456069946\n",
            "  Epoch 4 / 50\n",
            "    Training Loss: 0.631\tValidation Loss: 0.622\tTime: 19.210510730743408\n",
            "  Epoch 5 / 50\n",
            "    Training Loss: 0.600\tValidation Loss: 0.596\tTime: 19.18463921546936\n",
            "  Epoch 6 / 50\n",
            "    Training Loss: 0.566\tValidation Loss: 0.560\tTime: 19.25623083114624\n",
            "  Epoch 7 / 50\n",
            "    Training Loss: 0.520\tValidation Loss: 0.516\tTime: 19.173531770706177\n",
            "  Epoch 8 / 50\n",
            "    Training Loss: 0.483\tValidation Loss: 0.479\tTime: 18.9789080619812\n",
            "  Epoch 9 / 50\n",
            "    Training Loss: 0.435\tValidation Loss: 0.435\tTime: 19.106879711151123\n",
            "  Epoch 10 / 50\n",
            "    Training Loss: 0.402\tValidation Loss: 0.397\tTime: 19.09770154953003\n",
            "  Epoch 11 / 50\n",
            "    Training Loss: 0.357\tValidation Loss: 0.363\tTime: 19.758169412612915\n",
            "  Epoch 12 / 50\n",
            "    Training Loss: 0.321\tValidation Loss: 0.335\tTime: 19.80607509613037\n",
            "  Epoch 13 / 50\n",
            "    Training Loss: 0.297\tValidation Loss: 0.297\tTime: 19.36233425140381\n",
            "  Epoch 14 / 50\n",
            "    Training Loss: 0.277\tValidation Loss: 0.274\tTime: 19.767005920410156\n",
            "  Epoch 15 / 50\n",
            "    Training Loss: 0.256\tValidation Loss: 0.254\tTime: 19.48452377319336\n",
            "  Epoch 16 / 50\n",
            "    Training Loss: 0.244\tValidation Loss: 0.247\tTime: 20.065394401550293\n",
            "  Epoch 17 / 50\n",
            "    Training Loss: 0.228\tValidation Loss: 0.226\tTime: 19.513526916503906\n",
            "  Epoch 18 / 50\n",
            "    Training Loss: 0.213\tValidation Loss: 0.215\tTime: 19.430848121643066\n",
            "  Epoch 19 / 50\n",
            "    Training Loss: 0.196\tValidation Loss: 0.212\tTime: 19.499671697616577\n",
            "  Epoch 20 / 50\n",
            "    Training Loss: 0.224\tValidation Loss: 0.202\tTime: 19.37061905860901\n",
            "  Epoch 21 / 50\n",
            "    Training Loss: 0.174\tValidation Loss: 0.193\tTime: 19.13895320892334\n",
            "  Epoch 22 / 50\n",
            "    Training Loss: 0.193\tValidation Loss: 0.200\tTime: 17.093618869781494\n",
            "  Epoch 23 / 50\n",
            "    Training Loss: 0.190\tValidation Loss: 0.183\tTime: 19.392393350601196\n",
            "  Epoch 24 / 50\n",
            "    Training Loss: 0.182\tValidation Loss: 0.178\tTime: 19.55874252319336\n",
            "  Epoch 25 / 50\n",
            "    Training Loss: 0.181\tValidation Loss: 0.176\tTime: 19.39453411102295\n",
            "  Epoch 26 / 50\n",
            "    Training Loss: 0.177\tValidation Loss: 0.170\tTime: 19.912692546844482\n",
            "  Epoch 27 / 50\n",
            "    Training Loss: 0.181\tValidation Loss: 0.166\tTime: 19.909440994262695\n",
            "  Epoch 28 / 50\n",
            "    Training Loss: 0.166\tValidation Loss: 0.165\tTime: 19.80274486541748\n",
            "  Epoch 29 / 50\n",
            "    Training Loss: 0.154\tValidation Loss: 0.160\tTime: 19.717830896377563\n",
            "  Epoch 30 / 50\n",
            "    Training Loss: 0.160\tValidation Loss: 0.161\tTime: 17.27779269218445\n",
            "  Epoch 31 / 50\n",
            "    Training Loss: 0.151\tValidation Loss: 0.167\tTime: 17.019529342651367\n",
            "  Epoch 32 / 50\n",
            "    Training Loss: 0.158\tValidation Loss: 0.155\tTime: 19.14482307434082\n",
            "  Epoch 33 / 50\n",
            "    Training Loss: 0.143\tValidation Loss: 0.158\tTime: 17.08206534385681\n",
            "  Epoch 34 / 50\n",
            "    Training Loss: 0.143\tValidation Loss: 0.156\tTime: 16.916577577590942\n",
            "  Epoch 35 / 50\n",
            "    Training Loss: 0.176\tValidation Loss: 0.151\tTime: 18.891526699066162\n",
            "  Epoch 36 / 50\n",
            "    Training Loss: 0.171\tValidation Loss: 0.151\tTime: 19.557695388793945\n",
            "  Epoch 37 / 50\n",
            "    Training Loss: 0.138\tValidation Loss: 0.154\tTime: 17.458626985549927\n",
            "  Epoch 38 / 50\n",
            "    Training Loss: 0.140\tValidation Loss: 0.151\tTime: 17.282679319381714\n",
            "  Epoch 39 / 50\n",
            "    Training Loss: 0.158\tValidation Loss: 0.159\tTime: 17.478742599487305\n",
            "  Epoch 40 / 50\n",
            "    Training Loss: 0.164\tValidation Loss: 0.151\tTime: 17.588587999343872\n",
            "  Epoch 41 / 50\n",
            "    Training Loss: 0.164\tValidation Loss: 0.180\tTime: 17.424029111862183\n",
            "  Epoch 42 / 50\n",
            "    Training Loss: 0.131\tValidation Loss: 0.173\tTime: 17.436617851257324\n",
            "  Epoch 43 / 50\n",
            "    Training Loss: 0.143\tValidation Loss: 0.153\tTime: 17.140945434570312\n",
            "  Epoch 44 / 50\n",
            "    Training Loss: 0.146\tValidation Loss: 0.167\tTime: 17.072712898254395\n",
            "  Epoch 45 / 50\n",
            "    Training Loss: 0.145\tValidation Loss: 0.153\tTime: 17.174221992492676\n",
            "  Epoch 46 / 50\n",
            "    Training Loss: 0.131\tValidation Loss: 0.160\tTime: 16.94668745994568\n",
            "  Epoch 47 / 50\n",
            "    Training Loss: 0.154\tValidation Loss: 0.146\tTime: 19.02794361114502\n",
            "  Epoch 48 / 50\n",
            "    Training Loss: 0.151\tValidation Loss: 0.163\tTime: 17.15262794494629\n",
            "  Epoch 49 / 50\n",
            "    Training Loss: 0.151\tValidation Loss: 0.157\tTime: 16.84783387184143\n",
            "  Epoch 50 / 50\n",
            "    Training Loss: 0.150\tValidation Loss: 0.149\tTime: 17.446717262268066\n",
            "Saving model 19\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 20 / 25\n",
            "  Epoch 1 / 100\n",
            "    Training Loss: 0.691\tValidation Loss: 0.686\tTime: 8.60051703453064\n",
            "  Epoch 2 / 100\n",
            "    Training Loss: 0.688\tValidation Loss: 0.682\tTime: 9.20690393447876\n",
            "  Epoch 3 / 100\n",
            "    Training Loss: 0.683\tValidation Loss: 0.679\tTime: 9.28950834274292\n",
            "  Epoch 4 / 100\n",
            "    Training Loss: 0.682\tValidation Loss: 0.676\tTime: 8.82165241241455\n",
            "  Epoch 5 / 100\n",
            "    Training Loss: 0.681\tValidation Loss: 0.676\tTime: 6.664930820465088\n",
            "  Epoch 6 / 100\n",
            "    Training Loss: 0.680\tValidation Loss: 0.675\tTime: 9.004420042037964\n",
            "  Epoch 7 / 100\n",
            "    Training Loss: 0.677\tValidation Loss: 0.672\tTime: 9.31754755973816\n",
            "  Epoch 8 / 100\n",
            "    Training Loss: 0.673\tValidation Loss: 0.673\tTime: 6.574843883514404\n",
            "  Epoch 9 / 100\n",
            "    Training Loss: 0.672\tValidation Loss: 0.669\tTime: 8.89832091331482\n",
            "  Epoch 10 / 100\n",
            "    Training Loss: 0.669\tValidation Loss: 0.666\tTime: 8.943998098373413\n",
            "  Epoch 11 / 100\n",
            "    Training Loss: 0.666\tValidation Loss: 0.664\tTime: 8.914685726165771\n",
            "  Epoch 12 / 100\n",
            "    Training Loss: 0.664\tValidation Loss: 0.661\tTime: 8.626686096191406\n",
            "  Epoch 13 / 100\n",
            "    Training Loss: 0.664\tValidation Loss: 0.661\tTime: 6.276198863983154\n",
            "  Epoch 14 / 100\n",
            "    Training Loss: 0.660\tValidation Loss: 0.658\tTime: 8.489818096160889\n",
            "  Epoch 15 / 100\n",
            "    Training Loss: 0.659\tValidation Loss: 0.656\tTime: 8.579030752182007\n",
            "  Epoch 16 / 100\n",
            "    Training Loss: 0.655\tValidation Loss: 0.653\tTime: 9.058081150054932\n",
            "  Epoch 17 / 100\n",
            "    Training Loss: 0.656\tValidation Loss: 0.654\tTime: 6.4754040241241455\n",
            "  Epoch 18 / 100\n",
            "    Training Loss: 0.649\tValidation Loss: 0.653\tTime: 6.388009309768677\n",
            "  Epoch 19 / 100\n",
            "    Training Loss: 0.648\tValidation Loss: 0.647\tTime: 8.980398893356323\n",
            "  Epoch 20 / 100\n",
            "    Training Loss: 0.646\tValidation Loss: 0.646\tTime: 8.824882745742798\n",
            "  Epoch 21 / 100\n",
            "    Training Loss: 0.641\tValidation Loss: 0.641\tTime: 8.815768003463745\n",
            "  Epoch 22 / 100\n",
            "    Training Loss: 0.638\tValidation Loss: 0.637\tTime: 8.53035831451416\n",
            "  Epoch 23 / 100\n",
            "    Training Loss: 0.634\tValidation Loss: 0.635\tTime: 8.37828278541565\n",
            "  Epoch 24 / 100\n",
            "    Training Loss: 0.636\tValidation Loss: 0.631\tTime: 8.823725700378418\n",
            "  Epoch 25 / 100\n",
            "    Training Loss: 0.633\tValidation Loss: 0.629\tTime: 8.48400616645813\n",
            "  Epoch 26 / 100\n",
            "    Training Loss: 0.631\tValidation Loss: 0.628\tTime: 9.291690111160278\n",
            "  Epoch 27 / 100\n",
            "    Training Loss: 0.630\tValidation Loss: 0.623\tTime: 9.066299438476562\n",
            "  Epoch 28 / 100\n",
            "    Training Loss: 0.625\tValidation Loss: 0.625\tTime: 6.479138135910034\n",
            "  Epoch 29 / 100\n",
            "    Training Loss: 0.624\tValidation Loss: 0.620\tTime: 8.759434461593628\n",
            "  Epoch 30 / 100\n",
            "    Training Loss: 0.616\tValidation Loss: 0.617\tTime: 9.015433073043823\n",
            "  Epoch 31 / 100\n",
            "    Training Loss: 0.617\tValidation Loss: 0.615\tTime: 8.981914520263672\n",
            "  Epoch 32 / 100\n",
            "    Training Loss: 0.608\tValidation Loss: 0.615\tTime: 6.375208139419556\n",
            "  Epoch 33 / 100\n",
            "    Training Loss: 0.611\tValidation Loss: 0.612\tTime: 8.808426856994629\n",
            "  Epoch 34 / 100\n",
            "    Training Loss: 0.604\tValidation Loss: 0.604\tTime: 8.841897964477539\n",
            "  Epoch 35 / 100\n",
            "    Training Loss: 0.606\tValidation Loss: 0.603\tTime: 9.07034420967102\n",
            "  Epoch 36 / 100\n",
            "    Training Loss: 0.606\tValidation Loss: 0.601\tTime: 9.144349813461304\n",
            "  Epoch 37 / 100\n",
            "    Training Loss: 0.599\tValidation Loss: 0.597\tTime: 9.06830382347107\n",
            "  Epoch 38 / 100\n",
            "    Training Loss: 0.601\tValidation Loss: 0.595\tTime: 9.000011682510376\n",
            "  Epoch 39 / 100\n",
            "    Training Loss: 0.594\tValidation Loss: 0.591\tTime: 9.207326173782349\n",
            "  Epoch 40 / 100\n",
            "    Training Loss: 0.597\tValidation Loss: 0.589\tTime: 8.897520065307617\n",
            "  Epoch 41 / 100\n",
            "    Training Loss: 0.592\tValidation Loss: 0.585\tTime: 8.904654026031494\n",
            "  Epoch 42 / 100\n",
            "    Training Loss: 0.586\tValidation Loss: 0.591\tTime: 6.134119987487793\n",
            "  Epoch 43 / 100\n",
            "    Training Loss: 0.576\tValidation Loss: 0.578\tTime: 8.867854118347168\n",
            "  Epoch 44 / 100\n",
            "    Training Loss: 0.582\tValidation Loss: 0.577\tTime: 8.570047616958618\n",
            "  Epoch 45 / 100\n",
            "    Training Loss: 0.582\tValidation Loss: 0.572\tTime: 8.930862188339233\n",
            "  Epoch 46 / 100\n",
            "    Training Loss: 0.578\tValidation Loss: 0.572\tTime: 6.18208384513855\n",
            "  Epoch 47 / 100\n",
            "    Training Loss: 0.570\tValidation Loss: 0.570\tTime: 8.362571001052856\n",
            "  Epoch 48 / 100\n",
            "    Training Loss: 0.569\tValidation Loss: 0.564\tTime: 8.690205574035645\n",
            "  Epoch 49 / 100\n",
            "    Training Loss: 0.568\tValidation Loss: 0.559\tTime: 8.576843976974487\n",
            "  Epoch 50 / 100\n",
            "    Training Loss: 0.563\tValidation Loss: 0.556\tTime: 8.788991212844849\n",
            "  Epoch 51 / 100\n",
            "    Training Loss: 0.559\tValidation Loss: 0.554\tTime: 8.741560697555542\n",
            "  Epoch 52 / 100\n",
            "    Training Loss: 0.550\tValidation Loss: 0.550\tTime: 8.513679027557373\n",
            "  Epoch 53 / 100\n",
            "    Training Loss: 0.554\tValidation Loss: 0.548\tTime: 8.391399145126343\n",
            "  Epoch 54 / 100\n",
            "    Training Loss: 0.552\tValidation Loss: 0.551\tTime: 6.174582481384277\n",
            "  Epoch 55 / 100\n",
            "    Training Loss: 0.546\tValidation Loss: 0.542\tTime: 9.104699850082397\n",
            "  Epoch 56 / 100\n",
            "    Training Loss: 0.546\tValidation Loss: 0.540\tTime: 9.327898025512695\n",
            "  Epoch 57 / 100\n",
            "    Training Loss: 0.539\tValidation Loss: 0.533\tTime: 9.20263671875\n",
            "  Epoch 58 / 100\n",
            "    Training Loss: 0.532\tValidation Loss: 0.535\tTime: 6.192389011383057\n",
            "  Epoch 59 / 100\n",
            "    Training Loss: 0.538\tValidation Loss: 0.528\tTime: 8.710006952285767\n",
            "  Epoch 60 / 100\n",
            "    Training Loss: 0.528\tValidation Loss: 0.524\tTime: 8.763405561447144\n",
            "  Epoch 61 / 100\n",
            "    Training Loss: 0.518\tValidation Loss: 0.519\tTime: 9.173087120056152\n",
            "  Epoch 62 / 100\n",
            "    Training Loss: 0.528\tValidation Loss: 0.524\tTime: 6.567794322967529\n",
            "  Epoch 63 / 100\n",
            "    Training Loss: 0.525\tValidation Loss: 0.518\tTime: 9.267277717590332\n",
            "  Epoch 64 / 100\n",
            "    Training Loss: 0.526\tValidation Loss: 0.512\tTime: 9.158712387084961\n",
            "  Epoch 65 / 100\n",
            "    Training Loss: 0.514\tValidation Loss: 0.509\tTime: 9.221865892410278\n",
            "  Epoch 66 / 100\n",
            "    Training Loss: 0.512\tValidation Loss: 0.506\tTime: 9.37518835067749\n",
            "  Epoch 67 / 100\n",
            "    Training Loss: 0.519\tValidation Loss: 0.503\tTime: 9.343549251556396\n",
            "  Epoch 68 / 100\n",
            "    Training Loss: 0.505\tValidation Loss: 0.501\tTime: 8.984973669052124\n",
            "  Epoch 69 / 100\n",
            "    Training Loss: 0.501\tValidation Loss: 0.499\tTime: 8.769678115844727\n",
            "  Epoch 70 / 100\n",
            "    Training Loss: 0.511\tValidation Loss: 0.493\tTime: 8.606412172317505\n",
            "  Epoch 71 / 100\n",
            "    Training Loss: 0.503\tValidation Loss: 0.490\tTime: 8.845550060272217\n",
            "  Epoch 72 / 100\n",
            "    Training Loss: 0.495\tValidation Loss: 0.489\tTime: 8.699712753295898\n",
            "  Epoch 73 / 100\n",
            "    Training Loss: 0.490\tValidation Loss: 0.481\tTime: 8.652380228042603\n",
            "  Epoch 74 / 100\n",
            "    Training Loss: 0.489\tValidation Loss: 0.478\tTime: 8.493265867233276\n",
            "  Epoch 75 / 100\n",
            "    Training Loss: 0.495\tValidation Loss: 0.479\tTime: 6.083157062530518\n",
            "  Epoch 76 / 100\n",
            "    Training Loss: 0.477\tValidation Loss: 0.486\tTime: 5.912809371948242\n",
            "  Epoch 77 / 100\n",
            "    Training Loss: 0.486\tValidation Loss: 0.468\tTime: 8.269734144210815\n",
            "  Epoch 78 / 100\n",
            "    Training Loss: 0.478\tValidation Loss: 0.465\tTime: 8.554192781448364\n",
            "  Epoch 79 / 100\n",
            "    Training Loss: 0.485\tValidation Loss: 0.465\tTime: 6.074566841125488\n",
            "  Epoch 80 / 100\n",
            "    Training Loss: 0.473\tValidation Loss: 0.460\tTime: 8.235882759094238\n",
            "  Epoch 81 / 100\n",
            "    Training Loss: 0.474\tValidation Loss: 0.457\tTime: 8.553027153015137\n",
            "  Epoch 82 / 100\n",
            "    Training Loss: 0.470\tValidation Loss: 0.453\tTime: 8.580494165420532\n",
            "  Epoch 83 / 100\n",
            "    Training Loss: 0.462\tValidation Loss: 0.462\tTime: 6.135740280151367\n",
            "  Epoch 84 / 100\n",
            "    Training Loss: 0.454\tValidation Loss: 0.448\tTime: 8.843109607696533\n",
            "  Epoch 85 / 100\n",
            "    Training Loss: 0.458\tValidation Loss: 0.442\tTime: 9.124858856201172\n",
            "  Epoch 86 / 100\n",
            "    Training Loss: 0.453\tValidation Loss: 0.444\tTime: 6.3109235763549805\n",
            "  Epoch 87 / 100\n",
            "    Training Loss: 0.445\tValidation Loss: 0.435\tTime: 8.75402045249939\n",
            "  Epoch 88 / 100\n",
            "    Training Loss: 0.451\tValidation Loss: 0.440\tTime: 6.332535266876221\n",
            "  Epoch 89 / 100\n",
            "    Training Loss: 0.444\tValidation Loss: 0.431\tTime: 8.837557077407837\n",
            "  Epoch 90 / 100\n",
            "    Training Loss: 0.447\tValidation Loss: 0.429\tTime: 8.753863334655762\n",
            "  Epoch 91 / 100\n",
            "    Training Loss: 0.451\tValidation Loss: 0.427\tTime: 9.322612047195435\n",
            "  Epoch 92 / 100\n",
            "    Training Loss: 0.440\tValidation Loss: 0.421\tTime: 9.023348331451416\n",
            "  Epoch 93 / 100\n",
            "    Training Loss: 0.441\tValidation Loss: 0.421\tTime: 9.500313758850098\n",
            "  Epoch 94 / 100\n",
            "    Training Loss: 0.432\tValidation Loss: 0.413\tTime: 9.36903691291809\n",
            "  Epoch 95 / 100\n",
            "    Training Loss: 0.434\tValidation Loss: 0.414\tTime: 6.378387689590454\n",
            "  Epoch 96 / 100\n",
            "    Training Loss: 0.427\tValidation Loss: 0.408\tTime: 8.90569257736206\n",
            "  Epoch 97 / 100\n",
            "    Training Loss: 0.423\tValidation Loss: 0.406\tTime: 8.940388441085815\n",
            "  Epoch 98 / 100\n",
            "    Training Loss: 0.418\tValidation Loss: 0.405\tTime: 8.590036869049072\n",
            "  Epoch 99 / 100\n",
            "    Training Loss: 0.421\tValidation Loss: 0.404\tTime: 8.857954502105713\n",
            "  Epoch 100 / 100\n",
            "    Training Loss: 0.417\tValidation Loss: 0.396\tTime: 8.936703443527222\n",
            "Saving model 20\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 21 / 25\n",
            "  Epoch 1 / 100\n",
            "    Training Loss: 0.692\tValidation Loss: 0.689\tTime: 21.712635040283203\n",
            "  Epoch 2 / 100\n",
            "    Training Loss: 0.690\tValidation Loss: 0.683\tTime: 21.371421098709106\n",
            "  Epoch 3 / 100\n",
            "    Training Loss: 0.684\tValidation Loss: 0.696\tTime: 17.08853054046631\n",
            "  Epoch 4 / 100\n",
            "    Training Loss: 0.683\tValidation Loss: 0.680\tTime: 20.96828579902649\n",
            "  Epoch 5 / 100\n",
            "    Training Loss: 0.679\tValidation Loss: 0.677\tTime: 21.856982469558716\n",
            "  Epoch 6 / 100\n",
            "    Training Loss: 0.675\tValidation Loss: 0.670\tTime: 21.76628017425537\n",
            "  Epoch 7 / 100\n",
            "    Training Loss: 0.672\tValidation Loss: 0.668\tTime: 22.996495723724365\n",
            "  Epoch 8 / 100\n",
            "    Training Loss: 0.675\tValidation Loss: 0.668\tTime: 22.464752197265625\n",
            "  Epoch 9 / 100\n",
            "    Training Loss: 0.673\tValidation Loss: 0.663\tTime: 22.44853448867798\n",
            "  Epoch 10 / 100\n",
            "    Training Loss: 0.667\tValidation Loss: 0.662\tTime: 21.789636611938477\n",
            "  Epoch 11 / 100\n",
            "    Training Loss: 0.664\tValidation Loss: 0.658\tTime: 21.559622764587402\n",
            "  Epoch 12 / 100\n",
            "    Training Loss: 0.660\tValidation Loss: 0.655\tTime: 21.695167779922485\n",
            "  Epoch 13 / 100\n",
            "    Training Loss: 0.659\tValidation Loss: 0.654\tTime: 21.430206060409546\n",
            "  Epoch 14 / 100\n",
            "    Training Loss: 0.656\tValidation Loss: 0.651\tTime: 21.50296425819397\n",
            "  Epoch 15 / 100\n",
            "    Training Loss: 0.656\tValidation Loss: 0.648\tTime: 21.275453090667725\n",
            "  Epoch 16 / 100\n",
            "    Training Loss: 0.653\tValidation Loss: 0.647\tTime: 22.012017726898193\n",
            "  Epoch 17 / 100\n",
            "    Training Loss: 0.647\tValidation Loss: 0.640\tTime: 22.45752191543579\n",
            "  Epoch 18 / 100\n",
            "    Training Loss: 0.645\tValidation Loss: 0.637\tTime: 22.429107427597046\n",
            "  Epoch 19 / 100\n",
            "    Training Loss: 0.642\tValidation Loss: 0.636\tTime: 22.119471073150635\n",
            "  Epoch 20 / 100\n",
            "    Training Loss: 0.644\tValidation Loss: 0.632\tTime: 22.232887506484985\n",
            "  Epoch 21 / 100\n",
            "    Training Loss: 0.633\tValidation Loss: 0.634\tTime: 17.443986177444458\n",
            "  Epoch 22 / 100\n",
            "    Training Loss: 0.633\tValidation Loss: 0.635\tTime: 17.036672592163086\n",
            "  Epoch 23 / 100\n",
            "    Training Loss: 0.633\tValidation Loss: 0.628\tTime: 21.24142026901245\n",
            "  Epoch 24 / 100\n",
            "    Training Loss: 0.627\tValidation Loss: 0.620\tTime: 21.38504910469055\n",
            "  Epoch 25 / 100\n",
            "    Training Loss: 0.626\tValidation Loss: 0.617\tTime: 21.25837731361389\n",
            "  Epoch 26 / 100\n",
            "    Training Loss: 0.620\tValidation Loss: 0.616\tTime: 21.18570041656494\n",
            "  Epoch 27 / 100\n",
            "    Training Loss: 0.619\tValidation Loss: 0.611\tTime: 21.94204044342041\n",
            "  Epoch 28 / 100\n",
            "    Training Loss: 0.615\tValidation Loss: 0.606\tTime: 21.778834581375122\n",
            "  Epoch 29 / 100\n",
            "    Training Loss: 0.613\tValidation Loss: 0.618\tTime: 17.397358417510986\n",
            "  Epoch 30 / 100\n",
            "    Training Loss: 0.612\tValidation Loss: 0.601\tTime: 22.011585474014282\n",
            "  Epoch 31 / 100\n",
            "    Training Loss: 0.603\tValidation Loss: 0.603\tTime: 17.552759885787964\n",
            "  Epoch 32 / 100\n",
            "    Training Loss: 0.609\tValidation Loss: 0.595\tTime: 21.745097160339355\n",
            "  Epoch 33 / 100\n",
            "    Training Loss: 0.600\tValidation Loss: 0.598\tTime: 17.33523178100586\n",
            "  Epoch 34 / 100\n",
            "    Training Loss: 0.596\tValidation Loss: 0.587\tTime: 21.256398916244507\n",
            "  Epoch 35 / 100\n",
            "    Training Loss: 0.595\tValidation Loss: 0.585\tTime: 21.64780282974243\n",
            "  Epoch 36 / 100\n",
            "    Training Loss: 0.588\tValidation Loss: 0.587\tTime: 17.160019159317017\n",
            "  Epoch 37 / 100\n",
            "    Training Loss: 0.588\tValidation Loss: 0.578\tTime: 21.21144700050354\n",
            "  Epoch 38 / 100\n",
            "    Training Loss: 0.587\tValidation Loss: 0.596\tTime: 17.01682138442993\n",
            "  Epoch 39 / 100\n",
            "    Training Loss: 0.587\tValidation Loss: 0.578\tTime: 16.93341565132141\n",
            "  Epoch 40 / 100\n",
            "    Training Loss: 0.583\tValidation Loss: 0.577\tTime: 21.901235103607178\n",
            "  Epoch 41 / 100\n",
            "    Training Loss: 0.582\tValidation Loss: 0.579\tTime: 17.51154136657715\n",
            "  Epoch 42 / 100\n",
            "    Training Loss: 0.579\tValidation Loss: 0.586\tTime: 17.292379140853882\n",
            "  Epoch 43 / 100\n",
            "    Training Loss: 0.577\tValidation Loss: 0.563\tTime: 21.908222436904907\n",
            "  Epoch 44 / 100\n",
            "    Training Loss: 0.570\tValidation Loss: 0.562\tTime: 22.207852840423584\n",
            "  Epoch 45 / 100\n",
            "    Training Loss: 0.574\tValidation Loss: 0.554\tTime: 21.915678024291992\n",
            "  Epoch 46 / 100\n",
            "    Training Loss: 0.569\tValidation Loss: 0.560\tTime: 17.360677242279053\n",
            "  Epoch 47 / 100\n",
            "    Training Loss: 0.566\tValidation Loss: 0.551\tTime: 21.162714958190918\n",
            "  Epoch 48 / 100\n",
            "    Training Loss: 0.559\tValidation Loss: 0.551\tTime: 21.481404781341553\n",
            "  Epoch 49 / 100\n",
            "    Training Loss: 0.561\tValidation Loss: 0.542\tTime: 21.655649662017822\n",
            "  Epoch 50 / 100\n",
            "    Training Loss: 0.556\tValidation Loss: 0.543\tTime: 17.02493381500244\n",
            "  Epoch 51 / 100\n",
            "    Training Loss: 0.549\tValidation Loss: 0.557\tTime: 17.027364253997803\n",
            "  Epoch 52 / 100\n",
            "    Training Loss: 0.556\tValidation Loss: 0.540\tTime: 22.029160022735596\n",
            "  Epoch 53 / 100\n",
            "    Training Loss: 0.553\tValidation Loss: 0.535\tTime: 22.063950300216675\n",
            "  Epoch 54 / 100\n",
            "    Training Loss: 0.551\tValidation Loss: 0.533\tTime: 22.297837257385254\n",
            "  Epoch 55 / 100\n",
            "    Training Loss: 0.548\tValidation Loss: 0.534\tTime: 17.696309328079224\n",
            "  Epoch 56 / 100\n",
            "    Training Loss: 0.540\tValidation Loss: 0.522\tTime: 21.77313733100891\n",
            "  Epoch 57 / 100\n",
            "    Training Loss: 0.542\tValidation Loss: 0.532\tTime: 17.475316047668457\n",
            "  Epoch 58 / 100\n",
            "    Training Loss: 0.549\tValidation Loss: 0.527\tTime: 16.923781156539917\n",
            "  Epoch 59 / 100\n",
            "    Training Loss: 0.539\tValidation Loss: 0.523\tTime: 17.036429166793823\n",
            "  Epoch 60 / 100\n",
            "    Training Loss: 0.534\tValidation Loss: 0.510\tTime: 21.378175973892212\n",
            "  Epoch 61 / 100\n",
            "    Training Loss: 0.531\tValidation Loss: 0.514\tTime: 17.303517818450928\n",
            "  Epoch 62 / 100\n",
            "    Training Loss: 0.524\tValidation Loss: 0.524\tTime: 16.891199588775635\n",
            "  Epoch 63 / 100\n",
            "    Training Loss: 0.521\tValidation Loss: 0.527\tTime: 16.77752947807312\n",
            "  Epoch 64 / 100\n",
            "    Training Loss: 0.527\tValidation Loss: 0.520\tTime: 17.18665385246277\n",
            "  Epoch 65 / 100\n",
            "    Training Loss: 0.534\tValidation Loss: 0.499\tTime: 21.609615802764893\n",
            "  Epoch 66 / 100\n",
            "    Training Loss: 0.521\tValidation Loss: 0.495\tTime: 21.962679624557495\n",
            "  Epoch 67 / 100\n",
            "    Training Loss: 0.529\tValidation Loss: 0.492\tTime: 22.488807439804077\n",
            "  Epoch 68 / 100\n",
            "    Training Loss: 0.523\tValidation Loss: 0.495\tTime: 17.600057125091553\n",
            "  Epoch 69 / 100\n",
            "    Training Loss: 0.508\tValidation Loss: 0.510\tTime: 17.333823680877686\n",
            "  Epoch 70 / 100\n",
            "    Training Loss: 0.524\tValidation Loss: 0.532\tTime: 17.209850072860718\n",
            "  Epoch 71 / 100\n",
            "    Training Loss: 0.510\tValidation Loss: 0.498\tTime: 16.98371982574463\n",
            "  Epoch 72 / 100\n",
            "    Training Loss: 0.507\tValidation Loss: 0.553\tTime: 16.985013723373413\n",
            "  Epoch 73 / 100\n",
            "    Training Loss: 0.502\tValidation Loss: 0.488\tTime: 21.266395330429077\n",
            "  Epoch 74 / 100\n",
            "    Training Loss: 0.510\tValidation Loss: 0.478\tTime: 21.133957386016846\n",
            "  Epoch 75 / 100\n",
            "    Training Loss: 0.503\tValidation Loss: 0.487\tTime: 17.07099413871765\n",
            "  Epoch 76 / 100\n",
            "    Training Loss: 0.501\tValidation Loss: 0.472\tTime: 21.160338163375854\n",
            "  Epoch 77 / 100\n",
            "    Training Loss: 0.492\tValidation Loss: 0.478\tTime: 17.406983613967896\n",
            "  Epoch 78 / 100\n",
            "    Training Loss: 0.485\tValidation Loss: 0.466\tTime: 21.384368896484375\n",
            "  Epoch 79 / 100\n",
            "    Training Loss: 0.499\tValidation Loss: 0.461\tTime: 21.515338897705078\n",
            "  Epoch 80 / 100\n",
            "    Training Loss: 0.492\tValidation Loss: 0.466\tTime: 17.545627117156982\n",
            "  Epoch 81 / 100\n",
            "    Training Loss: 0.493\tValidation Loss: 0.460\tTime: 21.69326663017273\n",
            "  Epoch 82 / 100\n",
            "    Training Loss: 0.491\tValidation Loss: 0.454\tTime: 21.497769594192505\n",
            "  Epoch 83 / 100\n",
            "    Training Loss: 0.472\tValidation Loss: 0.462\tTime: 17.25575590133667\n",
            "  Epoch 84 / 100\n",
            "    Training Loss: 0.489\tValidation Loss: 0.469\tTime: 17.210022449493408\n",
            "  Epoch 85 / 100\n",
            "    Training Loss: 0.482\tValidation Loss: 0.453\tTime: 21.296847343444824\n",
            "  Epoch 86 / 100\n",
            "    Training Loss: 0.489\tValidation Loss: 0.444\tTime: 21.214038133621216\n",
            "  Epoch 87 / 100\n",
            "    Training Loss: 0.479\tValidation Loss: 0.446\tTime: 17.034653425216675\n",
            "  Epoch 88 / 100\n",
            "    Training Loss: 0.465\tValidation Loss: 0.444\tTime: 21.115105628967285\n",
            "  Epoch 89 / 100\n",
            "    Training Loss: 0.470\tValidation Loss: 0.438\tTime: 22.161073207855225\n",
            "  Epoch 90 / 100\n",
            "    Training Loss: 0.488\tValidation Loss: 0.442\tTime: 17.413013219833374\n",
            "  Epoch 91 / 100\n",
            "    Training Loss: 0.476\tValidation Loss: 0.464\tTime: 17.465180158615112\n",
            "  Epoch 92 / 100\n",
            "    Training Loss: 0.479\tValidation Loss: 0.438\tTime: 17.581343412399292\n",
            "  Epoch 93 / 100\n",
            "    Training Loss: 0.476\tValidation Loss: 0.432\tTime: 21.9100923538208\n",
            "  Epoch 94 / 100\n",
            "    Training Loss: 0.466\tValidation Loss: 0.437\tTime: 17.552001237869263\n",
            "  Epoch 95 / 100\n",
            "    Training Loss: 0.470\tValidation Loss: 0.459\tTime: 17.260003328323364\n",
            "  Epoch 96 / 100\n",
            "    Training Loss: 0.466\tValidation Loss: 0.425\tTime: 21.6181743144989\n",
            "  Epoch 97 / 100\n",
            "    Training Loss: 0.460\tValidation Loss: 0.440\tTime: 17.314403533935547\n",
            "  Epoch 98 / 100\n",
            "    Training Loss: 0.465\tValidation Loss: 0.428\tTime: 17.057947635650635\n",
            "  Epoch 99 / 100\n",
            "    Training Loss: 0.468\tValidation Loss: 0.421\tTime: 21.490773677825928\n",
            "  Epoch 100 / 100\n",
            "    Training Loss: 0.467\tValidation Loss: 0.431\tTime: 16.995450258255005\n",
            "Saving model 21\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 22 / 25\n",
            "  Epoch 1 / 100\n",
            "    Training Loss: 0.682\tValidation Loss: 0.656\tTime: 7.0214598178863525\n",
            "  Epoch 2 / 100\n",
            "    Training Loss: 0.643\tValidation Loss: 0.638\tTime: 7.007549524307251\n",
            "  Epoch 3 / 100\n",
            "    Training Loss: 0.610\tValidation Loss: 0.593\tTime: 7.12316632270813\n",
            "  Epoch 4 / 100\n",
            "    Training Loss: 0.589\tValidation Loss: 0.570\tTime: 6.998402833938599\n",
            "  Epoch 5 / 100\n",
            "    Training Loss: 0.563\tValidation Loss: 0.552\tTime: 6.899179458618164\n",
            "  Epoch 6 / 100\n",
            "    Training Loss: 0.546\tValidation Loss: 0.532\tTime: 6.87822699546814\n",
            "  Epoch 7 / 100\n",
            "    Training Loss: 0.519\tValidation Loss: 0.504\tTime: 6.827800512313843\n",
            "  Epoch 8 / 100\n",
            "    Training Loss: 0.508\tValidation Loss: 0.490\tTime: 6.834665775299072\n",
            "  Epoch 9 / 100\n",
            "    Training Loss: 0.482\tValidation Loss: 0.471\tTime: 6.8935346603393555\n",
            "  Epoch 10 / 100\n",
            "    Training Loss: 0.481\tValidation Loss: 0.468\tTime: 6.965471982955933\n",
            "  Epoch 11 / 100\n",
            "    Training Loss: 0.462\tValidation Loss: 0.445\tTime: 7.3809919357299805\n",
            "  Epoch 12 / 100\n",
            "    Training Loss: 0.448\tValidation Loss: 0.429\tTime: 7.007747173309326\n",
            "  Epoch 13 / 100\n",
            "    Training Loss: 0.447\tValidation Loss: 0.424\tTime: 7.045457363128662\n",
            "  Epoch 14 / 100\n",
            "    Training Loss: 0.426\tValidation Loss: 0.406\tTime: 7.080254554748535\n",
            "  Epoch 15 / 100\n",
            "    Training Loss: 0.410\tValidation Loss: 0.397\tTime: 6.914420127868652\n",
            "  Epoch 16 / 100\n",
            "    Training Loss: 0.406\tValidation Loss: 0.391\tTime: 6.946821212768555\n",
            "  Epoch 17 / 100\n",
            "    Training Loss: 0.388\tValidation Loss: 0.376\tTime: 6.851250886917114\n",
            "  Epoch 18 / 100\n",
            "    Training Loss: 0.373\tValidation Loss: 0.367\tTime: 7.102818965911865\n",
            "  Epoch 19 / 100\n",
            "    Training Loss: 0.370\tValidation Loss: 0.363\tTime: 6.948951721191406\n",
            "  Epoch 20 / 100\n",
            "    Training Loss: 0.359\tValidation Loss: 0.351\tTime: 6.577547073364258\n",
            "  Epoch 21 / 100\n",
            "    Training Loss: 0.350\tValidation Loss: 0.344\tTime: 6.644658327102661\n",
            "  Epoch 22 / 100\n",
            "    Training Loss: 0.358\tValidation Loss: 0.339\tTime: 6.725672960281372\n",
            "  Epoch 23 / 100\n",
            "    Training Loss: 0.344\tValidation Loss: 0.329\tTime: 6.769906520843506\n",
            "  Epoch 24 / 100\n",
            "    Training Loss: 0.335\tValidation Loss: 0.322\tTime: 6.769040107727051\n",
            "  Epoch 25 / 100\n",
            "    Training Loss: 0.325\tValidation Loss: 0.327\tTime: 6.0256547927856445\n",
            "  Epoch 26 / 100\n",
            "    Training Loss: 0.321\tValidation Loss: 0.315\tTime: 6.623905897140503\n",
            "  Epoch 27 / 100\n",
            "    Training Loss: 0.313\tValidation Loss: 0.305\tTime: 6.689598798751831\n",
            "  Epoch 28 / 100\n",
            "    Training Loss: 0.303\tValidation Loss: 0.300\tTime: 6.717094421386719\n",
            "  Epoch 29 / 100\n",
            "    Training Loss: 0.297\tValidation Loss: 0.293\tTime: 6.613297939300537\n",
            "  Epoch 30 / 100\n",
            "    Training Loss: 0.295\tValidation Loss: 0.291\tTime: 6.859473705291748\n",
            "  Epoch 31 / 100\n",
            "    Training Loss: 0.289\tValidation Loss: 0.283\tTime: 6.79187536239624\n",
            "  Epoch 32 / 100\n",
            "    Training Loss: 0.284\tValidation Loss: 0.282\tTime: 6.649927616119385\n",
            "  Epoch 33 / 100\n",
            "    Training Loss: 0.269\tValidation Loss: 0.275\tTime: 6.499491930007935\n",
            "  Epoch 34 / 100\n",
            "    Training Loss: 0.270\tValidation Loss: 0.270\tTime: 6.47466254234314\n",
            "  Epoch 35 / 100\n",
            "    Training Loss: 0.269\tValidation Loss: 0.269\tTime: 6.574366092681885\n",
            "  Epoch 36 / 100\n",
            "    Training Loss: 0.264\tValidation Loss: 0.266\tTime: 6.480649471282959\n",
            "  Epoch 37 / 100\n",
            "    Training Loss: 0.258\tValidation Loss: 0.258\tTime: 6.912993669509888\n",
            "  Epoch 38 / 100\n",
            "    Training Loss: 0.258\tValidation Loss: 0.256\tTime: 6.936420679092407\n",
            "  Epoch 39 / 100\n",
            "    Training Loss: 0.249\tValidation Loss: 0.252\tTime: 7.035923957824707\n",
            "  Epoch 40 / 100\n",
            "    Training Loss: 0.249\tValidation Loss: 0.249\tTime: 6.955658197402954\n",
            "  Epoch 41 / 100\n",
            "    Training Loss: 0.249\tValidation Loss: 0.246\tTime: 6.911916255950928\n",
            "  Epoch 42 / 100\n",
            "    Training Loss: 0.242\tValidation Loss: 0.243\tTime: 7.0312159061431885\n",
            "  Epoch 43 / 100\n",
            "    Training Loss: 0.239\tValidation Loss: 0.240\tTime: 6.7153589725494385\n",
            "  Epoch 44 / 100\n",
            "    Training Loss: 0.234\tValidation Loss: 0.238\tTime: 6.838495254516602\n",
            "  Epoch 45 / 100\n",
            "    Training Loss: 0.231\tValidation Loss: 0.234\tTime: 6.76934814453125\n",
            "  Epoch 46 / 100\n",
            "    Training Loss: 0.228\tValidation Loss: 0.233\tTime: 7.04575252532959\n",
            "  Epoch 47 / 100\n",
            "    Training Loss: 0.231\tValidation Loss: 0.229\tTime: 7.127701282501221\n",
            "  Epoch 48 / 100\n",
            "    Training Loss: 0.222\tValidation Loss: 0.242\tTime: 6.249092102050781\n",
            "  Epoch 49 / 100\n",
            "    Training Loss: 0.227\tValidation Loss: 0.225\tTime: 7.113325595855713\n",
            "  Epoch 50 / 100\n",
            "    Training Loss: 0.221\tValidation Loss: 0.223\tTime: 6.961223125457764\n",
            "  Epoch 51 / 100\n",
            "    Training Loss: 0.217\tValidation Loss: 0.221\tTime: 6.884115219116211\n",
            "  Epoch 52 / 100\n",
            "    Training Loss: 0.217\tValidation Loss: 0.219\tTime: 7.17138409614563\n",
            "  Epoch 53 / 100\n",
            "    Training Loss: 0.209\tValidation Loss: 0.217\tTime: 6.9345104694366455\n",
            "  Epoch 54 / 100\n",
            "    Training Loss: 0.210\tValidation Loss: 0.220\tTime: 5.992490291595459\n",
            "  Epoch 55 / 100\n",
            "    Training Loss: 0.206\tValidation Loss: 0.214\tTime: 6.853208780288696\n",
            "  Epoch 56 / 100\n",
            "    Training Loss: 0.210\tValidation Loss: 0.213\tTime: 6.702414274215698\n",
            "  Epoch 57 / 100\n",
            "    Training Loss: 0.210\tValidation Loss: 0.212\tTime: 6.864564418792725\n",
            "  Epoch 58 / 100\n",
            "    Training Loss: 0.202\tValidation Loss: 0.210\tTime: 6.826128005981445\n",
            "  Epoch 59 / 100\n",
            "    Training Loss: 0.190\tValidation Loss: 0.216\tTime: 5.926601409912109\n",
            "  Epoch 60 / 100\n",
            "    Training Loss: 0.206\tValidation Loss: 0.207\tTime: 6.763338565826416\n",
            "  Epoch 61 / 100\n",
            "    Training Loss: 0.196\tValidation Loss: 0.214\tTime: 6.029048442840576\n",
            "  Epoch 62 / 100\n",
            "    Training Loss: 0.188\tValidation Loss: 0.212\tTime: 5.955723762512207\n",
            "  Epoch 63 / 100\n",
            "    Training Loss: 0.188\tValidation Loss: 0.206\tTime: 6.668079853057861\n",
            "  Epoch 64 / 100\n",
            "    Training Loss: 0.183\tValidation Loss: 0.204\tTime: 6.692277669906616\n",
            "  Epoch 65 / 100\n",
            "    Training Loss: 0.194\tValidation Loss: 0.202\tTime: 6.684597492218018\n",
            "  Epoch 66 / 100\n",
            "    Training Loss: 0.197\tValidation Loss: 0.201\tTime: 6.61799168586731\n",
            "  Epoch 67 / 100\n",
            "    Training Loss: 0.190\tValidation Loss: 0.201\tTime: 5.912244558334351\n",
            "  Epoch 68 / 100\n",
            "    Training Loss: 0.192\tValidation Loss: 0.200\tTime: 6.552698612213135\n",
            "  Epoch 69 / 100\n",
            "    Training Loss: 0.187\tValidation Loss: 0.210\tTime: 5.8036956787109375\n",
            "  Epoch 70 / 100\n",
            "    Training Loss: 0.181\tValidation Loss: 0.198\tTime: 6.583145618438721\n",
            "  Epoch 71 / 100\n",
            "    Training Loss: 0.185\tValidation Loss: 0.197\tTime: 6.566165208816528\n",
            "  Epoch 72 / 100\n",
            "    Training Loss: 0.191\tValidation Loss: 0.197\tTime: 6.560795545578003\n",
            "  Epoch 73 / 100\n",
            "    Training Loss: 0.184\tValidation Loss: 0.195\tTime: 6.951661109924316\n",
            "  Epoch 74 / 100\n",
            "    Training Loss: 0.170\tValidation Loss: 0.194\tTime: 6.970687627792358\n",
            "  Epoch 75 / 100\n",
            "    Training Loss: 0.183\tValidation Loss: 0.195\tTime: 6.044515609741211\n",
            "  Epoch 76 / 100\n",
            "    Training Loss: 0.181\tValidation Loss: 0.192\tTime: 7.014498710632324\n",
            "  Epoch 77 / 100\n",
            "    Training Loss: 0.176\tValidation Loss: 0.191\tTime: 6.868136167526245\n",
            "  Epoch 78 / 100\n",
            "    Training Loss: 0.169\tValidation Loss: 0.192\tTime: 6.180148601531982\n",
            "  Epoch 79 / 100\n",
            "    Training Loss: 0.173\tValidation Loss: 0.190\tTime: 6.855786323547363\n",
            "  Epoch 80 / 100\n",
            "    Training Loss: 0.168\tValidation Loss: 0.189\tTime: 6.924671411514282\n",
            "  Epoch 81 / 100\n",
            "    Training Loss: 0.174\tValidation Loss: 0.190\tTime: 6.134659767150879\n",
            "  Epoch 82 / 100\n",
            "    Training Loss: 0.170\tValidation Loss: 0.192\tTime: 6.235640525817871\n",
            "  Epoch 83 / 100\n",
            "    Training Loss: 0.177\tValidation Loss: 0.186\tTime: 6.86637544631958\n",
            "  Epoch 84 / 100\n",
            "    Training Loss: 0.170\tValidation Loss: 0.187\tTime: 6.164553880691528\n",
            "  Epoch 85 / 100\n",
            "    Training Loss: 0.166\tValidation Loss: 0.184\tTime: 7.087862730026245\n",
            "  Epoch 86 / 100\n",
            "    Training Loss: 0.165\tValidation Loss: 0.184\tTime: 6.8309831619262695\n",
            "  Epoch 87 / 100\n",
            "    Training Loss: 0.170\tValidation Loss: 0.186\tTime: 6.081610441207886\n",
            "  Epoch 88 / 100\n",
            "    Training Loss: 0.170\tValidation Loss: 0.188\tTime: 5.9231555461883545\n",
            "  Epoch 89 / 100\n",
            "    Training Loss: 0.161\tValidation Loss: 0.196\tTime: 6.200718641281128\n",
            "  Epoch 90 / 100\n",
            "    Training Loss: 0.172\tValidation Loss: 0.186\tTime: 6.125290632247925\n",
            "  Epoch 91 / 100\n",
            "    Training Loss: 0.167\tValidation Loss: 0.182\tTime: 6.672091722488403\n",
            "  Epoch 92 / 100\n",
            "    Training Loss: 0.172\tValidation Loss: 0.183\tTime: 5.9671807289123535\n",
            "  Epoch 93 / 100\n",
            "    Training Loss: 0.163\tValidation Loss: 0.185\tTime: 5.895286798477173\n",
            "  Epoch 94 / 100\n",
            "    Training Loss: 0.160\tValidation Loss: 0.181\tTime: 6.706891775131226\n",
            "  Epoch 95 / 100\n",
            "    Training Loss: 0.157\tValidation Loss: 0.189\tTime: 5.8944618701934814\n",
            "  Epoch 96 / 100\n",
            "    Training Loss: 0.169\tValidation Loss: 0.188\tTime: 5.886167764663696\n",
            "  Epoch 97 / 100\n",
            "    Training Loss: 0.165\tValidation Loss: 0.181\tTime: 5.883075475692749\n",
            "  Epoch 98 / 100\n",
            "    Training Loss: 0.169\tValidation Loss: 0.179\tTime: 6.754676818847656\n",
            "  Epoch 99 / 100\n",
            "    Training Loss: 0.152\tValidation Loss: 0.186\tTime: 5.881617069244385\n",
            "  Epoch 100 / 100\n",
            "    Training Loss: 0.160\tValidation Loss: 0.178\tTime: 6.804266691207886\n",
            "Saving model 22\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment: 23 / 25\n",
            "  Epoch 1 / 100\n",
            "    Training Loss: 0.688\tValidation Loss: 0.683\tTime: 7.060687780380249\n",
            "  Epoch 2 / 100\n",
            "    Training Loss: 0.679\tValidation Loss: 0.674\tTime: 7.026077032089233\n",
            "  Epoch 3 / 100\n",
            "    Training Loss: 0.670\tValidation Loss: 0.662\tTime: 6.9621665477752686\n",
            "  Epoch 4 / 100\n",
            "    Training Loss: 0.655\tValidation Loss: 0.648\tTime: 7.2983784675598145\n",
            "  Epoch 5 / 100\n",
            "    Training Loss: 0.635\tValidation Loss: 0.625\tTime: 7.127439022064209\n",
            "  Epoch 6 / 100\n",
            "    Training Loss: 0.609\tValidation Loss: 0.596\tTime: 6.979413747787476\n",
            "  Epoch 7 / 100\n",
            "    Training Loss: 0.574\tValidation Loss: 0.561\tTime: 6.978217363357544\n",
            "  Epoch 8 / 100\n",
            "    Training Loss: 0.533\tValidation Loss: 0.521\tTime: 6.742923974990845\n",
            "  Epoch 9 / 100\n",
            "    Training Loss: 0.489\tValidation Loss: 0.480\tTime: 7.05812668800354\n",
            "  Epoch 10 / 100\n",
            "    Training Loss: 0.448\tValidation Loss: 0.432\tTime: 7.283965349197388\n",
            "  Epoch 11 / 100\n",
            "    Training Loss: 0.403\tValidation Loss: 0.389\tTime: 7.11611008644104\n",
            "  Epoch 12 / 100\n",
            "    Training Loss: 0.370\tValidation Loss: 0.353\tTime: 7.2133684158325195\n",
            "  Epoch 13 / 100\n",
            "    Training Loss: 0.328\tValidation Loss: 0.319\tTime: 7.234927415847778\n",
            "  Epoch 14 / 100\n",
            "    Training Loss: 0.301\tValidation Loss: 0.289\tTime: 7.133049964904785\n",
            "  Epoch 15 / 100\n",
            "    Training Loss: 0.275\tValidation Loss: 0.264\tTime: 7.2946412563323975\n",
            "  Epoch 16 / 100\n",
            "    Training Loss: 0.266\tValidation Loss: 0.241\tTime: 7.3303542137146\n",
            "  Epoch 17 / 100\n",
            "    Training Loss: 0.241\tValidation Loss: 0.233\tTime: 7.356833219528198\n",
            "  Epoch 18 / 100\n",
            "    Training Loss: 0.225\tValidation Loss: 0.215\tTime: 7.366164922714233\n",
            "  Epoch 19 / 100\n",
            "    Training Loss: 0.209\tValidation Loss: 0.204\tTime: 7.327909231185913\n",
            "  Epoch 20 / 100\n",
            "    Training Loss: 0.214\tValidation Loss: 0.196\tTime: 7.287310600280762\n",
            "  Epoch 21 / 100\n",
            "    Training Loss: 0.205\tValidation Loss: 0.192\tTime: 7.302118539810181\n",
            "  Epoch 22 / 100\n",
            "    Training Loss: 0.198\tValidation Loss: 0.185\tTime: 7.131643295288086\n",
            "  Epoch 23 / 100\n",
            "    Training Loss: 0.199\tValidation Loss: 0.175\tTime: 7.023340463638306\n",
            "  Epoch 24 / 100\n",
            "    Training Loss: 0.195\tValidation Loss: 0.181\tTime: 6.353774547576904\n",
            "  Epoch 25 / 100\n",
            "    Training Loss: 0.185\tValidation Loss: 0.172\tTime: 7.1259143352508545\n",
            "  Epoch 26 / 100\n",
            "    Training Loss: 0.191\tValidation Loss: 0.167\tTime: 6.978626489639282\n",
            "  Epoch 27 / 100\n",
            "    Training Loss: 0.177\tValidation Loss: 0.171\tTime: 6.018295764923096\n",
            "  Epoch 28 / 100\n",
            "    Training Loss: 0.177\tValidation Loss: 0.163\tTime: 6.8255698680877686\n",
            "  Epoch 29 / 100\n",
            "    Training Loss: 0.190\tValidation Loss: 0.168\tTime: 6.093378305435181\n",
            "  Epoch 30 / 100\n",
            "    Training Loss: 0.179\tValidation Loss: 0.158\tTime: 7.098667860031128\n",
            "  Epoch 31 / 100\n",
            "    Training Loss: 0.168\tValidation Loss: 0.164\tTime: 6.0040154457092285\n",
            "  Epoch 32 / 100\n",
            "    Training Loss: 0.153\tValidation Loss: 0.167\tTime: 6.045504331588745\n",
            "  Epoch 33 / 100\n",
            "    Training Loss: 0.162\tValidation Loss: 0.165\tTime: 6.116250276565552\n",
            "  Epoch 34 / 100\n",
            "    Training Loss: 0.186\tValidation Loss: 0.169\tTime: 6.045861721038818\n",
            "  Epoch 35 / 100\n",
            "    Training Loss: 0.163\tValidation Loss: 0.164\tTime: 6.018841028213501\n",
            "  Epoch 36 / 100\n",
            "    Training Loss: 0.160\tValidation Loss: 0.173\tTime: 6.028135299682617\n",
            "  Epoch 37 / 100\n",
            "    Training Loss: 0.174\tValidation Loss: 0.155\tTime: 6.823373794555664\n",
            "  Epoch 38 / 100\n",
            "    Training Loss: 0.161\tValidation Loss: 0.166\tTime: 6.021685600280762\n",
            "  Epoch 39 / 100\n",
            "    Training Loss: 0.164\tValidation Loss: 0.151\tTime: 6.943297386169434\n",
            "  Epoch 40 / 100\n",
            "    Training Loss: 0.155\tValidation Loss: 0.158\tTime: 5.891381025314331\n",
            "  Epoch 41 / 100\n",
            "    Training Loss: 0.154\tValidation Loss: 0.153\tTime: 5.827009677886963\n",
            "  Epoch 42 / 100\n",
            "    Training Loss: 0.158\tValidation Loss: 0.158\tTime: 5.797478914260864\n",
            "  Epoch 43 / 100\n",
            "    Training Loss: 0.146\tValidation Loss: 0.166\tTime: 5.845612525939941\n",
            "  Epoch 44 / 100\n",
            "    Training Loss: 0.149\tValidation Loss: 0.161\tTime: 5.8516764640808105\n",
            "  Epoch 45 / 100\n",
            "    Training Loss: 0.150\tValidation Loss: 0.174\tTime: 6.186611175537109\n",
            "  Epoch 46 / 100\n",
            "    Training Loss: 0.155\tValidation Loss: 0.156\tTime: 6.370129823684692\n",
            "  Epoch 47 / 100\n",
            "    Training Loss: 0.167\tValidation Loss: 0.185\tTime: 6.2724597454071045\n",
            "  Epoch 48 / 100\n",
            "    Training Loss: 0.176\tValidation Loss: 0.157\tTime: 6.288117408752441\n",
            "  Epoch 49 / 100\n",
            "    Training Loss: 0.157\tValidation Loss: 0.163\tTime: 6.2307045459747314\n",
            "  Epoch 50 / 100\n",
            "    Training Loss: 0.168\tValidation Loss: 0.176\tTime: 6.279099225997925\n",
            "  Epoch 51 / 100\n",
            "    Training Loss: 0.170\tValidation Loss: 0.165\tTime: 6.223977327346802\n",
            "  Epoch 52 / 100\n",
            "    Training Loss: 0.149\tValidation Loss: 0.147\tTime: 7.197461843490601\n",
            "  Epoch 53 / 100\n",
            "    Training Loss: 0.160\tValidation Loss: 0.155\tTime: 6.114225149154663\n",
            "  Epoch 54 / 100\n",
            "    Training Loss: 0.157\tValidation Loss: 0.173\tTime: 6.317544937133789\n",
            "  Epoch 55 / 100\n",
            "    Training Loss: 0.177\tValidation Loss: 0.169\tTime: 6.423838138580322\n",
            "  Epoch 56 / 100\n",
            "    Training Loss: 0.147\tValidation Loss: 0.156\tTime: 6.310200214385986\n",
            "  Epoch 57 / 100\n",
            "    Training Loss: 0.139\tValidation Loss: 0.161\tTime: 6.372650623321533\n",
            "  Epoch 58 / 100\n",
            "    Training Loss: 0.131\tValidation Loss: 0.149\tTime: 6.37910008430481\n",
            "  Epoch 59 / 100\n",
            "    Training Loss: 0.144\tValidation Loss: 0.153\tTime: 6.3554770946502686\n",
            "  Epoch 60 / 100\n",
            "    Training Loss: 0.146\tValidation Loss: 0.172\tTime: 6.325634241104126\n",
            "  Epoch 61 / 100\n",
            "    Training Loss: 0.128\tValidation Loss: 0.164\tTime: 6.404940366744995\n",
            "  Epoch 62 / 100\n",
            "    Training Loss: 0.145\tValidation Loss: 0.162\tTime: 6.273146629333496\n",
            "  Epoch 63 / 100\n",
            "    Training Loss: 0.166\tValidation Loss: 0.165\tTime: 6.312664031982422\n",
            "  Epoch 64 / 100\n",
            "    Training Loss: 0.180\tValidation Loss: 0.161\tTime: 6.044541358947754\n",
            "  Epoch 65 / 100\n",
            "    Training Loss: 0.152\tValidation Loss: 0.171\tTime: 6.069865942001343\n",
            "  Epoch 66 / 100\n",
            "    Training Loss: 0.167\tValidation Loss: 0.169\tTime: 6.150645971298218\n",
            "  Epoch 67 / 100\n",
            "    Training Loss: 0.163\tValidation Loss: 0.165\tTime: 6.083197832107544\n",
            "  Epoch 68 / 100\n",
            "    Training Loss: 0.142\tValidation Loss: 0.162\tTime: 6.09925389289856\n",
            "  Epoch 69 / 100\n",
            "    Training Loss: 0.188\tValidation Loss: 0.160\tTime: 6.066023349761963\n",
            "  Epoch 70 / 100\n",
            "    Training Loss: 0.142\tValidation Loss: 0.177\tTime: 6.0263471603393555\n",
            "  Epoch 71 / 100\n",
            "    Training Loss: 0.148\tValidation Loss: 0.147\tTime: 6.954125642776489\n",
            "  Epoch 72 / 100\n",
            "    Training Loss: 0.158\tValidation Loss: 0.170\tTime: 6.046410083770752\n",
            "  Epoch 73 / 100\n",
            "    Training Loss: 0.163\tValidation Loss: 0.142\tTime: 7.0608696937561035\n",
            "  Epoch 74 / 100\n",
            "    Training Loss: 0.135\tValidation Loss: 0.150\tTime: 6.006096363067627\n",
            "  Epoch 75 / 100\n",
            "    Training Loss: 0.175\tValidation Loss: 0.154\tTime: 6.103996515274048\n",
            "  Epoch 76 / 100\n",
            "    Training Loss: 0.163\tValidation Loss: 0.143\tTime: 6.04529333114624\n",
            "  Epoch 77 / 100\n",
            "    Training Loss: 0.146\tValidation Loss: 0.157\tTime: 6.000371694564819\n",
            "  Epoch 78 / 100\n",
            "    Training Loss: 0.164\tValidation Loss: 0.138\tTime: 6.863244533538818\n",
            "  Epoch 79 / 100\n",
            "    Training Loss: 0.148\tValidation Loss: 0.133\tTime: 6.803104639053345\n",
            "  Epoch 80 / 100\n",
            "    Training Loss: 0.142\tValidation Loss: 0.150\tTime: 5.919728517532349\n",
            "  Epoch 81 / 100\n",
            "    Training Loss: 0.154\tValidation Loss: 0.156\tTime: 5.884576082229614\n",
            "  Epoch 82 / 100\n",
            "    Training Loss: 0.125\tValidation Loss: 0.164\tTime: 5.867951154708862\n",
            "  Epoch 83 / 100\n",
            "    Training Loss: 0.136\tValidation Loss: 0.156\tTime: 6.100707530975342\n",
            "  Epoch 84 / 100\n",
            "    Training Loss: 0.158\tValidation Loss: 0.156\tTime: 6.365919351577759\n",
            "  Epoch 85 / 100\n",
            "    Training Loss: 0.136\tValidation Loss: 0.146\tTime: 6.225450038909912\n",
            "  Epoch 86 / 100\n",
            "    Training Loss: 0.155\tValidation Loss: 0.176\tTime: 6.317272186279297\n",
            "  Epoch 87 / 100\n",
            "    Training Loss: 0.153\tValidation Loss: 0.153\tTime: 6.335909605026245\n",
            "  Epoch 88 / 100\n",
            "    Training Loss: 0.127\tValidation Loss: 0.153\tTime: 6.115899562835693\n",
            "  Epoch 89 / 100\n",
            "    Training Loss: 0.139\tValidation Loss: 0.157\tTime: 6.206715106964111\n",
            "  Epoch 90 / 100\n",
            "    Training Loss: 0.145\tValidation Loss: 0.179\tTime: 6.158772706985474\n",
            "  Epoch 91 / 100\n",
            "    Training Loss: 0.144\tValidation Loss: 0.140\tTime: 6.312114715576172\n",
            "  Epoch 92 / 100\n",
            "    Training Loss: 0.157\tValidation Loss: 0.187\tTime: 6.20413613319397\n",
            "  Epoch 93 / 100\n",
            "    Training Loss: 0.132\tValidation Loss: 0.155\tTime: 6.444560289382935\n",
            "  Epoch 94 / 100\n",
            "    Training Loss: 0.167\tValidation Loss: 0.150\tTime: 6.566584825515747\n",
            "  Epoch 95 / 100\n",
            "    Training Loss: 0.154\tValidation Loss: 0.153\tTime: 6.4008848667144775\n",
            "  Epoch 96 / 100\n",
            "    Training Loss: 0.138\tValidation Loss: 0.153\tTime: 6.413938999176025\n",
            "  Epoch 97 / 100\n",
            "    Training Loss: 0.160\tValidation Loss: 0.136\tTime: 6.398962020874023\n",
            "  Epoch 98 / 100\n",
            "    Training Loss: 0.127\tValidation Loss: 0.162\tTime: 6.372207403182983\n",
            "  Epoch 99 / 100\n",
            "    Training Loss: 0.151\tValidation Loss: 0.170\tTime: 6.379123210906982\n",
            "  Epoch 100 / 100\n",
            "    Training Loss: 0.155\tValidation Loss: 0.140\tTime: 6.294490575790405\n",
            "Saving model 23\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment: 24 / 25\n",
            "  Epoch 1 / 100\n",
            "    Training Loss: 0.688\tValidation Loss: 0.677\tTime: 19.40361523628235\n",
            "  Epoch 2 / 100\n",
            "    Training Loss: 0.669\tValidation Loss: 0.660\tTime: 19.43151307106018\n",
            "  Epoch 3 / 100\n",
            "    Training Loss: 0.651\tValidation Loss: 0.641\tTime: 19.41648769378662\n",
            "  Epoch 4 / 100\n",
            "    Training Loss: 0.623\tValidation Loss: 0.616\tTime: 19.401766300201416\n",
            "  Epoch 5 / 100\n",
            "    Training Loss: 0.591\tValidation Loss: 0.589\tTime: 19.207049131393433\n",
            "  Epoch 6 / 100\n",
            "    Training Loss: 0.554\tValidation Loss: 0.562\tTime: 18.920023679733276\n",
            "  Epoch 7 / 100\n",
            "    Training Loss: 0.514\tValidation Loss: 0.513\tTime: 19.53230929374695\n",
            "  Epoch 8 / 100\n",
            "    Training Loss: 0.473\tValidation Loss: 0.473\tTime: 19.722702741622925\n",
            "  Epoch 9 / 100\n",
            "    Training Loss: 0.427\tValidation Loss: 0.427\tTime: 19.656394958496094\n",
            "  Epoch 10 / 100\n",
            "    Training Loss: 0.388\tValidation Loss: 0.396\tTime: 20.09011220932007\n",
            "  Epoch 11 / 100\n",
            "    Training Loss: 0.352\tValidation Loss: 0.350\tTime: 19.8582661151886\n",
            "  Epoch 12 / 100\n",
            "    Training Loss: 0.326\tValidation Loss: 0.319\tTime: 19.68176579475403\n",
            "  Epoch 13 / 100\n",
            "    Training Loss: 0.283\tValidation Loss: 0.293\tTime: 19.61658525466919\n",
            "  Epoch 14 / 100\n",
            "    Training Loss: 0.267\tValidation Loss: 0.273\tTime: 19.403497219085693\n",
            "  Epoch 15 / 100\n",
            "    Training Loss: 0.251\tValidation Loss: 0.252\tTime: 19.373685121536255\n",
            "  Epoch 16 / 100\n",
            "    Training Loss: 0.230\tValidation Loss: 0.244\tTime: 19.317444562911987\n",
            "  Epoch 17 / 100\n",
            "    Training Loss: 0.220\tValidation Loss: 0.226\tTime: 19.29202914237976\n",
            "  Epoch 18 / 100\n",
            "    Training Loss: 0.204\tValidation Loss: 0.211\tTime: 19.19977855682373\n",
            "  Epoch 19 / 100\n",
            "    Training Loss: 0.196\tValidation Loss: 0.201\tTime: 19.207554817199707\n",
            "  Epoch 20 / 100\n",
            "    Training Loss: 0.193\tValidation Loss: 0.199\tTime: 19.90036177635193\n",
            "  Epoch 21 / 100\n",
            "    Training Loss: 0.179\tValidation Loss: 0.186\tTime: 19.70438265800476\n",
            "  Epoch 22 / 100\n",
            "    Training Loss: 0.185\tValidation Loss: 0.182\tTime: 19.56734824180603\n",
            "  Epoch 23 / 100\n",
            "    Training Loss: 0.170\tValidation Loss: 0.177\tTime: 19.862725734710693\n",
            "  Epoch 24 / 100\n",
            "    Training Loss: 0.176\tValidation Loss: 0.177\tTime: 19.640647649765015\n",
            "  Epoch 25 / 100\n",
            "    Training Loss: 0.161\tValidation Loss: 0.172\tTime: 19.448906183242798\n",
            "  Epoch 26 / 100\n",
            "    Training Loss: 0.161\tValidation Loss: 0.175\tTime: 17.208961486816406\n",
            "  Epoch 27 / 100\n",
            "    Training Loss: 0.158\tValidation Loss: 0.166\tTime: 19.14771556854248\n",
            "  Epoch 28 / 100\n",
            "    Training Loss: 0.155\tValidation Loss: 0.162\tTime: 19.392189025878906\n",
            "  Epoch 29 / 100\n",
            "    Training Loss: 0.155\tValidation Loss: 0.167\tTime: 17.25678825378418\n",
            "  Epoch 30 / 100\n",
            "    Training Loss: 0.149\tValidation Loss: 0.159\tTime: 18.95816946029663\n",
            "  Epoch 31 / 100\n",
            "    Training Loss: 0.164\tValidation Loss: 0.166\tTime: 16.993428468704224\n",
            "  Epoch 32 / 100\n",
            "    Training Loss: 0.161\tValidation Loss: 0.163\tTime: 17.136844396591187\n",
            "  Epoch 33 / 100\n",
            "    Training Loss: 0.172\tValidation Loss: 0.155\tTime: 19.586981534957886\n",
            "  Epoch 34 / 100\n",
            "    Training Loss: 0.160\tValidation Loss: 0.164\tTime: 17.361762046813965\n",
            "  Epoch 35 / 100\n",
            "    Training Loss: 0.145\tValidation Loss: 0.154\tTime: 19.55819082260132\n",
            "  Epoch 36 / 100\n",
            "    Training Loss: 0.166\tValidation Loss: 0.154\tTime: 19.756319999694824\n",
            "  Epoch 37 / 100\n",
            "    Training Loss: 0.150\tValidation Loss: 0.151\tTime: 19.74212145805359\n",
            "  Epoch 38 / 100\n",
            "    Training Loss: 0.160\tValidation Loss: 0.154\tTime: 17.415529489517212\n",
            "  Epoch 39 / 100\n",
            "    Training Loss: 0.147\tValidation Loss: 0.151\tTime: 17.136793613433838\n",
            "  Epoch 40 / 100\n",
            "    Training Loss: 0.158\tValidation Loss: 0.159\tTime: 17.161582708358765\n",
            "  Epoch 41 / 100\n",
            "    Training Loss: 0.153\tValidation Loss: 0.155\tTime: 17.08037543296814\n",
            "  Epoch 42 / 100\n",
            "    Training Loss: 0.136\tValidation Loss: 0.151\tTime: 17.100276708602905\n",
            "  Epoch 43 / 100\n",
            "    Training Loss: 0.156\tValidation Loss: 0.147\tTime: 19.118274450302124\n",
            "  Epoch 44 / 100\n",
            "    Training Loss: 0.160\tValidation Loss: 0.151\tTime: 17.027400732040405\n",
            "  Epoch 45 / 100\n",
            "    Training Loss: 0.148\tValidation Loss: 0.146\tTime: 19.093191146850586\n",
            "  Epoch 46 / 100\n",
            "    Training Loss: 0.152\tValidation Loss: 0.147\tTime: 17.531631231307983\n",
            "  Epoch 47 / 100\n",
            "    Training Loss: 0.145\tValidation Loss: 0.168\tTime: 17.28515338897705\n",
            "  Epoch 48 / 100\n",
            "    Training Loss: 0.156\tValidation Loss: 0.148\tTime: 17.348453521728516\n",
            "  Epoch 49 / 100\n",
            "    Training Loss: 0.145\tValidation Loss: 0.153\tTime: 17.553577423095703\n",
            "  Epoch 50 / 100\n",
            "    Training Loss: 0.156\tValidation Loss: 0.156\tTime: 17.502758264541626\n",
            "  Epoch 51 / 100\n",
            "    Training Loss: 0.160\tValidation Loss: 0.146\tTime: 19.43568468093872\n",
            "  Epoch 52 / 100\n",
            "    Training Loss: 0.132\tValidation Loss: 0.149\tTime: 17.405591249465942\n",
            "  Epoch 53 / 100\n",
            "    Training Loss: 0.123\tValidation Loss: 0.141\tTime: 19.291909217834473\n",
            "  Epoch 54 / 100\n",
            "    Training Loss: 0.140\tValidation Loss: 0.154\tTime: 17.449925184249878\n",
            "  Epoch 55 / 100\n",
            "    Training Loss: 0.146\tValidation Loss: 0.166\tTime: 17.15487313270569\n",
            "  Epoch 56 / 100\n",
            "    Training Loss: 0.132\tValidation Loss: 0.143\tTime: 16.962311029434204\n",
            "  Epoch 57 / 100\n",
            "    Training Loss: 0.140\tValidation Loss: 0.149\tTime: 16.956907987594604\n",
            "  Epoch 58 / 100\n",
            "    Training Loss: 0.112\tValidation Loss: 0.148\tTime: 16.562191486358643\n",
            "  Epoch 59 / 100\n",
            "    Training Loss: 0.140\tValidation Loss: 0.156\tTime: 17.21382713317871\n",
            "  Epoch 60 / 100\n",
            "    Training Loss: 0.127\tValidation Loss: 0.162\tTime: 17.408459424972534\n",
            "  Epoch 61 / 100\n",
            "    Training Loss: 0.144\tValidation Loss: 0.170\tTime: 17.364144563674927\n",
            "  Epoch 62 / 100\n",
            "    Training Loss: 0.126\tValidation Loss: 0.149\tTime: 17.35559868812561\n",
            "  Epoch 63 / 100\n",
            "    Training Loss: 0.117\tValidation Loss: 0.146\tTime: 17.564276695251465\n",
            "  Epoch 64 / 100\n",
            "    Training Loss: 0.127\tValidation Loss: 0.141\tTime: 19.930084943771362\n",
            "  Epoch 65 / 100\n",
            "    Training Loss: 0.137\tValidation Loss: 0.148\tTime: 17.47075581550598\n",
            "  Epoch 66 / 100\n",
            "    Training Loss: 0.138\tValidation Loss: 0.156\tTime: 16.93082332611084\n",
            "  Epoch 67 / 100\n",
            "    Training Loss: 0.137\tValidation Loss: 0.160\tTime: 17.093968629837036\n",
            "  Epoch 68 / 100\n",
            "    Training Loss: 0.167\tValidation Loss: 0.146\tTime: 17.09272336959839\n",
            "  Epoch 69 / 100\n",
            "    Training Loss: 0.132\tValidation Loss: 0.166\tTime: 17.036611318588257\n",
            "  Epoch 70 / 100\n",
            "    Training Loss: 0.145\tValidation Loss: 0.149\tTime: 16.957216501235962\n",
            "  Epoch 71 / 100\n",
            "    Training Loss: 0.132\tValidation Loss: 0.138\tTime: 18.83254599571228\n",
            "  Epoch 72 / 100\n",
            "    Training Loss: 0.147\tValidation Loss: 0.168\tTime: 16.98958110809326\n",
            "  Epoch 73 / 100\n",
            "    Training Loss: 0.119\tValidation Loss: 0.152\tTime: 17.38121747970581\n",
            "  Epoch 74 / 100\n",
            "    Training Loss: 0.119\tValidation Loss: 0.150\tTime: 17.30092191696167\n",
            "  Epoch 75 / 100\n",
            "    Training Loss: 0.140\tValidation Loss: 0.142\tTime: 17.216961145401\n",
            "  Epoch 76 / 100\n",
            "    Training Loss: 0.162\tValidation Loss: 0.155\tTime: 17.397911310195923\n",
            "  Epoch 77 / 100\n",
            "    Training Loss: 0.133\tValidation Loss: 0.170\tTime: 17.457927465438843\n",
            "  Epoch 78 / 100\n",
            "    Training Loss: 0.147\tValidation Loss: 0.137\tTime: 19.524953603744507\n",
            "  Epoch 79 / 100\n",
            "    Training Loss: 0.137\tValidation Loss: 0.148\tTime: 17.43379521369934\n",
            "  Epoch 80 / 100\n",
            "    Training Loss: 0.132\tValidation Loss: 0.137\tTime: 19.240711212158203\n",
            "  Epoch 81 / 100\n",
            "    Training Loss: 0.132\tValidation Loss: 0.143\tTime: 17.222370862960815\n",
            "  Epoch 82 / 100\n",
            "    Training Loss: 0.133\tValidation Loss: 0.136\tTime: 19.34824776649475\n",
            "  Epoch 83 / 100\n",
            "    Training Loss: 0.125\tValidation Loss: 0.133\tTime: 19.259087800979614\n",
            "  Epoch 84 / 100\n",
            "    Training Loss: 0.123\tValidation Loss: 0.158\tTime: 17.027414560317993\n",
            "  Epoch 85 / 100\n",
            "    Training Loss: 0.159\tValidation Loss: 0.152\tTime: 16.4733943939209\n",
            "  Epoch 86 / 100\n",
            "    Training Loss: 0.138\tValidation Loss: 0.161\tTime: 17.126253843307495\n",
            "  Epoch 87 / 100\n",
            "    Training Loss: 0.137\tValidation Loss: 0.136\tTime: 17.450974941253662\n",
            "  Epoch 88 / 100\n",
            "    Training Loss: 0.133\tValidation Loss: 0.130\tTime: 19.286073923110962\n",
            "  Epoch 89 / 100\n",
            "    Training Loss: 0.134\tValidation Loss: 0.152\tTime: 17.474263668060303\n",
            "  Epoch 90 / 100\n",
            "    Training Loss: 0.137\tValidation Loss: 0.166\tTime: 17.50422477722168\n",
            "  Epoch 91 / 100\n",
            "    Training Loss: 0.120\tValidation Loss: 0.144\tTime: 17.387901544570923\n",
            "  Epoch 92 / 100\n",
            "    Training Loss: 0.141\tValidation Loss: 0.140\tTime: 17.339982271194458\n",
            "  Epoch 93 / 100\n",
            "    Training Loss: 0.135\tValidation Loss: 0.135\tTime: 17.153831720352173\n",
            "  Epoch 94 / 100\n",
            "    Training Loss: 0.148\tValidation Loss: 0.138\tTime: 17.120989084243774\n",
            "  Epoch 95 / 100\n",
            "    Training Loss: 0.137\tValidation Loss: 0.164\tTime: 16.974346160888672\n",
            "  Epoch 96 / 100\n",
            "    Training Loss: 0.115\tValidation Loss: 0.166\tTime: 16.99788761138916\n",
            "  Epoch 97 / 100\n",
            "    Training Loss: 0.120\tValidation Loss: 0.142\tTime: 17.056244373321533\n",
            "  Epoch 98 / 100\n",
            "    Training Loss: 0.132\tValidation Loss: 0.151\tTime: 16.944774627685547\n",
            "  Epoch 99 / 100\n",
            "    Training Loss: 0.141\tValidation Loss: 0.155\tTime: 16.850992441177368\n",
            "  Epoch 100 / 100\n",
            "    Training Loss: 0.135\tValidation Loss: 0.156\tTime: 17.25547432899475\n",
            "Saving model 24\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n"
          ]
        }
      ],
      "source": [
        "epochses = [2, 10, 20, 50, 100]\n",
        "# epochses = [1, 1, 1, 1, 1]\n",
        "exp_title = 'test_epochs'\n",
        "# xlm-roberta-base xlm-roberta-large bert-base-uncased roberta-base roberta-large\n",
        "pretrained_model_names = ['xlm-roberta-base', 'xlm-roberta-large', 'bert-base-uncased', 'roberta-base', 'roberta-large']\n",
        "model_type = Ext_Arch\n",
        "lr = 0.00001\n",
        "optimizer_type = torch.optim.AdamW\n",
        "loss_func_type = nn.NLLLoss\n",
        "max_length = 25 \n",
        "padding = 'max_length'\n",
        "truncation=True\n",
        "batch_size = 32\n",
        "\n",
        "\n",
        "\n",
        "exp_index = 0\n",
        "total_exp = len(epochses) * len(pretrained_model_names)\n",
        "for epochs in epochses:\n",
        "    for pretrained_model_name in pretrained_model_names:\n",
        "        model = get_model(pretrained_model_name, model_type, device)\n",
        "        train_dataloader, val_dataloader, test_data, weights = tokenize_data(pretrained_model_name, max_length, padding, truncation, batch_size, device)\n",
        "        loss_func  = loss_func_type(weight=weights)\n",
        "\n",
        "        print(f'Experiment: {exp_index} / {total_exp}')\n",
        "        optimizer = optimizer_type(model.parameters(), lr=lr)\n",
        "        do_exp(model, train_dataloader, val_dataloader, test_data, device, loss_func, optimizer, epochs, lr, \n",
        "            batch_size, max_length, padding, truncation, pretrained_model_name, exp_title, exp_index, root)\n",
        "        exp_index += 1\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQbGdblQ76jX"
      },
      "source": [
        "# Test lr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "117e2deb939347d5bf327fbb74e613e0",
            "1536eb6dcda945b980ffac3832697add",
            "b301739aa5a94670940c98ae3b97b1b9",
            "7aaf01a432394e3980441affa542f256",
            "249eae2745954bce84671ff98f05a424",
            "344466135aed495d958e87cb3bb995b9",
            "841d5a83cd1b4a09a6cb7187c40ddad9",
            "b5624635544c44a2a2a114bc06aa96bb",
            "a8a4c0629f7f40ba840702b334d0bcc8",
            "3cb3aef4c8ca45b991f070cea03d13f0",
            "122c0e76191c4c70a9725a841c214ce9",
            "e604ecd3f18d4196b4b851dce47312cf",
            "7486d264c0714a1d9c1ffa62b03c69a0",
            "df8fbc55971948649e3a7a07b895075a",
            "33ba86a3f77e4b7a97d49f3aec3e8dbc",
            "527a9e0a5b53450a911be6f404b2e90b",
            "3f1d57fdddd44ccaa5ba0a8805047d0a",
            "8cac5224f56f4c4ea44d1184aee5d45e",
            "facff6806996469dabd548e5816e2779",
            "746192d4fd3048dd8511de69f7df6a16",
            "a1c5ecf962fe469bb52ec4a8237cf994",
            "e05b169cf3a543a9a1c3edbec9033a95",
            "69cc43fc8a7f440a947eeabec72f2869",
            "6eae2e8806204f0cbba9ccb04762e8f1",
            "92c2b616cc8d4a28bf53d61cf3001080",
            "06b49200098f4c008b52eab17ab840b7",
            "6e3882e996a24fbcbff6f02d64f83e3c",
            "eb3856b60e404178b49e7c2a8e85284d",
            "87d71d0843224209a353afbe124681af",
            "42343c318f4b411f9eb154667ba9cf99",
            "345192a850d14902b00526ea185744e3",
            "97118c9a4f334f9fa910c53f3318bb11",
            "cc3e42be2a9042e5b5d85889d7542986",
            "685de2fa4aa04bc29eeaae3f5f0b7794",
            "5070caab0f8a4d0aab8d6401950c1111",
            "21d817d20d334225898c77c287303286",
            "596f2cc5e5ab459f822e89b7d8ded139",
            "414a7bca420842a89d28b7d1b65e2714",
            "e1d69d2fad224624989c8fac6ad6cf44",
            "c2316dde02604dbdb7d6f845a1291312",
            "c024e359a7594858878cb9fe4326eb87",
            "ad07a78388de4860b130f5a8005a86e2",
            "1e6c7510884a4f18be40c30aac4d395c",
            "2d634983540848ffad080c3d5369c452",
            "5dfff112602943478309cbea38f66706",
            "ec4c584794a54627b389fc2f8352a8e8",
            "2bb31ed3979147c6a3024cb79bba95b9",
            "ad11cba01e524d07bde3c73e8292f1e6",
            "d244171bb3d140019e5262e31e08162d",
            "3303631f627348d6937bb4666f2d2fa4",
            "db9805ea34a54d68a25612e5afda8599",
            "25a999ed476d4bf4b1d926a06704950f",
            "c643f23cbb8a4476a624466cbe6f7fb7",
            "d0bfd76426ad4d5aa483c46f9e351bd7",
            "6b330fea79de4dff85d047751984fe2e",
            "ce193bde1719452982319f8cf8a23e67",
            "cdfb12634f7c4bc9b376ac5c2f7afbad",
            "16db16903284459a85900b08fabe35fe",
            "d426b00b655d457ca7d509a582029d4b",
            "3a7ba2c8f5004708b84f46934824276f",
            "04b914d7d5434235af96802a32fb6644",
            "4247348e687c40a4b618a78e115e0301",
            "e4d01fd89cd1435ab10a78ea7b29875d",
            "5287cdebfc4b4dd48ea0f2589344433d",
            "c32b518d41c3431d9fd9f40981a26f70",
            "b11693ef9187409583a82d451dcc49a7",
            "e50c676009a54250bc20d180fd480122",
            "feb69f5439874077a409d30609e2008b",
            "87968782f3824a9a961899cf6a373e95",
            "191d9bd1fa6a4ed5ba05dab0a28603d3",
            "929259a9f39341a89b25f77da151ac83",
            "e9a12b54c8ef49f3a0b84c37a3568d2a",
            "e61cfcd5aeb346cebc8714a6d42943a8",
            "bea1a4d6746f45c195933bf54cadcf17",
            "24c7e69994f844ce958a2229335c9e7a",
            "d9b0d4f328c8432a9db0148762d0516f",
            "ec8da29506974d73a9eebb20e8107da1",
            "e32fe978cad644888720c5ffe58c6090",
            "69cdd1ea64494a00ac382857754c30d0",
            "ce2fcbe7f0d74af4aa11175c9e0002a5",
            "903395681a844b06a5448dd576740e44",
            "bb7eedfc5d764a7585f3884fb41c0776",
            "4256d2b06ada4bdda95a4c04065727d3",
            "5f5e04ea2b11470fa72412b212174e2d",
            "7183c5d7fdea4fc3bb5135da5a7005be",
            "04bf85d9b80242128180fb3210526c58",
            "5c3d265bfad542b7a2aec7b946c9bbf9",
            "e3f83a30e35d4e508e00b4f997a8c822",
            "2762bac8e3dd4c959c6c310a3cee4cfe",
            "6885562c034d4c66947ce90a096ddaad",
            "13e4d20b340e44cdbb95064c66c06cf6",
            "a68c16d54b6b4a6ea267c3dbd984ce90",
            "d08f255ae07444f6baa31075f8667767",
            "119191c2a24b4bbea14d9184699474a4",
            "00ee88eb00534c10b9254bc8e7b543d8",
            "ea1b52435bed4bbd9718bc2e5b6e5a73",
            "a0a753b14e40472b833d752462d88699",
            "783a932326334a598842e652d8784eed",
            "fe8f0c25d57b4c48b52eeb16d67cec57",
            "5ee2eae2f4774d019e072f99d78e06bf",
            "aaf642fa182d41599d399d50ac80ac7e",
            "7e2112fde6584a4c82fbab180c10aa9c",
            "ed9f6a1d9e1b4a70a106fff0dd8306bc",
            "888350201b1a467db6d7c4830d2d0da0",
            "68aed31bfa324e83916985e768793861",
            "c00131d5abbc46c483422946d6da378b",
            "04fac31357fb44559ca019c4a494ef14",
            "d46a5441ae8348f49afa7cbeb25d480e",
            "7ad7e28dc4cd434ea02a8699417d4013",
            "58d7d93a3ca6479ea872e709e91effb2",
            "c47d43189aa74bbf94fc3a9b9d86f894",
            "deb570ddbe844878b42019aec3a1ecb3",
            "0b3a0b28bd284e3b944e15875f4ca247",
            "2921ba71d7264927b06d61c45f77839b",
            "f43c23d340a34160b97cf52940b3c404",
            "23a6e2fc47624214897c86d95f2e08e4",
            "964ac2df12744847a8ee906ad55c747a",
            "a5667495abba42b5bf497423637fa8a8",
            "2c56ae40c5df4f51932770fa884f6fdd",
            "b2730716305f4b70874d2df4a1dbc101",
            "a638b5928dfa4101b8a217f015d97765",
            "022523889b6647faaf097d6611ebde8a",
            "8756353704db4c86bce5bc1b9baf75ec",
            "63086abcf0fe434b97df24595834b42a",
            "e5acc54fd1424710a929f1d30767cce8",
            "a18949e0306a4ab1aa0d96b1e195730c",
            "616bd8761f4e4d0d943bedc67ef1a31d",
            "684a31dd40ff42528050470b15aa8257",
            "793428bad3f34f108f928b2bcfd50eb4",
            "648d5c4db6314b9590ee70735972e2e8",
            "432ee689940845ce972ae77c8fbf987d",
            "4bd38697ed6746378b3754845307652c",
            "b788c7f10baf48dc870b6d30d99c6b56",
            "6146b97a41794723ac542b3dc52d6eae",
            "bb560e89bece428792aaaf2a0618fd85",
            "779b91513610478685d2a9c75ad99e8c",
            "71733289f1c043cba9b33aad398a9761",
            "53dd03019c28481c8a54b651ca621494",
            "e43c379877b74127810f8b652999fc57",
            "2e77a59fa33b4f7cb574a2822b065592",
            "e178f7e2fc1746b2ad3de4e2449e7231",
            "bdcc4bebc4b44c3684276b909c1eb11e",
            "83bb587c4c9b44409b145dfc6ee7be10",
            "12a18e913e1740979e77c9df29b18056",
            "2157b9da3d994395bae549c75c1823b5",
            "3c83f7f9e52040dc8c83e964e248f4df",
            "a9ce3d1e274644cab4b6d221dc7906db",
            "0595664e846c4253a3b37125bfc6989b",
            "0c068a7f3ea74e4f9b487b4a1bab831a",
            "8fbadf83d6684af2bc374ca74fda5759"
          ]
        },
        "id": "V3SLUXsA76jX",
        "outputId": "e4efc165-bcbe-4cc5-ddee-c8ce7cb8b2a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment: 0 / 25\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.688\tValidation Loss: 0.701\tTime: 9.384676218032837\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.672\tValidation Loss: 0.660\tTime: 8.98707389831543\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.653\tValidation Loss: 0.641\tTime: 9.172810316085815\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.631\tValidation Loss: 0.608\tTime: 9.235761404037476\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.617\tValidation Loss: 0.666\tTime: 6.408978223800659\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.597\tValidation Loss: 0.568\tTime: 9.227068185806274\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.570\tValidation Loss: 0.543\tTime: 9.070326566696167\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.544\tValidation Loss: 0.557\tTime: 6.462035894393921\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.537\tValidation Loss: 0.501\tTime: 9.587048530578613\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.508\tValidation Loss: 0.469\tTime: 9.083573341369629\n",
            "Saving model 0\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 1 / 25\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.698\tValidation Loss: 0.719\tTime: 21.234189748764038\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.678\tValidation Loss: 0.660\tTime: 21.804248571395874\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.669\tValidation Loss: 0.650\tTime: 21.794618844985962\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.654\tValidation Loss: 0.625\tTime: 21.43434762954712\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.632\tValidation Loss: 0.609\tTime: 21.327823638916016\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.615\tValidation Loss: 0.591\tTime: 21.935737371444702\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.599\tValidation Loss: 0.566\tTime: 22.054574966430664\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.591\tValidation Loss: 0.618\tTime: 17.67042565345764\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.571\tValidation Loss: 0.574\tTime: 17.371167182922363\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.552\tValidation Loss: 0.586\tTime: 17.4185049533844\n",
            "Saving model 1\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 2 / 25\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.596\tValidation Loss: 0.496\tTime: 7.5768210887908936\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.457\tValidation Loss: 0.390\tTime: 6.973065137863159\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.362\tValidation Loss: 0.328\tTime: 7.024078607559204\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.315\tValidation Loss: 0.294\tTime: 7.032901763916016\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.268\tValidation Loss: 0.274\tTime: 6.753798007965088\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.235\tValidation Loss: 0.236\tTime: 6.955423831939697\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.234\tValidation Loss: 0.220\tTime: 6.837333917617798\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.208\tValidation Loss: 0.206\tTime: 6.772629737854004\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.208\tValidation Loss: 0.226\tTime: 6.028726577758789\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.201\tValidation Loss: 0.230\tTime: 5.947469234466553\n",
            "Saving model 2\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment: 3 / 25\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.638\tValidation Loss: 0.518\tTime: 6.982634782791138\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.357\tValidation Loss: 0.225\tTime: 6.913594484329224\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.184\tValidation Loss: 0.178\tTime: 6.9228010177612305\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.166\tValidation Loss: 0.154\tTime: 6.959776401519775\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.179\tValidation Loss: 0.140\tTime: 6.8702919483184814\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.153\tValidation Loss: 0.174\tTime: 5.95003867149353\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.205\tValidation Loss: 0.148\tTime: 5.997107982635498\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.159\tValidation Loss: 0.128\tTime: 6.7296624183654785\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.146\tValidation Loss: 0.154\tTime: 5.822422981262207\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.161\tValidation Loss: 0.201\tTime: 5.85389518737793\n",
            "Saving model 3\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment: 4 / 25\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.622\tValidation Loss: 0.485\tTime: 19.765199184417725\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.342\tValidation Loss: 0.253\tTime: 19.856430530548096\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.204\tValidation Loss: 0.169\tTime: 19.643911361694336\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.173\tValidation Loss: 0.156\tTime: 19.79982352256775\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.172\tValidation Loss: 0.157\tTime: 17.652902603149414\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.170\tValidation Loss: 0.184\tTime: 17.34670090675354\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.138\tValidation Loss: 0.144\tTime: 19.368715047836304\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.162\tValidation Loss: 0.190\tTime: 17.279385566711426\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.182\tValidation Loss: 0.173\tTime: 17.068943977355957\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.182\tValidation Loss: 0.176\tTime: 17.037272453308105\n",
            "Saving model 4\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 5 / 25\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.692\tValidation Loss: 0.686\tTime: 8.571052551269531\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.688\tValidation Loss: 0.683\tTime: 8.48027229309082\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.683\tValidation Loss: 0.681\tTime: 8.769491910934448\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.680\tValidation Loss: 0.676\tTime: 8.90062665939331\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.682\tValidation Loss: 0.675\tTime: 8.639001369476318\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.676\tValidation Loss: 0.674\tTime: 8.56258749961853\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.678\tValidation Loss: 0.671\tTime: 8.992575645446777\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.672\tValidation Loss: 0.667\tTime: 9.273621320724487\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.671\tValidation Loss: 0.667\tTime: 9.389075994491577\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.667\tValidation Loss: 0.663\tTime: 9.625975370407104\n",
            "Saving model 5\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 6 / 25\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.694\tValidation Loss: 0.689\tTime: 22.215279579162598\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.688\tValidation Loss: 0.684\tTime: 22.604748010635376\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.685\tValidation Loss: 0.680\tTime: 22.613112449645996\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.684\tValidation Loss: 0.677\tTime: 21.95241928100586\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.680\tValidation Loss: 0.674\tTime: 22.01115584373474\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.679\tValidation Loss: 0.672\tTime: 21.737416744232178\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.674\tValidation Loss: 0.671\tTime: 22.28404927253723\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.672\tValidation Loss: 0.667\tTime: 22.005165338516235\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.672\tValidation Loss: 0.665\tTime: 22.039591550827026\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.668\tValidation Loss: 0.662\tTime: 22.41301941871643\n",
            "Saving model 6\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 7 / 25\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.679\tValidation Loss: 0.656\tTime: 7.328940391540527\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.640\tValidation Loss: 0.629\tTime: 6.974507570266724\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.612\tValidation Loss: 0.596\tTime: 6.9196250438690186\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.587\tValidation Loss: 0.575\tTime: 7.057965040206909\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.562\tValidation Loss: 0.545\tTime: 7.0996387004852295\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.546\tValidation Loss: 0.524\tTime: 7.123373031616211\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.523\tValidation Loss: 0.505\tTime: 7.202749729156494\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.502\tValidation Loss: 0.491\tTime: 7.154817819595337\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.486\tValidation Loss: 0.473\tTime: 6.969358682632446\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.467\tValidation Loss: 0.463\tTime: 6.9629528522491455\n",
            "Saving model 7\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment: 8 / 25\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.687\tValidation Loss: 0.682\tTime: 7.386786937713623\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.678\tValidation Loss: 0.672\tTime: 7.0878376960754395\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.666\tValidation Loss: 0.661\tTime: 6.967107534408569\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.650\tValidation Loss: 0.643\tTime: 7.010512351989746\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.627\tValidation Loss: 0.620\tTime: 7.033883810043335\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.600\tValidation Loss: 0.590\tTime: 7.0021233558654785\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.564\tValidation Loss: 0.553\tTime: 6.900899887084961\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.525\tValidation Loss: 0.512\tTime: 7.031943321228027\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.488\tValidation Loss: 0.470\tTime: 7.089817762374878\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.439\tValidation Loss: 0.429\tTime: 7.093398332595825\n",
            "Saving model 8\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment: 9 / 25\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.687\tValidation Loss: 0.680\tTime: 19.23778009414673\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.671\tValidation Loss: 0.662\tTime: 19.091171264648438\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.652\tValidation Loss: 0.644\tTime: 18.89937400817871\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.628\tValidation Loss: 0.628\tTime: 19.96523118019104\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.600\tValidation Loss: 0.597\tTime: 19.835174798965454\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.562\tValidation Loss: 0.557\tTime: 19.717493057250977\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.515\tValidation Loss: 0.520\tTime: 19.82207179069519\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.470\tValidation Loss: 0.471\tTime: 19.908567190170288\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.435\tValidation Loss: 0.430\tTime: 19.51427984237671\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.389\tValidation Loss: 0.393\tTime: 19.35019564628601\n",
            "Saving model 9\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 10 / 25\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.688\tValidation Loss: 0.678\tTime: 8.140584707260132\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.677\tValidation Loss: 0.680\tTime: 6.112443685531616\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.670\tValidation Loss: 0.661\tTime: 9.130825996398926\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.665\tValidation Loss: 0.655\tTime: 9.248009443283081\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.651\tValidation Loss: 0.641\tTime: 9.13002634048462\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.637\tValidation Loss: 0.627\tTime: 8.89483380317688\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.638\tValidation Loss: 0.621\tTime: 9.042594909667969\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.620\tValidation Loss: 0.610\tTime: 8.889613389968872\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.608\tValidation Loss: 0.595\tTime: 8.759715557098389\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.592\tValidation Loss: 0.577\tTime: 8.52224326133728\n",
            "Saving model 10\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 11 / 25\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.688\tValidation Loss: 0.686\tTime: 21.574922561645508\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.686\tValidation Loss: 0.669\tTime: 22.2239191532135\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.668\tValidation Loss: 0.673\tTime: 17.45039129257202\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.661\tValidation Loss: 0.657\tTime: 22.184667110443115\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.652\tValidation Loss: 0.651\tTime: 22.201545238494873\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.641\tValidation Loss: 0.625\tTime: 22.135294914245605\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.633\tValidation Loss: 0.618\tTime: 21.64305877685547\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.620\tValidation Loss: 0.618\tTime: 21.43834352493286\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.609\tValidation Loss: 0.583\tTime: 22.015984296798706\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.592\tValidation Loss: 0.574\tTime: 20.995299577713013\n",
            "Saving model 11\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 12 / 25\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.636\tValidation Loss: 0.571\tTime: 6.874417066574097\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.535\tValidation Loss: 0.486\tTime: 6.561742305755615\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.467\tValidation Loss: 0.425\tTime: 6.547701358795166\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.417\tValidation Loss: 0.377\tTime: 6.904219627380371\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.370\tValidation Loss: 0.339\tTime: 6.810344696044922\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.341\tValidation Loss: 0.316\tTime: 7.012528657913208\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.309\tValidation Loss: 0.292\tTime: 6.83024787902832\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.283\tValidation Loss: 0.265\tTime: 6.911852836608887\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.257\tValidation Loss: 0.286\tTime: 6.100362062454224\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.247\tValidation Loss: 0.246\tTime: 6.8182053565979\n",
            "Saving model 12\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment: 13 / 25\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.671\tValidation Loss: 0.636\tTime: 7.148282766342163\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.571\tValidation Loss: 0.509\tTime: 7.3503124713897705\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.406\tValidation Loss: 0.309\tTime: 7.2253453731536865\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.258\tValidation Loss: 0.218\tTime: 7.285539627075195\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.218\tValidation Loss: 0.195\tTime: 7.14328408241272\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.183\tValidation Loss: 0.216\tTime: 6.274386644363403\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.189\tValidation Loss: 0.181\tTime: 7.201221466064453\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.187\tValidation Loss: 0.149\tTime: 7.2696287631988525\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.173\tValidation Loss: 0.173\tTime: 6.390199661254883\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.162\tValidation Loss: 0.130\tTime: 7.115704536437988\n",
            "Saving model 13\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment: 14 / 25\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.658\tValidation Loss: 0.608\tTime: 19.66554856300354\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.509\tValidation Loss: 0.420\tTime: 19.343764543533325\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.323\tValidation Loss: 0.261\tTime: 19.417988300323486\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.226\tValidation Loss: 0.216\tTime: 19.413706064224243\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.204\tValidation Loss: 0.191\tTime: 19.21601414680481\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.188\tValidation Loss: 0.172\tTime: 19.3000009059906\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.172\tValidation Loss: 0.164\tTime: 19.52919316291809\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.171\tValidation Loss: 0.158\tTime: 19.926947832107544\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.162\tValidation Loss: 0.149\tTime: 20.06926155090332\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.161\tValidation Loss: 0.147\tTime: 19.818901777267456\n",
            "Saving model 14\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 15 / 25\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.692\tValidation Loss: 0.688\tTime: 9.117328405380249\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.689\tValidation Loss: 0.686\tTime: 9.201773405075073\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.686\tValidation Loss: 0.685\tTime: 9.204100131988525\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.686\tValidation Loss: 0.683\tTime: 8.62373948097229\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.685\tValidation Loss: 0.682\tTime: 8.594176292419434\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.683\tValidation Loss: 0.681\tTime: 8.631101131439209\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.683\tValidation Loss: 0.680\tTime: 8.578443050384521\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.684\tValidation Loss: 0.679\tTime: 8.609221696853638\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.680\tValidation Loss: 0.678\tTime: 8.540823936462402\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.681\tValidation Loss: 0.677\tTime: 8.57925534248352\n",
            "Saving model 15\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 16 / 25\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.693\tValidation Loss: 0.691\tTime: 21.440985679626465\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.690\tValidation Loss: 0.686\tTime: 21.27452516555786\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.688\tValidation Loss: 0.685\tTime: 21.447784185409546\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.689\tValidation Loss: 0.686\tTime: 17.497167110443115\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.687\tValidation Loss: 0.682\tTime: 21.63076138496399\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.685\tValidation Loss: 0.681\tTime: 21.838478803634644\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.682\tValidation Loss: 0.679\tTime: 22.255830764770508\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.682\tValidation Loss: 0.682\tTime: 17.445269107818604\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.678\tValidation Loss: 0.676\tTime: 21.230704307556152\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.679\tValidation Loss: 0.674\tTime: 21.68586277961731\n",
            "Saving model 16\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 17 / 25\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.693\tValidation Loss: 0.680\tTime: 7.125881671905518\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.673\tValidation Loss: 0.665\tTime: 6.6738808155059814\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.656\tValidation Loss: 0.648\tTime: 6.827911853790283\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.644\tValidation Loss: 0.636\tTime: 6.494648694992065\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.632\tValidation Loss: 0.623\tTime: 6.653598308563232\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.618\tValidation Loss: 0.613\tTime: 6.651676893234253\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.610\tValidation Loss: 0.603\tTime: 6.534994125366211\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.592\tValidation Loss: 0.585\tTime: 6.534541368484497\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.590\tValidation Loss: 0.581\tTime: 6.549392938613892\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.576\tValidation Loss: 0.566\tTime: 6.484116554260254\n",
            "Saving model 17\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment: 18 / 25\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.690\tValidation Loss: 0.687\tTime: 6.858391761779785\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.687\tValidation Loss: 0.684\tTime: 7.22352409362793\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.684\tValidation Loss: 0.681\tTime: 7.164997816085815\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.678\tValidation Loss: 0.676\tTime: 7.3879923820495605\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.674\tValidation Loss: 0.672\tTime: 7.255888938903809\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.668\tValidation Loss: 0.665\tTime: 7.054116487503052\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.662\tValidation Loss: 0.658\tTime: 7.162554025650024\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.654\tValidation Loss: 0.651\tTime: 7.135417222976685\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.645\tValidation Loss: 0.642\tTime: 7.110525131225586\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.634\tValidation Loss: 0.632\tTime: 7.369479656219482\n",
            "Saving model 18\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment: 19 / 25\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.690\tValidation Loss: 0.685\tTime: 20.147188901901245\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.682\tValidation Loss: 0.678\tTime: 19.832292795181274\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.676\tValidation Loss: 0.669\tTime: 19.61144232749939\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.664\tValidation Loss: 0.663\tTime: 19.324511766433716\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.655\tValidation Loss: 0.653\tTime: 19.229888200759888\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.642\tValidation Loss: 0.642\tTime: 19.492387771606445\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.628\tValidation Loss: 0.629\tTime: 19.20427107810974\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.618\tValidation Loss: 0.618\tTime: 19.02246117591858\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.595\tValidation Loss: 0.603\tTime: 19.08474040031433\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.582\tValidation Loss: 0.592\tTime: 19.716729640960693\n",
            "Saving model 19\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 20 / 25\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.692\tValidation Loss: 0.691\tTime: 8.669955015182495\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.692\tValidation Loss: 0.690\tTime: 9.120801210403442\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.690\tValidation Loss: 0.689\tTime: 8.859442949295044\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.690\tValidation Loss: 0.688\tTime: 9.061519861221313\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.688\tValidation Loss: 0.688\tTime: 9.318010807037354\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.690\tValidation Loss: 0.687\tTime: 9.00506043434143\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.688\tValidation Loss: 0.687\tTime: 8.964369535446167\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.686\tValidation Loss: 0.687\tTime: 9.408500671386719\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.687\tValidation Loss: 0.686\tTime: 8.987514972686768\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.687\tValidation Loss: 0.686\tTime: 8.775242567062378\n",
            "Saving model 20\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 21 / 25\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.692\tValidation Loss: 0.691\tTime: 21.587594985961914\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.693\tValidation Loss: 0.691\tTime: 21.56532049179077\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.692\tValidation Loss: 0.690\tTime: 21.514600038528442\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.691\tValidation Loss: 0.689\tTime: 21.274529933929443\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.691\tValidation Loss: 0.689\tTime: 21.262617826461792\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.690\tValidation Loss: 0.688\tTime: 21.260697603225708\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.690\tValidation Loss: 0.688\tTime: 21.643112182617188\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.690\tValidation Loss: 0.688\tTime: 21.9432315826416\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.689\tValidation Loss: 0.687\tTime: 21.93039631843567\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.689\tValidation Loss: 0.686\tTime: 21.917057752609253\n",
            "Saving model 21\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 22 / 25\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.687\tValidation Loss: 0.684\tTime: 7.239932298660278\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.683\tValidation Loss: 0.680\tTime: 6.545945882797241\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.680\tValidation Loss: 0.676\tTime: 6.736321687698364\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.675\tValidation Loss: 0.673\tTime: 5.847983121871948\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.671\tValidation Loss: 0.669\tTime: 5.5728912353515625\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.666\tValidation Loss: 0.665\tTime: 5.635416507720947\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.667\tValidation Loss: 0.662\tTime: 5.612251281738281\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.661\tValidation Loss: 0.658\tTime: 5.589742660522461\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.658\tValidation Loss: 0.655\tTime: 5.658547639846802\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.656\tValidation Loss: 0.652\tTime: 5.5881078243255615\n",
            "Saving model 22\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment: 23 / 25\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.691\tValidation Loss: 0.689\tTime: 5.909358501434326\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.691\tValidation Loss: 0.688\tTime: 6.878870248794556\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.690\tValidation Loss: 0.688\tTime: 7.125334024429321\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.689\tValidation Loss: 0.687\tTime: 7.085195064544678\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.687\tValidation Loss: 0.686\tTime: 7.0030364990234375\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.687\tValidation Loss: 0.685\tTime: 6.887040138244629\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.685\tValidation Loss: 0.685\tTime: 6.841687440872192\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.686\tValidation Loss: 0.684\tTime: 6.862112283706665\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.684\tValidation Loss: 0.683\tTime: 6.835390090942383\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.684\tValidation Loss: 0.682\tTime: 7.520166397094727\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\admin\\anaconda3\\envs\\www\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "c:\\Users\\admin\\anaconda3\\envs\\www\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "c:\\Users\\admin\\anaconda3\\envs\\www\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving model 23\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment: 24 / 25\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.699\tValidation Loss: 0.694\tTime: 20.125925302505493\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.691\tValidation Loss: 0.690\tTime: 19.70368480682373\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.689\tValidation Loss: 0.688\tTime: 19.86930751800537\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.685\tValidation Loss: 0.685\tTime: 19.780381441116333\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.684\tValidation Loss: 0.683\tTime: 19.733349084854126\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.682\tValidation Loss: 0.682\tTime: 19.35635781288147\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.680\tValidation Loss: 0.681\tTime: 19.4407856464386\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.679\tValidation Loss: 0.679\tTime: 19.214702129364014\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.677\tValidation Loss: 0.676\tTime: 19.07482600212097\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.676\tValidation Loss: 0.674\tTime: 19.098419666290283\n",
            "Saving model 24\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n"
          ]
        }
      ],
      "source": [
        "lrs = [0.0001, 0.00001, 0.00005, 0.000005, 0.000001]\n",
        "exp_title = 'test_lr_stable'\n",
        "# xlm-roberta-base xlm-roberta-large bert-base-uncased roberta-base roberta-large\n",
        "pretrained_model_names = ['xlm-roberta-base', 'xlm-roberta-large', 'bert-base-uncased', 'roberta-base', 'roberta-large']\n",
        "model_type = Ext_Arch\n",
        "optimizer_type = torch.optim.AdamW\n",
        "loss_func_type = nn.NLLLoss\n",
        "max_length = 25 \n",
        "padding = 'max_length'\n",
        "truncation=True\n",
        "batch_size = 32\n",
        "epochs=10\n",
        "\n",
        "\n",
        "\n",
        "exp_index = 0\n",
        "total_exp = len(lrs) * len(pretrained_model_names)\n",
        "for lr in lrs:\n",
        "    for pretrained_model_name in pretrained_model_names:\n",
        "        model = get_model(pretrained_model_name, model_type, device)\n",
        "        train_dataloader, val_dataloader, test_data, weights = tokenize_data(pretrained_model_name, max_length, padding, truncation, batch_size, device)\n",
        "        loss_func  = loss_func_type(weight=weights)\n",
        "\n",
        "        print(f'Experiment: {exp_index} / {total_exp}')\n",
        "        optimizer = optimizer_type(model.parameters(), lr=lr)\n",
        "        do_exp(model, train_dataloader, val_dataloader, test_data, device, loss_func, optimizer, epochs, lr, \n",
        "            batch_size, max_length, padding, truncation, pretrained_model_name, exp_title, exp_index, root)\n",
        "        exp_index += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Test lr, epoch dep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment: 0 / 25\n",
            "  Epoch 1 / 1\n",
            "    Training Loss: 0.689\tValidation Loss: 0.694\tTime: 8.305525779724121\n",
            "Saving model 0\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 1 / 25\n",
            "  Epoch 1 / 1\n",
            "    Training Loss: 0.690\tValidation Loss: 0.669\tTime: 22.299724578857422\n",
            "Saving model 1\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 2 / 25\n",
            "  Epoch 1 / 1\n",
            "    Training Loss: 0.604\tValidation Loss: 0.501\tTime: 7.1038734912872314\n",
            "Saving model 2\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment: 3 / 25\n",
            "  Epoch 1 / 1\n",
            "    Training Loss: 0.642\tValidation Loss: 0.526\tTime: 7.261673927307129\n",
            "Saving model 3\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment: 4 / 25\n",
            "  Epoch 1 / 1\n",
            "    Training Loss: 0.617\tValidation Loss: 0.502\tTime: 19.45695686340332\n",
            "Saving model 4\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 5 / 25\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.689\tValidation Loss: 0.688\tTime: 8.605825901031494\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.687\tValidation Loss: 0.684\tTime: 8.967984199523926\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.683\tValidation Loss: 0.682\tTime: 8.902788639068604\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.680\tValidation Loss: 0.678\tTime: 8.809981107711792\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.679\tValidation Loss: 0.674\tTime: 8.60951280593872\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.676\tValidation Loss: 0.672\tTime: 8.69033932685852\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.674\tValidation Loss: 0.673\tTime: 6.19948148727417\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.671\tValidation Loss: 0.668\tTime: 8.511719226837158\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.670\tValidation Loss: 0.667\tTime: 8.497183322906494\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.668\tValidation Loss: 0.670\tTime: 6.137986898422241\n",
            "Saving model 5\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 6 / 25\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.691\tValidation Loss: 0.687\tTime: 21.552088499069214\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.686\tValidation Loss: 0.681\tTime: 21.613912343978882\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.684\tValidation Loss: 0.682\tTime: 17.0469708442688\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.682\tValidation Loss: 0.680\tTime: 21.553411722183228\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.679\tValidation Loss: 0.672\tTime: 21.961207151412964\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.678\tValidation Loss: 0.677\tTime: 17.509825944900513\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.673\tValidation Loss: 0.669\tTime: 21.663870811462402\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.669\tValidation Loss: 0.665\tTime: 22.10545778274536\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.671\tValidation Loss: 0.678\tTime: 17.54410696029663\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.668\tValidation Loss: 0.661\tTime: 21.282437086105347\n",
            "Saving model 6\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 7 / 25\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.680\tValidation Loss: 0.658\tTime: 7.191450595855713\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.646\tValidation Loss: 0.629\tTime: 6.831500291824341\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.618\tValidation Loss: 0.603\tTime: 6.642383813858032\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.595\tValidation Loss: 0.579\tTime: 6.77571439743042\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.577\tValidation Loss: 0.560\tTime: 6.790369987487793\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.554\tValidation Loss: 0.537\tTime: 6.594111919403076\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.532\tValidation Loss: 0.518\tTime: 6.6564247608184814\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.515\tValidation Loss: 0.501\tTime: 6.691840410232544\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.499\tValidation Loss: 0.491\tTime: 6.588455677032471\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.477\tValidation Loss: 0.466\tTime: 6.580073356628418\n",
            "Saving model 7\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment: 8 / 25\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.690\tValidation Loss: 0.683\tTime: 6.936259031295776\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.680\tValidation Loss: 0.676\tTime: 6.833522081375122\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.670\tValidation Loss: 0.664\tTime: 6.6956610679626465\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.655\tValidation Loss: 0.649\tTime: 6.852328300476074\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.637\tValidation Loss: 0.628\tTime: 7.187200546264648\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.611\tValidation Loss: 0.604\tTime: 7.17669939994812\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.581\tValidation Loss: 0.568\tTime: 7.1889636516571045\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.538\tValidation Loss: 0.532\tTime: 7.214895248413086\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.503\tValidation Loss: 0.490\tTime: 7.135709047317505\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.457\tValidation Loss: 0.446\tTime: 7.108999967575073\n",
            "Saving model 8\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment: 9 / 25\n",
            "  Epoch 1 / 10\n",
            "    Training Loss: 0.687\tValidation Loss: 0.676\tTime: 19.293002605438232\n",
            "  Epoch 2 / 10\n",
            "    Training Loss: 0.668\tValidation Loss: 0.661\tTime: 19.546091556549072\n",
            "  Epoch 3 / 10\n",
            "    Training Loss: 0.647\tValidation Loss: 0.645\tTime: 19.718096494674683\n",
            "  Epoch 4 / 10\n",
            "    Training Loss: 0.624\tValidation Loss: 0.620\tTime: 19.489075899124146\n",
            "  Epoch 5 / 10\n",
            "    Training Loss: 0.597\tValidation Loss: 0.592\tTime: 19.57503318786621\n",
            "  Epoch 6 / 10\n",
            "    Training Loss: 0.556\tValidation Loss: 0.551\tTime: 19.357890367507935\n",
            "  Epoch 7 / 10\n",
            "    Training Loss: 0.518\tValidation Loss: 0.515\tTime: 19.521750688552856\n",
            "  Epoch 8 / 10\n",
            "    Training Loss: 0.475\tValidation Loss: 0.477\tTime: 19.367180585861206\n",
            "  Epoch 9 / 10\n",
            "    Training Loss: 0.424\tValidation Loss: 0.428\tTime: 19.118544340133667\n",
            "  Epoch 10 / 10\n",
            "    Training Loss: 0.394\tValidation Loss: 0.388\tTime: 19.48832631111145\n",
            "Saving model 9\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 10 / 25\n",
            "  Epoch 1 / 2\n",
            "    Training Loss: 0.690\tValidation Loss: 0.683\tTime: 9.39280366897583\n",
            "  Epoch 2 / 2\n",
            "    Training Loss: 0.681\tValidation Loss: 0.674\tTime: 9.047741889953613\n",
            "Saving model 10\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 11 / 25\n",
            "  Epoch 1 / 2\n",
            "    Training Loss: 0.690\tValidation Loss: 0.683\tTime: 22.07840847969055\n",
            "  Epoch 2 / 2\n",
            "    Training Loss: 0.680\tValidation Loss: 0.669\tTime: 21.47667932510376\n",
            "Saving model 11\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 12 / 25\n",
            "  Epoch 1 / 2\n",
            "    Training Loss: 0.627\tValidation Loss: 0.572\tTime: 7.423233985900879\n",
            "  Epoch 2 / 2\n",
            "    Training Loss: 0.526\tValidation Loss: 0.472\tTime: 6.88323712348938\n",
            "Saving model 12\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment: 13 / 25\n",
            "  Epoch 1 / 2\n",
            "    Training Loss: 0.674\tValidation Loss: 0.642\tTime: 7.200195550918579\n",
            "  Epoch 2 / 2\n",
            "    Training Loss: 0.571\tValidation Loss: 0.493\tTime: 7.065182447433472\n",
            "Saving model 13\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment: 14 / 25\n",
            "  Epoch 1 / 2\n",
            "    Training Loss: 0.662\tValidation Loss: 0.617\tTime: 19.55107593536377\n",
            "  Epoch 2 / 2\n",
            "    Training Loss: 0.525\tValidation Loss: 0.434\tTime: 20.029360055923462\n",
            "Saving model 14\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 15 / 25\n",
            "  Epoch 1 / 20\n",
            "    Training Loss: 0.692\tValidation Loss: 0.688\tTime: 8.60250473022461\n",
            "  Epoch 2 / 20\n",
            "    Training Loss: 0.689\tValidation Loss: 0.687\tTime: 8.549460649490356\n",
            "  Epoch 3 / 20\n",
            "    Training Loss: 0.688\tValidation Loss: 0.685\tTime: 8.906750679016113\n",
            "  Epoch 4 / 20\n",
            "    Training Loss: 0.685\tValidation Loss: 0.685\tTime: 9.140103578567505\n",
            "  Epoch 5 / 20\n",
            "    Training Loss: 0.683\tValidation Loss: 0.682\tTime: 8.846349716186523\n",
            "  Epoch 6 / 20\n",
            "    Training Loss: 0.683\tValidation Loss: 0.682\tTime: 8.734655618667603\n",
            "  Epoch 7 / 20\n",
            "    Training Loss: 0.679\tValidation Loss: 0.680\tTime: 8.401552200317383\n",
            "  Epoch 8 / 20\n",
            "    Training Loss: 0.681\tValidation Loss: 0.680\tTime: 8.723973751068115\n",
            "  Epoch 9 / 20\n",
            "    Training Loss: 0.679\tValidation Loss: 0.677\tTime: 8.898946046829224\n",
            "  Epoch 10 / 20\n",
            "    Training Loss: 0.683\tValidation Loss: 0.676\tTime: 9.024626016616821\n",
            "  Epoch 11 / 20\n",
            "    Training Loss: 0.676\tValidation Loss: 0.678\tTime: 6.67641806602478\n",
            "  Epoch 12 / 20\n",
            "    Training Loss: 0.679\tValidation Loss: 0.674\tTime: 9.308583498001099\n",
            "  Epoch 13 / 20\n",
            "    Training Loss: 0.677\tValidation Loss: 0.673\tTime: 9.225464582443237\n",
            "  Epoch 14 / 20\n",
            "    Training Loss: 0.674\tValidation Loss: 0.673\tTime: 9.356030941009521\n",
            "  Epoch 15 / 20\n",
            "    Training Loss: 0.672\tValidation Loss: 0.671\tTime: 9.196622848510742\n",
            "  Epoch 16 / 20\n",
            "    Training Loss: 0.672\tValidation Loss: 0.671\tTime: 9.206594944000244\n",
            "  Epoch 17 / 20\n",
            "    Training Loss: 0.673\tValidation Loss: 0.670\tTime: 9.164611577987671\n",
            "  Epoch 18 / 20\n",
            "    Training Loss: 0.672\tValidation Loss: 0.669\tTime: 9.251387357711792\n",
            "  Epoch 19 / 20\n",
            "    Training Loss: 0.672\tValidation Loss: 0.669\tTime: 6.3884265422821045\n",
            "  Epoch 20 / 20\n",
            "    Training Loss: 0.670\tValidation Loss: 0.670\tTime: 6.273738384246826\n",
            "Saving model 15\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 16 / 25\n",
            "  Epoch 1 / 20\n",
            "    Training Loss: 0.692\tValidation Loss: 0.690\tTime: 21.719252109527588\n",
            "  Epoch 2 / 20\n",
            "    Training Loss: 0.689\tValidation Loss: 0.687\tTime: 21.862343549728394\n",
            "  Epoch 3 / 20\n",
            "    Training Loss: 0.690\tValidation Loss: 0.688\tTime: 17.48815941810608\n",
            "  Epoch 4 / 20\n",
            "    Training Loss: 0.687\tValidation Loss: 0.685\tTime: 21.416710376739502\n",
            "  Epoch 5 / 20\n",
            "    Training Loss: 0.686\tValidation Loss: 0.683\tTime: 21.4706289768219\n",
            "  Epoch 6 / 20\n",
            "    Training Loss: 0.682\tValidation Loss: 0.681\tTime: 21.314714670181274\n",
            "  Epoch 7 / 20\n",
            "    Training Loss: 0.684\tValidation Loss: 0.680\tTime: 22.467296600341797\n",
            "  Epoch 8 / 20\n",
            "    Training Loss: 0.680\tValidation Loss: 0.678\tTime: 22.16298818588257\n",
            "  Epoch 9 / 20\n",
            "    Training Loss: 0.682\tValidation Loss: 0.678\tTime: 22.21227765083313\n",
            "  Epoch 10 / 20\n",
            "    Training Loss: 0.680\tValidation Loss: 0.675\tTime: 22.554903507232666\n",
            "  Epoch 11 / 20\n",
            "    Training Loss: 0.676\tValidation Loss: 0.673\tTime: 22.60679531097412\n",
            "  Epoch 12 / 20\n",
            "    Training Loss: 0.677\tValidation Loss: 0.672\tTime: 21.864072561264038\n",
            "  Epoch 13 / 20\n",
            "    Training Loss: 0.674\tValidation Loss: 0.672\tTime: 21.658108711242676\n",
            "  Epoch 14 / 20\n",
            "    Training Loss: 0.674\tValidation Loss: 0.670\tTime: 21.80705714225769\n",
            "  Epoch 15 / 20\n",
            "    Training Loss: 0.670\tValidation Loss: 0.668\tTime: 22.019073247909546\n",
            "  Epoch 16 / 20\n",
            "    Training Loss: 0.670\tValidation Loss: 0.668\tTime: 16.9365816116333\n",
            "  Epoch 17 / 20\n",
            "    Training Loss: 0.669\tValidation Loss: 0.664\tTime: 20.771466493606567\n",
            "  Epoch 18 / 20\n",
            "    Training Loss: 0.669\tValidation Loss: 0.666\tTime: 17.8349928855896\n",
            "  Epoch 19 / 20\n",
            "    Training Loss: 0.668\tValidation Loss: 0.664\tTime: 22.04468297958374\n",
            "  Epoch 20 / 20\n",
            "    Training Loss: 0.666\tValidation Loss: 0.661\tTime: 22.259986639022827\n",
            "Saving model 16\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 17 / 25\n",
            "  Epoch 1 / 20\n",
            "    Training Loss: 0.679\tValidation Loss: 0.670\tTime: 7.6309754848480225\n",
            "  Epoch 2 / 20\n",
            "    Training Loss: 0.662\tValidation Loss: 0.651\tTime: 7.093231439590454\n",
            "  Epoch 3 / 20\n",
            "    Training Loss: 0.647\tValidation Loss: 0.634\tTime: 7.130752086639404\n",
            "  Epoch 4 / 20\n",
            "    Training Loss: 0.633\tValidation Loss: 0.622\tTime: 7.040127992630005\n",
            "  Epoch 5 / 20\n",
            "    Training Loss: 0.620\tValidation Loss: 0.613\tTime: 6.861365079879761\n",
            "  Epoch 6 / 20\n",
            "    Training Loss: 0.604\tValidation Loss: 0.598\tTime: 6.880627155303955\n",
            "  Epoch 7 / 20\n",
            "    Training Loss: 0.599\tValidation Loss: 0.584\tTime: 6.988003969192505\n",
            "  Epoch 8 / 20\n",
            "    Training Loss: 0.584\tValidation Loss: 0.571\tTime: 6.679986476898193\n",
            "  Epoch 9 / 20\n",
            "    Training Loss: 0.573\tValidation Loss: 0.564\tTime: 6.741818904876709\n",
            "  Epoch 10 / 20\n",
            "    Training Loss: 0.561\tValidation Loss: 0.549\tTime: 6.816537380218506\n",
            "  Epoch 11 / 20\n",
            "    Training Loss: 0.552\tValidation Loss: 0.539\tTime: 6.895442247390747\n",
            "  Epoch 12 / 20\n",
            "    Training Loss: 0.541\tValidation Loss: 0.528\tTime: 6.588886499404907\n",
            "  Epoch 13 / 20\n",
            "    Training Loss: 0.531\tValidation Loss: 0.518\tTime: 6.762075901031494\n",
            "  Epoch 14 / 20\n",
            "    Training Loss: 0.522\tValidation Loss: 0.511\tTime: 6.682839393615723\n",
            "  Epoch 15 / 20\n",
            "    Training Loss: 0.510\tValidation Loss: 0.507\tTime: 6.82219123840332\n",
            "  Epoch 16 / 20\n",
            "    Training Loss: 0.503\tValidation Loss: 0.491\tTime: 6.8592963218688965\n",
            "  Epoch 17 / 20\n",
            "    Training Loss: 0.489\tValidation Loss: 0.488\tTime: 6.713922023773193\n",
            "  Epoch 18 / 20\n",
            "    Training Loss: 0.490\tValidation Loss: 0.476\tTime: 6.737200498580933\n",
            "  Epoch 19 / 20\n",
            "    Training Loss: 0.479\tValidation Loss: 0.467\tTime: 6.6871514320373535\n",
            "  Epoch 20 / 20\n",
            "    Training Loss: 0.475\tValidation Loss: 0.462\tTime: 6.660415172576904\n",
            "Saving model 17\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment: 18 / 25\n",
            "  Epoch 1 / 20\n",
            "    Training Loss: 0.691\tValidation Loss: 0.687\tTime: 7.426605939865112\n",
            "  Epoch 2 / 20\n",
            "    Training Loss: 0.686\tValidation Loss: 0.683\tTime: 7.338244199752808\n",
            "  Epoch 3 / 20\n",
            "    Training Loss: 0.681\tValidation Loss: 0.678\tTime: 7.3635337352752686\n",
            "  Epoch 4 / 20\n",
            "    Training Loss: 0.675\tValidation Loss: 0.672\tTime: 7.108066558837891\n",
            "  Epoch 5 / 20\n",
            "    Training Loss: 0.669\tValidation Loss: 0.667\tTime: 7.1007630825042725\n",
            "  Epoch 6 / 20\n",
            "    Training Loss: 0.662\tValidation Loss: 0.660\tTime: 7.124624490737915\n",
            "  Epoch 7 / 20\n",
            "    Training Loss: 0.654\tValidation Loss: 0.653\tTime: 7.136192321777344\n",
            "  Epoch 8 / 20\n",
            "    Training Loss: 0.646\tValidation Loss: 0.644\tTime: 7.134419202804565\n",
            "  Epoch 9 / 20\n",
            "    Training Loss: 0.637\tValidation Loss: 0.634\tTime: 7.30797266960144\n",
            "  Epoch 10 / 20\n",
            "    Training Loss: 0.623\tValidation Loss: 0.622\tTime: 7.185981512069702\n",
            "  Epoch 11 / 20\n",
            "    Training Loss: 0.609\tValidation Loss: 0.608\tTime: 7.369380950927734\n",
            "  Epoch 12 / 20\n",
            "    Training Loss: 0.596\tValidation Loss: 0.594\tTime: 7.272578477859497\n",
            "  Epoch 13 / 20\n",
            "    Training Loss: 0.579\tValidation Loss: 0.578\tTime: 7.3038434982299805\n",
            "  Epoch 14 / 20\n",
            "    Training Loss: 0.559\tValidation Loss: 0.559\tTime: 7.164158582687378\n",
            "  Epoch 15 / 20\n",
            "    Training Loss: 0.540\tValidation Loss: 0.540\tTime: 7.280386686325073\n",
            "  Epoch 16 / 20\n",
            "    Training Loss: 0.518\tValidation Loss: 0.518\tTime: 7.349917411804199\n",
            "  Epoch 17 / 20\n",
            "    Training Loss: 0.497\tValidation Loss: 0.498\tTime: 7.049682855606079\n",
            "  Epoch 18 / 20\n",
            "    Training Loss: 0.478\tValidation Loss: 0.476\tTime: 6.953489542007446\n",
            "  Epoch 19 / 20\n",
            "    Training Loss: 0.452\tValidation Loss: 0.454\tTime: 7.161326169967651\n",
            "  Epoch 20 / 20\n",
            "    Training Loss: 0.427\tValidation Loss: 0.431\tTime: 7.086909532546997\n",
            "Saving model 18\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment: 19 / 25\n",
            "  Epoch 1 / 20\n",
            "    Training Loss: 0.693\tValidation Loss: 0.685\tTime: 19.85492777824402\n",
            "  Epoch 2 / 20\n",
            "    Training Loss: 0.682\tValidation Loss: 0.677\tTime: 19.657390117645264\n",
            "  Epoch 3 / 20\n",
            "    Training Loss: 0.676\tValidation Loss: 0.670\tTime: 19.50367021560669\n",
            "  Epoch 4 / 20\n",
            "    Training Loss: 0.666\tValidation Loss: 0.663\tTime: 19.085962772369385\n",
            "  Epoch 5 / 20\n",
            "    Training Loss: 0.655\tValidation Loss: 0.653\tTime: 19.531947374343872\n",
            "  Epoch 6 / 20\n",
            "    Training Loss: 0.643\tValidation Loss: 0.643\tTime: 20.070655345916748\n",
            "  Epoch 7 / 20\n",
            "    Training Loss: 0.630\tValidation Loss: 0.631\tTime: 19.6427960395813\n",
            "  Epoch 8 / 20\n",
            "    Training Loss: 0.618\tValidation Loss: 0.618\tTime: 20.163737773895264\n",
            "  Epoch 9 / 20\n",
            "    Training Loss: 0.603\tValidation Loss: 0.605\tTime: 19.97196316719055\n",
            "  Epoch 10 / 20\n",
            "    Training Loss: 0.586\tValidation Loss: 0.592\tTime: 19.931375980377197\n",
            "  Epoch 11 / 20\n",
            "    Training Loss: 0.568\tValidation Loss: 0.571\tTime: 19.691742658615112\n",
            "  Epoch 12 / 20\n",
            "    Training Loss: 0.543\tValidation Loss: 0.552\tTime: 19.68345856666565\n",
            "  Epoch 13 / 20\n",
            "    Training Loss: 0.527\tValidation Loss: 0.533\tTime: 19.467190980911255\n",
            "  Epoch 14 / 20\n",
            "    Training Loss: 0.503\tValidation Loss: 0.509\tTime: 19.261512517929077\n",
            "  Epoch 15 / 20\n",
            "    Training Loss: 0.486\tValidation Loss: 0.489\tTime: 19.435879707336426\n",
            "  Epoch 16 / 20\n",
            "    Training Loss: 0.455\tValidation Loss: 0.472\tTime: 19.174171447753906\n",
            "  Epoch 17 / 20\n",
            "    Training Loss: 0.439\tValidation Loss: 0.450\tTime: 19.64514946937561\n",
            "  Epoch 18 / 20\n",
            "    Training Loss: 0.420\tValidation Loss: 0.427\tTime: 19.928940057754517\n",
            "  Epoch 19 / 20\n",
            "    Training Loss: 0.399\tValidation Loss: 0.406\tTime: 20.014784574508667\n",
            "  Epoch 20 / 20\n",
            "    Training Loss: 0.387\tValidation Loss: 0.387\tTime: 19.6927752494812\n",
            "Saving model 19\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 20 / 25\n",
            "  Epoch 1 / 100\n",
            "    Training Loss: 0.693\tValidation Loss: 0.690\tTime: 9.022475957870483\n",
            "  Epoch 2 / 100\n",
            "    Training Loss: 0.691\tValidation Loss: 0.689\tTime: 9.046724796295166\n",
            "  Epoch 3 / 100\n",
            "    Training Loss: 0.691\tValidation Loss: 0.689\tTime: 9.138295888900757\n",
            "  Epoch 4 / 100\n",
            "    Training Loss: 0.690\tValidation Loss: 0.688\tTime: 9.209532260894775\n",
            "  Epoch 5 / 100\n",
            "    Training Loss: 0.690\tValidation Loss: 0.688\tTime: 9.280846118927002\n",
            "  Epoch 6 / 100\n",
            "    Training Loss: 0.689\tValidation Loss: 0.688\tTime: 8.965817928314209\n",
            "  Epoch 7 / 100\n",
            "    Training Loss: 0.689\tValidation Loss: 0.687\tTime: 8.996744871139526\n",
            "  Epoch 8 / 100\n",
            "    Training Loss: 0.689\tValidation Loss: 0.687\tTime: 8.669588804244995\n",
            "  Epoch 9 / 100\n",
            "    Training Loss: 0.688\tValidation Loss: 0.687\tTime: 8.869143724441528\n",
            "  Epoch 10 / 100\n",
            "    Training Loss: 0.688\tValidation Loss: 0.686\tTime: 8.830598592758179\n",
            "  Epoch 11 / 100\n",
            "    Training Loss: 0.686\tValidation Loss: 0.686\tTime: 8.901650190353394\n",
            "  Epoch 12 / 100\n",
            "    Training Loss: 0.688\tValidation Loss: 0.686\tTime: 8.890707969665527\n",
            "  Epoch 13 / 100\n",
            "    Training Loss: 0.688\tValidation Loss: 0.685\tTime: 8.984771490097046\n",
            "  Epoch 14 / 100\n",
            "    Training Loss: 0.686\tValidation Loss: 0.685\tTime: 8.703003406524658\n",
            "  Epoch 15 / 100\n",
            "    Training Loss: 0.685\tValidation Loss: 0.685\tTime: 8.771463394165039\n",
            "  Epoch 16 / 100\n",
            "    Training Loss: 0.685\tValidation Loss: 0.684\tTime: 8.770021677017212\n",
            "  Epoch 17 / 100\n",
            "    Training Loss: 0.686\tValidation Loss: 0.684\tTime: 8.715786457061768\n",
            "  Epoch 18 / 100\n",
            "    Training Loss: 0.684\tValidation Loss: 0.684\tTime: 8.824793577194214\n",
            "  Epoch 19 / 100\n",
            "    Training Loss: 0.684\tValidation Loss: 0.684\tTime: 6.1860363483428955\n",
            "  Epoch 20 / 100\n",
            "    Training Loss: 0.687\tValidation Loss: 0.683\tTime: 9.925772190093994\n",
            "  Epoch 21 / 100\n",
            "    Training Loss: 0.685\tValidation Loss: 0.683\tTime: 10.04896330833435\n",
            "  Epoch 22 / 100\n",
            "    Training Loss: 0.684\tValidation Loss: 0.682\tTime: 9.938946723937988\n",
            "  Epoch 23 / 100\n",
            "    Training Loss: 0.683\tValidation Loss: 0.682\tTime: 9.171308040618896\n",
            "  Epoch 24 / 100\n",
            "    Training Loss: 0.684\tValidation Loss: 0.682\tTime: 9.397759437561035\n",
            "  Epoch 25 / 100\n",
            "    Training Loss: 0.685\tValidation Loss: 0.681\tTime: 8.930374383926392\n",
            "  Epoch 26 / 100\n",
            "    Training Loss: 0.682\tValidation Loss: 0.681\tTime: 9.548693180084229\n",
            "  Epoch 27 / 100\n",
            "    Training Loss: 0.684\tValidation Loss: 0.681\tTime: 9.465033292770386\n",
            "  Epoch 28 / 100\n",
            "    Training Loss: 0.681\tValidation Loss: 0.681\tTime: 9.356984615325928\n",
            "  Epoch 29 / 100\n",
            "    Training Loss: 0.681\tValidation Loss: 0.681\tTime: 6.6158599853515625\n",
            "  Epoch 30 / 100\n",
            "    Training Loss: 0.682\tValidation Loss: 0.681\tTime: 9.030766487121582\n",
            "  Epoch 31 / 100\n",
            "    Training Loss: 0.680\tValidation Loss: 0.680\tTime: 9.335150003433228\n",
            "  Epoch 32 / 100\n",
            "    Training Loss: 0.680\tValidation Loss: 0.680\tTime: 9.310374975204468\n",
            "  Epoch 33 / 100\n",
            "    Training Loss: 0.686\tValidation Loss: 0.680\tTime: 8.87420654296875\n",
            "  Epoch 34 / 100\n",
            "    Training Loss: 0.681\tValidation Loss: 0.680\tTime: 9.046900033950806\n",
            "  Epoch 35 / 100\n",
            "    Training Loss: 0.682\tValidation Loss: 0.680\tTime: 9.064120531082153\n",
            "  Epoch 36 / 100\n",
            "    Training Loss: 0.681\tValidation Loss: 0.680\tTime: 8.850321531295776\n",
            "  Epoch 37 / 100\n",
            "    Training Loss: 0.682\tValidation Loss: 0.679\tTime: 9.44884729385376\n",
            "  Epoch 38 / 100\n",
            "    Training Loss: 0.682\tValidation Loss: 0.679\tTime: 8.72276759147644\n",
            "  Epoch 39 / 100\n",
            "    Training Loss: 0.681\tValidation Loss: 0.679\tTime: 6.208918809890747\n",
            "  Epoch 40 / 100\n",
            "    Training Loss: 0.680\tValidation Loss: 0.679\tTime: 8.733266353607178\n",
            "  Epoch 41 / 100\n",
            "    Training Loss: 0.681\tValidation Loss: 0.678\tTime: 9.003239870071411\n",
            "  Epoch 42 / 100\n",
            "    Training Loss: 0.679\tValidation Loss: 0.678\tTime: 8.903502225875854\n",
            "  Epoch 43 / 100\n",
            "    Training Loss: 0.679\tValidation Loss: 0.678\tTime: 8.648315191268921\n",
            "  Epoch 44 / 100\n",
            "    Training Loss: 0.680\tValidation Loss: 0.678\tTime: 6.033738374710083\n",
            "  Epoch 45 / 100\n",
            "    Training Loss: 0.680\tValidation Loss: 0.678\tTime: 8.551563024520874\n",
            "  Epoch 46 / 100\n",
            "    Training Loss: 0.680\tValidation Loss: 0.677\tTime: 8.467814445495605\n",
            "  Epoch 47 / 100\n",
            "    Training Loss: 0.681\tValidation Loss: 0.677\tTime: 9.086772441864014\n",
            "  Epoch 48 / 100\n",
            "    Training Loss: 0.679\tValidation Loss: 0.677\tTime: 6.629284381866455\n",
            "  Epoch 49 / 100\n",
            "    Training Loss: 0.681\tValidation Loss: 0.677\tTime: 9.142412900924683\n",
            "  Epoch 50 / 100\n",
            "    Training Loss: 0.680\tValidation Loss: 0.677\tTime: 9.212750673294067\n",
            "  Epoch 51 / 100\n",
            "    Training Loss: 0.677\tValidation Loss: 0.677\tTime: 8.973377466201782\n",
            "  Epoch 52 / 100\n",
            "    Training Loss: 0.678\tValidation Loss: 0.676\tTime: 9.156232118606567\n",
            "  Epoch 53 / 100\n",
            "    Training Loss: 0.681\tValidation Loss: 0.676\tTime: 9.041837215423584\n",
            "  Epoch 54 / 100\n",
            "    Training Loss: 0.679\tValidation Loss: 0.676\tTime: 6.692920684814453\n",
            "  Epoch 55 / 100\n",
            "    Training Loss: 0.676\tValidation Loss: 0.676\tTime: 9.036117553710938\n",
            "  Epoch 56 / 100\n",
            "    Training Loss: 0.679\tValidation Loss: 0.676\tTime: 9.294848442077637\n",
            "  Epoch 57 / 100\n",
            "    Training Loss: 0.678\tValidation Loss: 0.675\tTime: 9.100698709487915\n",
            "  Epoch 58 / 100\n",
            "    Training Loss: 0.678\tValidation Loss: 0.675\tTime: 6.467378377914429\n",
            "  Epoch 59 / 100\n",
            "    Training Loss: 0.673\tValidation Loss: 0.675\tTime: 9.130150556564331\n",
            "  Epoch 60 / 100\n",
            "    Training Loss: 0.676\tValidation Loss: 0.675\tTime: 9.185555934906006\n",
            "  Epoch 61 / 100\n",
            "    Training Loss: 0.676\tValidation Loss: 0.675\tTime: 6.204711437225342\n",
            "  Epoch 62 / 100\n",
            "    Training Loss: 0.678\tValidation Loss: 0.675\tTime: 8.68807053565979\n",
            "  Epoch 63 / 100\n",
            "    Training Loss: 0.675\tValidation Loss: 0.674\tTime: 8.891147136688232\n",
            "  Epoch 64 / 100\n",
            "    Training Loss: 0.677\tValidation Loss: 0.674\tTime: 8.63074517250061\n",
            "  Epoch 65 / 100\n",
            "    Training Loss: 0.673\tValidation Loss: 0.674\tTime: 8.818267345428467\n",
            "  Epoch 66 / 100\n",
            "    Training Loss: 0.673\tValidation Loss: 0.674\tTime: 7.135075807571411\n",
            "  Epoch 67 / 100\n",
            "    Training Loss: 0.678\tValidation Loss: 0.674\tTime: 5.212660074234009\n",
            "  Epoch 68 / 100\n",
            "    Training Loss: 0.675\tValidation Loss: 0.674\tTime: 6.6762824058532715\n",
            "  Epoch 69 / 100\n",
            "    Training Loss: 0.673\tValidation Loss: 0.673\tTime: 6.6757073402404785\n",
            "  Epoch 70 / 100\n",
            "    Training Loss: 0.675\tValidation Loss: 0.673\tTime: 6.630185127258301\n",
            "  Epoch 71 / 100\n",
            "    Training Loss: 0.674\tValidation Loss: 0.673\tTime: 6.610887289047241\n",
            "  Epoch 72 / 100\n",
            "    Training Loss: 0.675\tValidation Loss: 0.672\tTime: 6.609065294265747\n",
            "  Epoch 73 / 100\n",
            "    Training Loss: 0.676\tValidation Loss: 0.672\tTime: 6.587815761566162\n",
            "  Epoch 74 / 100\n",
            "    Training Loss: 0.673\tValidation Loss: 0.672\tTime: 6.557203531265259\n",
            "  Epoch 75 / 100\n",
            "    Training Loss: 0.675\tValidation Loss: 0.671\tTime: 6.561004638671875\n",
            "  Epoch 76 / 100\n",
            "    Training Loss: 0.672\tValidation Loss: 0.671\tTime: 6.588406085968018\n",
            "  Epoch 77 / 100\n",
            "    Training Loss: 0.675\tValidation Loss: 0.671\tTime: 6.582104921340942\n",
            "  Epoch 78 / 100\n",
            "    Training Loss: 0.671\tValidation Loss: 0.671\tTime: 6.625381231307983\n",
            "  Epoch 79 / 100\n",
            "    Training Loss: 0.672\tValidation Loss: 0.671\tTime: 6.662950754165649\n",
            "  Epoch 80 / 100\n",
            "    Training Loss: 0.673\tValidation Loss: 0.671\tTime: 6.610727071762085\n",
            "  Epoch 81 / 100\n",
            "    Training Loss: 0.673\tValidation Loss: 0.671\tTime: 5.167248010635376\n",
            "  Epoch 82 / 100\n",
            "    Training Loss: 0.670\tValidation Loss: 0.671\tTime: 6.598578929901123\n",
            "  Epoch 83 / 100\n",
            "    Training Loss: 0.670\tValidation Loss: 0.670\tTime: 6.601581811904907\n",
            "  Epoch 84 / 100\n",
            "    Training Loss: 0.671\tValidation Loss: 0.670\tTime: 6.559481143951416\n",
            "  Epoch 85 / 100\n",
            "    Training Loss: 0.671\tValidation Loss: 0.670\tTime: 6.590568780899048\n",
            "  Epoch 86 / 100\n",
            "    Training Loss: 0.673\tValidation Loss: 0.670\tTime: 6.593366622924805\n",
            "  Epoch 87 / 100\n",
            "    Training Loss: 0.670\tValidation Loss: 0.670\tTime: 6.603871822357178\n",
            "  Epoch 88 / 100\n",
            "    Training Loss: 0.666\tValidation Loss: 0.669\tTime: 6.576958894729614\n",
            "  Epoch 89 / 100\n",
            "    Training Loss: 0.670\tValidation Loss: 0.669\tTime: 6.561051607131958\n",
            "  Epoch 90 / 100\n",
            "    Training Loss: 0.671\tValidation Loss: 0.669\tTime: 6.607418775558472\n",
            "  Epoch 91 / 100\n",
            "    Training Loss: 0.671\tValidation Loss: 0.668\tTime: 6.617898464202881\n",
            "  Epoch 92 / 100\n",
            "    Training Loss: 0.670\tValidation Loss: 0.668\tTime: 6.575069904327393\n",
            "  Epoch 93 / 100\n",
            "    Training Loss: 0.667\tValidation Loss: 0.668\tTime: 6.585519790649414\n",
            "  Epoch 94 / 100\n",
            "    Training Loss: 0.669\tValidation Loss: 0.668\tTime: 5.162122964859009\n",
            "  Epoch 95 / 100\n",
            "    Training Loss: 0.670\tValidation Loss: 0.668\tTime: 6.4982311725616455\n",
            "  Epoch 96 / 100\n",
            "    Training Loss: 0.667\tValidation Loss: 0.667\tTime: 6.596953392028809\n",
            "  Epoch 97 / 100\n",
            "    Training Loss: 0.668\tValidation Loss: 0.667\tTime: 6.655760288238525\n",
            "  Epoch 98 / 100\n",
            "    Training Loss: 0.670\tValidation Loss: 0.667\tTime: 5.180054426193237\n",
            "  Epoch 99 / 100\n",
            "    Training Loss: 0.669\tValidation Loss: 0.667\tTime: 6.594603538513184\n",
            "  Epoch 100 / 100\n",
            "    Training Loss: 0.667\tValidation Loss: 0.667\tTime: 6.635660171508789\n",
            "Saving model 20\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 21 / 25\n",
            "  Epoch 1 / 100\n",
            "    Training Loss: 0.692\tValidation Loss: 0.692\tTime: 15.771573543548584\n",
            "  Epoch 2 / 100\n",
            "    Training Loss: 0.692\tValidation Loss: 0.690\tTime: 15.98863697052002\n",
            "  Epoch 3 / 100\n",
            "    Training Loss: 0.691\tValidation Loss: 0.688\tTime: 16.240784406661987\n",
            "  Epoch 4 / 100\n",
            "    Training Loss: 0.690\tValidation Loss: 0.688\tTime: 13.555881261825562\n",
            "  Epoch 5 / 100\n",
            "    Training Loss: 0.689\tValidation Loss: 0.687\tTime: 15.481370449066162\n",
            "  Epoch 6 / 100\n",
            "    Training Loss: 0.688\tValidation Loss: 0.686\tTime: 15.991136074066162\n",
            "  Epoch 7 / 100\n",
            "    Training Loss: 0.688\tValidation Loss: 0.685\tTime: 16.108009815216064\n",
            "  Epoch 8 / 100\n",
            "    Training Loss: 0.687\tValidation Loss: 0.685\tTime: 15.96115756034851\n",
            "  Epoch 9 / 100\n",
            "    Training Loss: 0.687\tValidation Loss: 0.685\tTime: 15.860723733901978\n",
            "  Epoch 10 / 100\n",
            "    Training Loss: 0.687\tValidation Loss: 0.684\tTime: 16.068825006484985\n",
            "  Epoch 11 / 100\n",
            "    Training Loss: 0.687\tValidation Loss: 0.684\tTime: 15.93558382987976\n",
            "  Epoch 12 / 100\n",
            "    Training Loss: 0.684\tValidation Loss: 0.683\tTime: 15.969429016113281\n",
            "  Epoch 13 / 100\n",
            "    Training Loss: 0.686\tValidation Loss: 0.683\tTime: 15.878568172454834\n",
            "  Epoch 14 / 100\n",
            "    Training Loss: 0.686\tValidation Loss: 0.683\tTime: 15.940249681472778\n",
            "  Epoch 15 / 100\n",
            "    Training Loss: 0.687\tValidation Loss: 0.683\tTime: 15.924391984939575\n",
            "  Epoch 16 / 100\n",
            "    Training Loss: 0.684\tValidation Loss: 0.682\tTime: 15.913229942321777\n",
            "  Epoch 17 / 100\n",
            "    Training Loss: 0.686\tValidation Loss: 0.682\tTime: 15.972788572311401\n",
            "  Epoch 18 / 100\n",
            "    Training Loss: 0.686\tValidation Loss: 0.682\tTime: 13.409673929214478\n",
            "  Epoch 19 / 100\n",
            "    Training Loss: 0.683\tValidation Loss: 0.682\tTime: 12.816719770431519\n",
            "  Epoch 20 / 100\n",
            "    Training Loss: 0.686\tValidation Loss: 0.682\tTime: 15.42402172088623\n",
            "  Epoch 21 / 100\n",
            "    Training Loss: 0.683\tValidation Loss: 0.682\tTime: 15.955906629562378\n",
            "  Epoch 22 / 100\n",
            "    Training Loss: 0.683\tValidation Loss: 0.681\tTime: 16.053510665893555\n",
            "  Epoch 23 / 100\n",
            "    Training Loss: 0.682\tValidation Loss: 0.681\tTime: 15.978140354156494\n",
            "  Epoch 24 / 100\n",
            "    Training Loss: 0.683\tValidation Loss: 0.680\tTime: 15.94450068473816\n",
            "  Epoch 25 / 100\n",
            "    Training Loss: 0.683\tValidation Loss: 0.679\tTime: 15.978925943374634\n",
            "  Epoch 26 / 100\n",
            "    Training Loss: 0.680\tValidation Loss: 0.679\tTime: 15.71644115447998\n",
            "  Epoch 27 / 100\n",
            "    Training Loss: 0.682\tValidation Loss: 0.679\tTime: 13.292761325836182\n",
            "  Epoch 28 / 100\n",
            "    Training Loss: 0.683\tValidation Loss: 0.679\tTime: 15.301803827285767\n",
            "  Epoch 29 / 100\n",
            "    Training Loss: 0.682\tValidation Loss: 0.679\tTime: 13.30620288848877\n",
            "  Epoch 30 / 100\n",
            "    Training Loss: 0.682\tValidation Loss: 0.679\tTime: 12.796874284744263\n",
            "  Epoch 31 / 100\n",
            "    Training Loss: 0.681\tValidation Loss: 0.679\tTime: 15.51505970954895\n",
            "  Epoch 32 / 100\n",
            "    Training Loss: 0.681\tValidation Loss: 0.679\tTime: 16.124786615371704\n",
            "  Epoch 33 / 100\n",
            "    Training Loss: 0.680\tValidation Loss: 0.678\tTime: 15.984208583831787\n",
            "  Epoch 34 / 100\n",
            "    Training Loss: 0.681\tValidation Loss: 0.678\tTime: 13.364130020141602\n",
            "  Epoch 35 / 100\n",
            "    Training Loss: 0.682\tValidation Loss: 0.678\tTime: 15.442294359207153\n",
            "  Epoch 36 / 100\n",
            "    Training Loss: 0.680\tValidation Loss: 0.677\tTime: 15.651776313781738\n",
            "  Epoch 37 / 100\n",
            "    Training Loss: 0.682\tValidation Loss: 0.678\tTime: 13.345650911331177\n",
            "  Epoch 38 / 100\n",
            "    Training Loss: 0.682\tValidation Loss: 0.677\tTime: 12.575550317764282\n",
            "  Epoch 39 / 100\n",
            "    Training Loss: 0.680\tValidation Loss: 0.676\tTime: 15.558349132537842\n",
            "  Epoch 40 / 100\n",
            "    Training Loss: 0.679\tValidation Loss: 0.676\tTime: 16.068292140960693\n",
            "  Epoch 41 / 100\n",
            "    Training Loss: 0.677\tValidation Loss: 0.676\tTime: 16.042871952056885\n",
            "  Epoch 42 / 100\n",
            "    Training Loss: 0.676\tValidation Loss: 0.675\tTime: 16.100876569747925\n",
            "  Epoch 43 / 100\n",
            "    Training Loss: 0.679\tValidation Loss: 0.675\tTime: 16.011874675750732\n",
            "  Epoch 44 / 100\n",
            "    Training Loss: 0.680\tValidation Loss: 0.675\tTime: 15.963669300079346\n",
            "  Epoch 45 / 100\n",
            "    Training Loss: 0.679\tValidation Loss: 0.675\tTime: 13.233490943908691\n",
            "  Epoch 46 / 100\n",
            "    Training Loss: 0.678\tValidation Loss: 0.675\tTime: 15.439146995544434\n",
            "  Epoch 47 / 100\n",
            "    Training Loss: 0.678\tValidation Loss: 0.674\tTime: 16.136099576950073\n",
            "  Epoch 48 / 100\n",
            "    Training Loss: 0.677\tValidation Loss: 0.674\tTime: 16.028138875961304\n",
            "  Epoch 49 / 100\n",
            "    Training Loss: 0.678\tValidation Loss: 0.674\tTime: 15.939798593521118\n",
            "  Epoch 50 / 100\n",
            "    Training Loss: 0.676\tValidation Loss: 0.673\tTime: 16.09514594078064\n",
            "  Epoch 51 / 100\n",
            "    Training Loss: 0.677\tValidation Loss: 0.673\tTime: 13.549229383468628\n",
            "  Epoch 52 / 100\n",
            "    Training Loss: 0.676\tValidation Loss: 0.673\tTime: 15.293178796768188\n",
            "  Epoch 53 / 100\n",
            "    Training Loss: 0.677\tValidation Loss: 0.673\tTime: 13.444258451461792\n",
            "  Epoch 54 / 100\n",
            "    Training Loss: 0.673\tValidation Loss: 0.672\tTime: 15.522768020629883\n",
            "  Epoch 55 / 100\n",
            "    Training Loss: 0.674\tValidation Loss: 0.671\tTime: 16.132280588150024\n",
            "  Epoch 56 / 100\n",
            "    Training Loss: 0.674\tValidation Loss: 0.672\tTime: 13.301403999328613\n",
            "  Epoch 57 / 100\n",
            "    Training Loss: 0.674\tValidation Loss: 0.671\tTime: 15.554369688034058\n",
            "  Epoch 58 / 100\n",
            "    Training Loss: 0.671\tValidation Loss: 0.671\tTime: 16.012981176376343\n",
            "  Epoch 59 / 100\n",
            "    Training Loss: 0.674\tValidation Loss: 0.671\tTime: 13.400647163391113\n",
            "  Epoch 60 / 100\n",
            "    Training Loss: 0.674\tValidation Loss: 0.670\tTime: 15.313328742980957\n",
            "  Epoch 61 / 100\n",
            "    Training Loss: 0.674\tValidation Loss: 0.671\tTime: 13.494641780853271\n",
            "  Epoch 62 / 100\n",
            "    Training Loss: 0.674\tValidation Loss: 0.670\tTime: 12.737314939498901\n",
            "  Epoch 63 / 100\n",
            "    Training Loss: 0.671\tValidation Loss: 0.669\tTime: 15.517717123031616\n",
            "  Epoch 64 / 100\n",
            "    Training Loss: 0.673\tValidation Loss: 0.669\tTime: 16.027207612991333\n",
            "  Epoch 65 / 100\n",
            "    Training Loss: 0.674\tValidation Loss: 0.669\tTime: 13.492412805557251\n",
            "  Epoch 66 / 100\n",
            "    Training Loss: 0.671\tValidation Loss: 0.670\tTime: 12.916621208190918\n",
            "  Epoch 67 / 100\n",
            "    Training Loss: 0.670\tValidation Loss: 0.669\tTime: 12.96238088607788\n",
            "  Epoch 68 / 100\n",
            "    Training Loss: 0.671\tValidation Loss: 0.668\tTime: 15.486494541168213\n",
            "  Epoch 69 / 100\n",
            "    Training Loss: 0.670\tValidation Loss: 0.668\tTime: 15.933360815048218\n",
            "  Epoch 70 / 100\n",
            "    Training Loss: 0.669\tValidation Loss: 0.668\tTime: 13.566519021987915\n",
            "  Epoch 71 / 100\n",
            "    Training Loss: 0.670\tValidation Loss: 0.668\tTime: 13.039881944656372\n",
            "  Epoch 72 / 100\n",
            "    Training Loss: 0.672\tValidation Loss: 0.668\tTime: 15.48931622505188\n",
            "  Epoch 73 / 100\n",
            "    Training Loss: 0.673\tValidation Loss: 0.667\tTime: 15.953519105911255\n",
            "  Epoch 74 / 100\n",
            "    Training Loss: 0.671\tValidation Loss: 0.667\tTime: 15.9462890625\n",
            "  Epoch 75 / 100\n",
            "    Training Loss: 0.670\tValidation Loss: 0.667\tTime: 13.4239981174469\n",
            "  Epoch 76 / 100\n",
            "    Training Loss: 0.668\tValidation Loss: 0.666\tTime: 15.155895709991455\n",
            "  Epoch 77 / 100\n",
            "    Training Loss: 0.669\tValidation Loss: 0.666\tTime: 16.0670485496521\n",
            "  Epoch 78 / 100\n",
            "    Training Loss: 0.669\tValidation Loss: 0.666\tTime: 16.020594835281372\n",
            "  Epoch 79 / 100\n",
            "    Training Loss: 0.668\tValidation Loss: 0.666\tTime: 16.131053924560547\n",
            "  Epoch 80 / 100\n",
            "    Training Loss: 0.668\tValidation Loss: 0.665\tTime: 16.20694923400879\n",
            "  Epoch 81 / 100\n",
            "    Training Loss: 0.666\tValidation Loss: 0.665\tTime: 16.072532892227173\n",
            "  Epoch 82 / 100\n",
            "    Training Loss: 0.667\tValidation Loss: 0.665\tTime: 16.029479503631592\n",
            "  Epoch 83 / 100\n",
            "    Training Loss: 0.666\tValidation Loss: 0.664\tTime: 15.913071870803833\n",
            "  Epoch 84 / 100\n",
            "    Training Loss: 0.663\tValidation Loss: 0.664\tTime: 15.960373640060425\n",
            "  Epoch 85 / 100\n",
            "    Training Loss: 0.666\tValidation Loss: 0.664\tTime: 13.377439260482788\n",
            "  Epoch 86 / 100\n",
            "    Training Loss: 0.661\tValidation Loss: 0.664\tTime: 12.71810531616211\n",
            "  Epoch 87 / 100\n",
            "    Training Loss: 0.670\tValidation Loss: 0.664\tTime: 12.892661809921265\n",
            "  Epoch 88 / 100\n",
            "    Training Loss: 0.667\tValidation Loss: 0.664\tTime: 12.92768383026123\n",
            "  Epoch 89 / 100\n",
            "    Training Loss: 0.668\tValidation Loss: 0.664\tTime: 12.887292623519897\n",
            "  Epoch 90 / 100\n",
            "    Training Loss: 0.666\tValidation Loss: 0.664\tTime: 12.872556209564209\n",
            "  Epoch 91 / 100\n",
            "    Training Loss: 0.665\tValidation Loss: 0.663\tTime: 15.499318361282349\n",
            "  Epoch 92 / 100\n",
            "    Training Loss: 0.663\tValidation Loss: 0.662\tTime: 16.0566565990448\n",
            "  Epoch 93 / 100\n",
            "    Training Loss: 0.663\tValidation Loss: 0.662\tTime: 15.953917503356934\n",
            "  Epoch 94 / 100\n",
            "    Training Loss: 0.661\tValidation Loss: 0.662\tTime: 13.3661470413208\n",
            "  Epoch 95 / 100\n",
            "    Training Loss: 0.663\tValidation Loss: 0.662\tTime: 15.231781959533691\n",
            "  Epoch 96 / 100\n",
            "    Training Loss: 0.665\tValidation Loss: 0.661\tTime: 16.04798650741577\n",
            "  Epoch 97 / 100\n",
            "    Training Loss: 0.663\tValidation Loss: 0.661\tTime: 13.5226411819458\n",
            "  Epoch 98 / 100\n",
            "    Training Loss: 0.662\tValidation Loss: 0.662\tTime: 12.79630970954895\n",
            "  Epoch 99 / 100\n",
            "    Training Loss: 0.664\tValidation Loss: 0.662\tTime: 12.871712684631348\n",
            "  Epoch 100 / 100\n",
            "    Training Loss: 0.663\tValidation Loss: 0.660\tTime: 15.353905200958252\n",
            "Saving model 21\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n",
            "Experiment: 22 / 25\n",
            "  Epoch 1 / 100\n",
            "    Training Loss: 0.689\tValidation Loss: 0.687\tTime: 5.857926368713379\n",
            "  Epoch 2 / 100\n",
            "    Training Loss: 0.684\tValidation Loss: 0.681\tTime: 5.52641224861145\n",
            "  Epoch 3 / 100\n",
            "    Training Loss: 0.679\tValidation Loss: 0.676\tTime: 5.521138906478882\n",
            "  Epoch 4 / 100\n",
            "    Training Loss: 0.675\tValidation Loss: 0.671\tTime: 5.535553693771362\n",
            "  Epoch 5 / 100\n",
            "    Training Loss: 0.671\tValidation Loss: 0.666\tTime: 5.54919695854187\n",
            "  Epoch 6 / 100\n",
            "    Training Loss: 0.665\tValidation Loss: 0.662\tTime: 5.540514945983887\n",
            "  Epoch 7 / 100\n",
            "    Training Loss: 0.663\tValidation Loss: 0.658\tTime: 5.569437026977539\n",
            "  Epoch 8 / 100\n",
            "    Training Loss: 0.656\tValidation Loss: 0.654\tTime: 5.54245662689209\n",
            "  Epoch 9 / 100\n",
            "    Training Loss: 0.654\tValidation Loss: 0.651\tTime: 5.543866872787476\n",
            "  Epoch 10 / 100\n",
            "    Training Loss: 0.652\tValidation Loss: 0.648\tTime: 5.519603252410889\n",
            "  Epoch 11 / 100\n",
            "    Training Loss: 0.646\tValidation Loss: 0.645\tTime: 5.529469013214111\n",
            "  Epoch 12 / 100\n",
            "    Training Loss: 0.646\tValidation Loss: 0.641\tTime: 5.515413761138916\n",
            "  Epoch 13 / 100\n",
            "    Training Loss: 0.641\tValidation Loss: 0.638\tTime: 5.504493713378906\n",
            "  Epoch 14 / 100\n",
            "    Training Loss: 0.638\tValidation Loss: 0.635\tTime: 5.507071495056152\n",
            "  Epoch 15 / 100\n",
            "    Training Loss: 0.633\tValidation Loss: 0.632\tTime: 5.5278003215789795\n",
            "  Epoch 16 / 100\n",
            "    Training Loss: 0.632\tValidation Loss: 0.630\tTime: 5.524336814880371\n",
            "  Epoch 17 / 100\n",
            "    Training Loss: 0.630\tValidation Loss: 0.626\tTime: 5.541619777679443\n",
            "  Epoch 18 / 100\n",
            "    Training Loss: 0.627\tValidation Loss: 0.623\tTime: 5.615399599075317\n",
            "  Epoch 19 / 100\n",
            "    Training Loss: 0.626\tValidation Loss: 0.622\tTime: 5.562411308288574\n",
            "  Epoch 20 / 100\n",
            "    Training Loss: 0.621\tValidation Loss: 0.619\tTime: 5.6301186084747314\n",
            "  Epoch 21 / 100\n",
            "    Training Loss: 0.623\tValidation Loss: 0.616\tTime: 5.792757034301758\n",
            "  Epoch 22 / 100\n",
            "    Training Loss: 0.617\tValidation Loss: 0.614\tTime: 6.077605485916138\n",
            "  Epoch 23 / 100\n",
            "    Training Loss: 0.616\tValidation Loss: 0.612\tTime: 6.627511739730835\n",
            "  Epoch 24 / 100\n",
            "    Training Loss: 0.614\tValidation Loss: 0.610\tTime: 6.559724569320679\n",
            "  Epoch 25 / 100\n",
            "    Training Loss: 0.610\tValidation Loss: 0.607\tTime: 6.44577169418335\n",
            "  Epoch 26 / 100\n",
            "    Training Loss: 0.608\tValidation Loss: 0.605\tTime: 6.326042175292969\n",
            "  Epoch 27 / 100\n",
            "    Training Loss: 0.603\tValidation Loss: 0.601\tTime: 6.241311311721802\n",
            "  Epoch 28 / 100\n",
            "    Training Loss: 0.603\tValidation Loss: 0.599\tTime: 6.224688291549683\n",
            "  Epoch 29 / 100\n",
            "    Training Loss: 0.604\tValidation Loss: 0.597\tTime: 6.349010944366455\n",
            "  Epoch 30 / 100\n",
            "    Training Loss: 0.601\tValidation Loss: 0.595\tTime: 6.276390075683594\n",
            "  Epoch 31 / 100\n",
            "    Training Loss: 0.598\tValidation Loss: 0.592\tTime: 6.382017374038696\n",
            "  Epoch 32 / 100\n",
            "    Training Loss: 0.592\tValidation Loss: 0.590\tTime: 6.316497564315796\n",
            "  Epoch 33 / 100\n",
            "    Training Loss: 0.591\tValidation Loss: 0.587\tTime: 6.3119800090789795\n",
            "  Epoch 34 / 100\n",
            "    Training Loss: 0.593\tValidation Loss: 0.587\tTime: 6.252525568008423\n",
            "  Epoch 35 / 100\n",
            "    Training Loss: 0.588\tValidation Loss: 0.584\tTime: 6.222063779830933\n",
            "  Epoch 36 / 100\n",
            "    Training Loss: 0.586\tValidation Loss: 0.581\tTime: 6.322280168533325\n",
            "  Epoch 37 / 100\n",
            "    Training Loss: 0.584\tValidation Loss: 0.579\tTime: 6.310811996459961\n",
            "  Epoch 38 / 100\n",
            "    Training Loss: 0.580\tValidation Loss: 0.578\tTime: 6.253089189529419\n",
            "  Epoch 39 / 100\n",
            "    Training Loss: 0.580\tValidation Loss: 0.575\tTime: 6.2852623462677\n",
            "  Epoch 40 / 100\n",
            "    Training Loss: 0.575\tValidation Loss: 0.572\tTime: 6.257035255432129\n",
            "  Epoch 41 / 100\n",
            "    Training Loss: 0.573\tValidation Loss: 0.570\tTime: 6.340655088424683\n",
            "  Epoch 42 / 100\n",
            "    Training Loss: 0.570\tValidation Loss: 0.567\tTime: 6.316917896270752\n",
            "  Epoch 43 / 100\n",
            "    Training Loss: 0.572\tValidation Loss: 0.566\tTime: 6.2626800537109375\n",
            "  Epoch 44 / 100\n",
            "    Training Loss: 0.568\tValidation Loss: 0.564\tTime: 6.2780139446258545\n",
            "  Epoch 45 / 100\n",
            "    Training Loss: 0.562\tValidation Loss: 0.561\tTime: 6.314384937286377\n",
            "  Epoch 46 / 100\n",
            "    Training Loss: 0.565\tValidation Loss: 0.559\tTime: 6.408729314804077\n",
            "  Epoch 47 / 100\n",
            "    Training Loss: 0.565\tValidation Loss: 0.558\tTime: 6.297706604003906\n",
            "  Epoch 48 / 100\n",
            "    Training Loss: 0.565\tValidation Loss: 0.556\tTime: 6.409246206283569\n",
            "  Epoch 49 / 100\n",
            "    Training Loss: 0.560\tValidation Loss: 0.555\tTime: 6.295445203781128\n",
            "  Epoch 50 / 100\n",
            "    Training Loss: 0.559\tValidation Loss: 0.552\tTime: 6.293229103088379\n",
            "  Epoch 51 / 100\n",
            "    Training Loss: 0.556\tValidation Loss: 0.550\tTime: 6.304695129394531\n",
            "  Epoch 52 / 100\n",
            "    Training Loss: 0.552\tValidation Loss: 0.548\tTime: 6.407331466674805\n",
            "  Epoch 53 / 100\n",
            "    Training Loss: 0.555\tValidation Loss: 0.547\tTime: 6.438690662384033\n",
            "  Epoch 54 / 100\n",
            "    Training Loss: 0.550\tValidation Loss: 0.544\tTime: 6.322036027908325\n",
            "  Epoch 55 / 100\n",
            "    Training Loss: 0.548\tValidation Loss: 0.542\tTime: 6.4666478633880615\n",
            "  Epoch 56 / 100\n",
            "    Training Loss: 0.546\tValidation Loss: 0.540\tTime: 6.55404257774353\n",
            "  Epoch 57 / 100\n",
            "    Training Loss: 0.546\tValidation Loss: 0.538\tTime: 6.419851064682007\n",
            "  Epoch 58 / 100\n",
            "    Training Loss: 0.544\tValidation Loss: 0.536\tTime: 6.569666385650635\n",
            "  Epoch 59 / 100\n",
            "    Training Loss: 0.537\tValidation Loss: 0.534\tTime: 6.5445942878723145\n",
            "  Epoch 60 / 100\n",
            "    Training Loss: 0.535\tValidation Loss: 0.532\tTime: 6.418587923049927\n",
            "  Epoch 61 / 100\n",
            "    Training Loss: 0.534\tValidation Loss: 0.530\tTime: 6.427858591079712\n",
            "  Epoch 62 / 100\n",
            "    Training Loss: 0.536\tValidation Loss: 0.528\tTime: 6.534034729003906\n",
            "  Epoch 63 / 100\n",
            "    Training Loss: 0.532\tValidation Loss: 0.526\tTime: 6.368459939956665\n",
            "  Epoch 64 / 100\n",
            "    Training Loss: 0.531\tValidation Loss: 0.523\tTime: 6.213586330413818\n",
            "  Epoch 65 / 100\n",
            "    Training Loss: 0.525\tValidation Loss: 0.522\tTime: 6.213827133178711\n",
            "  Epoch 66 / 100\n",
            "    Training Loss: 0.527\tValidation Loss: 0.520\tTime: 6.14577841758728\n",
            "  Epoch 67 / 100\n",
            "    Training Loss: 0.522\tValidation Loss: 0.518\tTime: 6.224785566329956\n",
            "  Epoch 68 / 100\n",
            "    Training Loss: 0.523\tValidation Loss: 0.517\tTime: 6.421487808227539\n",
            "  Epoch 69 / 100\n",
            "    Training Loss: 0.523\tValidation Loss: 0.516\tTime: 6.31848669052124\n",
            "  Epoch 70 / 100\n",
            "    Training Loss: 0.522\tValidation Loss: 0.513\tTime: 6.357820272445679\n",
            "  Epoch 71 / 100\n",
            "    Training Loss: 0.515\tValidation Loss: 0.511\tTime: 6.33715295791626\n",
            "  Epoch 72 / 100\n",
            "    Training Loss: 0.515\tValidation Loss: 0.509\tTime: 6.235371828079224\n",
            "  Epoch 73 / 100\n",
            "    Training Loss: 0.514\tValidation Loss: 0.508\tTime: 6.296857118606567\n",
            "  Epoch 74 / 100\n",
            "    Training Loss: 0.516\tValidation Loss: 0.506\tTime: 6.332987546920776\n",
            "  Epoch 75 / 100\n",
            "    Training Loss: 0.507\tValidation Loss: 0.505\tTime: 6.2255518436431885\n",
            "  Epoch 76 / 100\n",
            "    Training Loss: 0.513\tValidation Loss: 0.503\tTime: 6.337878704071045\n",
            "  Epoch 77 / 100\n",
            "    Training Loss: 0.507\tValidation Loss: 0.501\tTime: 6.227294206619263\n",
            "  Epoch 78 / 100\n",
            "    Training Loss: 0.510\tValidation Loss: 0.499\tTime: 6.2383904457092285\n",
            "  Epoch 79 / 100\n",
            "    Training Loss: 0.502\tValidation Loss: 0.498\tTime: 6.195058584213257\n",
            "  Epoch 80 / 100\n",
            "    Training Loss: 0.503\tValidation Loss: 0.496\tTime: 6.132496118545532\n",
            "  Epoch 81 / 100\n",
            "    Training Loss: 0.504\tValidation Loss: 0.494\tTime: 6.278306484222412\n",
            "  Epoch 82 / 100\n",
            "    Training Loss: 0.498\tValidation Loss: 0.492\tTime: 6.239516019821167\n",
            "  Epoch 83 / 100\n",
            "    Training Loss: 0.499\tValidation Loss: 0.491\tTime: 6.329372406005859\n",
            "  Epoch 84 / 100\n",
            "    Training Loss: 0.496\tValidation Loss: 0.489\tTime: 6.424991846084595\n",
            "  Epoch 85 / 100\n",
            "    Training Loss: 0.498\tValidation Loss: 0.488\tTime: 6.3787360191345215\n",
            "  Epoch 86 / 100\n",
            "    Training Loss: 0.487\tValidation Loss: 0.485\tTime: 6.35792875289917\n",
            "  Epoch 87 / 100\n",
            "    Training Loss: 0.489\tValidation Loss: 0.484\tTime: 6.292937755584717\n",
            "  Epoch 88 / 100\n",
            "    Training Loss: 0.489\tValidation Loss: 0.483\tTime: 6.276365041732788\n",
            "  Epoch 89 / 100\n",
            "    Training Loss: 0.485\tValidation Loss: 0.481\tTime: 6.311898469924927\n",
            "  Epoch 90 / 100\n",
            "    Training Loss: 0.495\tValidation Loss: 0.480\tTime: 6.364417791366577\n",
            "  Epoch 91 / 100\n",
            "    Training Loss: 0.490\tValidation Loss: 0.479\tTime: 6.338398456573486\n",
            "  Epoch 92 / 100\n",
            "    Training Loss: 0.489\tValidation Loss: 0.477\tTime: 6.495260953903198\n",
            "  Epoch 93 / 100\n",
            "    Training Loss: 0.482\tValidation Loss: 0.474\tTime: 6.521710395812988\n",
            "  Epoch 94 / 100\n",
            "    Training Loss: 0.481\tValidation Loss: 0.473\tTime: 6.417116165161133\n",
            "  Epoch 95 / 100\n",
            "    Training Loss: 0.484\tValidation Loss: 0.473\tTime: 6.488004207611084\n",
            "  Epoch 96 / 100\n",
            "    Training Loss: 0.481\tValidation Loss: 0.471\tTime: 6.4312214851379395\n",
            "  Epoch 97 / 100\n",
            "    Training Loss: 0.476\tValidation Loss: 0.470\tTime: 6.3167853355407715\n",
            "  Epoch 98 / 100\n",
            "    Training Loss: 0.474\tValidation Loss: 0.468\tTime: 6.580651760101318\n",
            "  Epoch 99 / 100\n",
            "    Training Loss: 0.482\tValidation Loss: 0.467\tTime: 6.445701837539673\n",
            "  Epoch 100 / 100\n",
            "    Training Loss: 0.464\tValidation Loss: 0.465\tTime: 6.546227216720581\n",
            "Saving model 22\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment: 23 / 25\n",
            "  Epoch 1 / 100\n",
            "    Training Loss: 0.690\tValidation Loss: 0.689\tTime: 6.5107057094573975\n",
            "  Epoch 2 / 100\n",
            "    Training Loss: 0.689\tValidation Loss: 0.688\tTime: 6.61509895324707\n",
            "  Epoch 3 / 100\n",
            "    Training Loss: 0.689\tValidation Loss: 0.687\tTime: 6.507620096206665\n",
            "  Epoch 4 / 100\n",
            "    Training Loss: 0.688\tValidation Loss: 0.687\tTime: 6.4930360317230225\n",
            "  Epoch 5 / 100\n",
            "    Training Loss: 0.687\tValidation Loss: 0.686\tTime: 6.587199926376343\n",
            "  Epoch 6 / 100\n",
            "    Training Loss: 0.686\tValidation Loss: 0.685\tTime: 6.3977673053741455\n",
            "  Epoch 7 / 100\n",
            "    Training Loss: 0.686\tValidation Loss: 0.684\tTime: 6.594753742218018\n",
            "  Epoch 8 / 100\n",
            "    Training Loss: 0.684\tValidation Loss: 0.683\tTime: 6.554877042770386\n",
            "  Epoch 9 / 100\n",
            "    Training Loss: 0.683\tValidation Loss: 0.683\tTime: 6.6191487312316895\n",
            "  Epoch 10 / 100\n",
            "    Training Loss: 0.683\tValidation Loss: 0.682\tTime: 6.53437876701355\n",
            "  Epoch 11 / 100\n",
            "    Training Loss: 0.682\tValidation Loss: 0.681\tTime: 6.409296274185181\n",
            "  Epoch 12 / 100\n",
            "    Training Loss: 0.681\tValidation Loss: 0.680\tTime: 6.425727128982544\n",
            "  Epoch 13 / 100\n",
            "    Training Loss: 0.679\tValidation Loss: 0.679\tTime: 6.5161683559417725\n",
            "  Epoch 14 / 100\n",
            "    Training Loss: 0.679\tValidation Loss: 0.678\tTime: 6.528404235839844\n",
            "  Epoch 15 / 100\n",
            "    Training Loss: 0.679\tValidation Loss: 0.678\tTime: 6.404543161392212\n",
            "  Epoch 16 / 100\n",
            "    Training Loss: 0.678\tValidation Loss: 0.677\tTime: 6.358230352401733\n",
            "  Epoch 17 / 100\n",
            "    Training Loss: 0.676\tValidation Loss: 0.676\tTime: 6.456179141998291\n",
            "  Epoch 18 / 100\n",
            "    Training Loss: 0.675\tValidation Loss: 0.675\tTime: 6.431140899658203\n",
            "  Epoch 19 / 100\n",
            "    Training Loss: 0.674\tValidation Loss: 0.674\tTime: 6.571233749389648\n",
            "  Epoch 20 / 100\n",
            "    Training Loss: 0.673\tValidation Loss: 0.673\tTime: 6.533574342727661\n",
            "  Epoch 21 / 100\n",
            "    Training Loss: 0.670\tValidation Loss: 0.672\tTime: 6.612398624420166\n",
            "  Epoch 22 / 100\n",
            "    Training Loss: 0.671\tValidation Loss: 0.671\tTime: 6.5045647621154785\n",
            "  Epoch 23 / 100\n",
            "    Training Loss: 0.668\tValidation Loss: 0.669\tTime: 6.51094126701355\n",
            "  Epoch 24 / 100\n",
            "    Training Loss: 0.668\tValidation Loss: 0.668\tTime: 6.614914417266846\n",
            "  Epoch 25 / 100\n",
            "    Training Loss: 0.667\tValidation Loss: 0.667\tTime: 6.529454231262207\n",
            "  Epoch 26 / 100\n",
            "    Training Loss: 0.665\tValidation Loss: 0.666\tTime: 6.610255241394043\n",
            "  Epoch 27 / 100\n",
            "    Training Loss: 0.663\tValidation Loss: 0.665\tTime: 6.644985675811768\n",
            "  Epoch 28 / 100\n",
            "    Training Loss: 0.663\tValidation Loss: 0.663\tTime: 6.72373628616333\n",
            "  Epoch 29 / 100\n",
            "    Training Loss: 0.662\tValidation Loss: 0.662\tTime: 6.820431232452393\n",
            "  Epoch 30 / 100\n",
            "    Training Loss: 0.660\tValidation Loss: 0.661\tTime: 6.762710094451904\n",
            "  Epoch 31 / 100\n",
            "    Training Loss: 0.660\tValidation Loss: 0.660\tTime: 6.819440603256226\n",
            "  Epoch 32 / 100\n",
            "    Training Loss: 0.657\tValidation Loss: 0.659\tTime: 6.640733480453491\n",
            "  Epoch 33 / 100\n",
            "    Training Loss: 0.657\tValidation Loss: 0.657\tTime: 6.758734703063965\n",
            "  Epoch 34 / 100\n",
            "    Training Loss: 0.654\tValidation Loss: 0.656\tTime: 6.7294745445251465\n",
            "  Epoch 35 / 100\n",
            "    Training Loss: 0.653\tValidation Loss: 0.654\tTime: 6.843489646911621\n",
            "  Epoch 36 / 100\n",
            "    Training Loss: 0.651\tValidation Loss: 0.653\tTime: 6.7331578731536865\n",
            "  Epoch 37 / 100\n",
            "    Training Loss: 0.650\tValidation Loss: 0.652\tTime: 6.655706405639648\n",
            "  Epoch 38 / 100\n",
            "    Training Loss: 0.649\tValidation Loss: 0.650\tTime: 6.406026124954224\n",
            "  Epoch 39 / 100\n",
            "    Training Loss: 0.646\tValidation Loss: 0.648\tTime: 6.453918933868408\n",
            "  Epoch 40 / 100\n",
            "    Training Loss: 0.644\tValidation Loss: 0.647\tTime: 6.4778666496276855\n",
            "  Epoch 41 / 100\n",
            "    Training Loss: 0.644\tValidation Loss: 0.645\tTime: 6.468695640563965\n",
            "  Epoch 42 / 100\n",
            "    Training Loss: 0.642\tValidation Loss: 0.643\tTime: 6.500088453292847\n",
            "  Epoch 43 / 100\n",
            "    Training Loss: 0.639\tValidation Loss: 0.641\tTime: 6.550674915313721\n",
            "  Epoch 44 / 100\n",
            "    Training Loss: 0.639\tValidation Loss: 0.640\tTime: 6.37120246887207\n",
            "  Epoch 45 / 100\n",
            "    Training Loss: 0.636\tValidation Loss: 0.638\tTime: 6.569228410720825\n",
            "  Epoch 46 / 100\n",
            "    Training Loss: 0.633\tValidation Loss: 0.636\tTime: 6.5646302700042725\n",
            "  Epoch 47 / 100\n",
            "    Training Loss: 0.630\tValidation Loss: 0.634\tTime: 6.537443399429321\n",
            "  Epoch 48 / 100\n",
            "    Training Loss: 0.629\tValidation Loss: 0.632\tTime: 6.492273569107056\n",
            "  Epoch 49 / 100\n",
            "    Training Loss: 0.627\tValidation Loss: 0.630\tTime: 6.433273077011108\n",
            "  Epoch 50 / 100\n",
            "    Training Loss: 0.626\tValidation Loss: 0.628\tTime: 6.465116262435913\n",
            "  Epoch 51 / 100\n",
            "    Training Loss: 0.622\tValidation Loss: 0.625\tTime: 6.470128774642944\n",
            "  Epoch 52 / 100\n",
            "    Training Loss: 0.618\tValidation Loss: 0.623\tTime: 6.632157802581787\n",
            "  Epoch 53 / 100\n",
            "    Training Loss: 0.617\tValidation Loss: 0.621\tTime: 6.505417108535767\n",
            "  Epoch 54 / 100\n",
            "    Training Loss: 0.615\tValidation Loss: 0.619\tTime: 6.483214616775513\n",
            "  Epoch 55 / 100\n",
            "    Training Loss: 0.612\tValidation Loss: 0.616\tTime: 6.482672452926636\n",
            "  Epoch 56 / 100\n",
            "    Training Loss: 0.610\tValidation Loss: 0.614\tTime: 6.555199146270752\n",
            "  Epoch 57 / 100\n",
            "    Training Loss: 0.608\tValidation Loss: 0.611\tTime: 6.60991358757019\n",
            "  Epoch 58 / 100\n",
            "    Training Loss: 0.603\tValidation Loss: 0.609\tTime: 6.683167219161987\n",
            "  Epoch 59 / 100\n",
            "    Training Loss: 0.602\tValidation Loss: 0.606\tTime: 6.51001763343811\n",
            "  Epoch 60 / 100\n",
            "    Training Loss: 0.599\tValidation Loss: 0.604\tTime: 6.678391456604004\n",
            "  Epoch 61 / 100\n",
            "    Training Loss: 0.597\tValidation Loss: 0.601\tTime: 6.50486946105957\n",
            "  Epoch 62 / 100\n",
            "    Training Loss: 0.593\tValidation Loss: 0.599\tTime: 6.634298801422119\n",
            "  Epoch 63 / 100\n",
            "    Training Loss: 0.591\tValidation Loss: 0.596\tTime: 6.631752252578735\n",
            "  Epoch 64 / 100\n",
            "    Training Loss: 0.587\tValidation Loss: 0.593\tTime: 6.57177996635437\n",
            "  Epoch 65 / 100\n",
            "    Training Loss: 0.585\tValidation Loss: 0.589\tTime: 6.720450401306152\n",
            "  Epoch 66 / 100\n",
            "    Training Loss: 0.584\tValidation Loss: 0.586\tTime: 6.821537256240845\n",
            "  Epoch 67 / 100\n",
            "    Training Loss: 0.580\tValidation Loss: 0.583\tTime: 6.738439559936523\n",
            "  Epoch 68 / 100\n",
            "    Training Loss: 0.577\tValidation Loss: 0.581\tTime: 6.75041651725769\n",
            "  Epoch 69 / 100\n",
            "    Training Loss: 0.570\tValidation Loss: 0.578\tTime: 6.703091859817505\n",
            "  Epoch 70 / 100\n",
            "    Training Loss: 0.568\tValidation Loss: 0.574\tTime: 6.6585774421691895\n",
            "  Epoch 71 / 100\n",
            "    Training Loss: 0.563\tValidation Loss: 0.571\tTime: 6.705554485321045\n",
            "  Epoch 72 / 100\n",
            "    Training Loss: 0.563\tValidation Loss: 0.568\tTime: 6.730782985687256\n",
            "  Epoch 73 / 100\n",
            "    Training Loss: 0.560\tValidation Loss: 0.564\tTime: 6.6913158893585205\n",
            "  Epoch 74 / 100\n",
            "    Training Loss: 0.554\tValidation Loss: 0.561\tTime: 6.593518972396851\n",
            "  Epoch 75 / 100\n",
            "    Training Loss: 0.549\tValidation Loss: 0.557\tTime: 6.4940173625946045\n",
            "  Epoch 76 / 100\n",
            "    Training Loss: 0.543\tValidation Loss: 0.554\tTime: 6.586740970611572\n",
            "  Epoch 77 / 100\n",
            "    Training Loss: 0.544\tValidation Loss: 0.550\tTime: 6.501375913619995\n",
            "  Epoch 78 / 100\n",
            "    Training Loss: 0.540\tValidation Loss: 0.547\tTime: 6.599372148513794\n",
            "  Epoch 79 / 100\n",
            "    Training Loss: 0.533\tValidation Loss: 0.543\tTime: 6.545689821243286\n",
            "  Epoch 80 / 100\n",
            "    Training Loss: 0.530\tValidation Loss: 0.539\tTime: 6.540061712265015\n",
            "  Epoch 81 / 100\n",
            "    Training Loss: 0.530\tValidation Loss: 0.536\tTime: 6.448134899139404\n",
            "  Epoch 82 / 100\n",
            "    Training Loss: 0.527\tValidation Loss: 0.532\tTime: 6.43375039100647\n",
            "  Epoch 83 / 100\n",
            "    Training Loss: 0.521\tValidation Loss: 0.528\tTime: 6.502518892288208\n",
            "  Epoch 84 / 100\n",
            "    Training Loss: 0.519\tValidation Loss: 0.525\tTime: 6.512762069702148\n",
            "  Epoch 85 / 100\n",
            "    Training Loss: 0.512\tValidation Loss: 0.521\tTime: 6.397203683853149\n",
            "  Epoch 86 / 100\n",
            "    Training Loss: 0.507\tValidation Loss: 0.517\tTime: 6.490642070770264\n",
            "  Epoch 87 / 100\n",
            "    Training Loss: 0.503\tValidation Loss: 0.513\tTime: 6.439944267272949\n",
            "  Epoch 88 / 100\n",
            "    Training Loss: 0.500\tValidation Loss: 0.509\tTime: 6.529698133468628\n",
            "  Epoch 89 / 100\n",
            "    Training Loss: 0.497\tValidation Loss: 0.505\tTime: 6.513109922409058\n",
            "  Epoch 90 / 100\n",
            "    Training Loss: 0.497\tValidation Loss: 0.501\tTime: 6.420555591583252\n",
            "  Epoch 91 / 100\n",
            "    Training Loss: 0.488\tValidation Loss: 0.497\tTime: 6.372694492340088\n",
            "  Epoch 92 / 100\n",
            "    Training Loss: 0.485\tValidation Loss: 0.493\tTime: 6.557394742965698\n",
            "  Epoch 93 / 100\n",
            "    Training Loss: 0.481\tValidation Loss: 0.489\tTime: 6.531389474868774\n",
            "  Epoch 94 / 100\n",
            "    Training Loss: 0.480\tValidation Loss: 0.485\tTime: 6.534696817398071\n",
            "  Epoch 95 / 100\n",
            "    Training Loss: 0.478\tValidation Loss: 0.481\tTime: 6.569591045379639\n",
            "  Epoch 96 / 100\n",
            "    Training Loss: 0.468\tValidation Loss: 0.477\tTime: 6.6437036991119385\n",
            "  Epoch 97 / 100\n",
            "    Training Loss: 0.465\tValidation Loss: 0.473\tTime: 6.501832723617554\n",
            "  Epoch 98 / 100\n",
            "    Training Loss: 0.464\tValidation Loss: 0.468\tTime: 6.556658029556274\n",
            "  Epoch 99 / 100\n",
            "    Training Loss: 0.456\tValidation Loss: 0.464\tTime: 6.443584680557251\n",
            "  Epoch 100 / 100\n",
            "    Training Loss: 0.452\tValidation Loss: 0.460\tTime: 6.55475378036499\n",
            "Saving model 23\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment: 24 / 25\n",
            "  Epoch 1 / 100\n",
            "    Training Loss: 0.700\tValidation Loss: 0.697\tTime: 19.120361804962158\n",
            "  Epoch 2 / 100\n",
            "    Training Loss: 0.694\tValidation Loss: 0.692\tTime: 18.905736923217773\n",
            "  Epoch 3 / 100\n",
            "    Training Loss: 0.691\tValidation Loss: 0.690\tTime: 18.668697118759155\n",
            "  Epoch 4 / 100\n",
            "    Training Loss: 0.689\tValidation Loss: 0.688\tTime: 18.449039936065674\n",
            "  Epoch 5 / 100\n",
            "    Training Loss: 0.687\tValidation Loss: 0.686\tTime: 18.43589425086975\n",
            "  Epoch 6 / 100\n",
            "    Training Loss: 0.685\tValidation Loss: 0.685\tTime: 18.44917321205139\n",
            "  Epoch 7 / 100\n",
            "    Training Loss: 0.685\tValidation Loss: 0.683\tTime: 18.353456497192383\n",
            "  Epoch 8 / 100\n",
            "    Training Loss: 0.682\tValidation Loss: 0.681\tTime: 18.224706649780273\n",
            "  Epoch 9 / 100\n",
            "    Training Loss: 0.680\tValidation Loss: 0.679\tTime: 18.0592143535614\n",
            "  Epoch 10 / 100\n",
            "    Training Loss: 0.678\tValidation Loss: 0.678\tTime: 18.267192363739014\n",
            "  Epoch 11 / 100\n",
            "    Training Loss: 0.677\tValidation Loss: 0.678\tTime: 18.470248460769653\n",
            "  Epoch 12 / 100\n",
            "    Training Loss: 0.676\tValidation Loss: 0.676\tTime: 18.45929455757141\n",
            "  Epoch 13 / 100\n",
            "    Training Loss: 0.676\tValidation Loss: 0.674\tTime: 18.56917905807495\n",
            "  Epoch 14 / 100\n",
            "    Training Loss: 0.672\tValidation Loss: 0.673\tTime: 18.69664454460144\n",
            "  Epoch 15 / 100\n",
            "    Training Loss: 0.672\tValidation Loss: 0.671\tTime: 18.885128498077393\n",
            "  Epoch 16 / 100\n",
            "    Training Loss: 0.668\tValidation Loss: 0.670\tTime: 18.85846734046936\n",
            "  Epoch 17 / 100\n",
            "    Training Loss: 0.668\tValidation Loss: 0.668\tTime: 18.503758430480957\n",
            "  Epoch 18 / 100\n",
            "    Training Loss: 0.667\tValidation Loss: 0.666\tTime: 18.44999051094055\n",
            "  Epoch 19 / 100\n",
            "    Training Loss: 0.665\tValidation Loss: 0.665\tTime: 18.389437198638916\n",
            "  Epoch 20 / 100\n",
            "    Training Loss: 0.661\tValidation Loss: 0.663\tTime: 18.398826599121094\n",
            "  Epoch 21 / 100\n",
            "    Training Loss: 0.661\tValidation Loss: 0.662\tTime: 18.286007404327393\n",
            "  Epoch 22 / 100\n",
            "    Training Loss: 0.658\tValidation Loss: 0.660\tTime: 18.272608280181885\n",
            "  Epoch 23 / 100\n",
            "    Training Loss: 0.656\tValidation Loss: 0.659\tTime: 18.38863492012024\n",
            "  Epoch 24 / 100\n",
            "    Training Loss: 0.656\tValidation Loss: 0.658\tTime: 18.607138633728027\n",
            "  Epoch 25 / 100\n",
            "    Training Loss: 0.654\tValidation Loss: 0.657\tTime: 18.52762532234192\n",
            "  Epoch 26 / 100\n",
            "    Training Loss: 0.652\tValidation Loss: 0.654\tTime: 18.519741773605347\n",
            "  Epoch 27 / 100\n",
            "    Training Loss: 0.648\tValidation Loss: 0.652\tTime: 18.794100046157837\n",
            "  Epoch 28 / 100\n",
            "    Training Loss: 0.646\tValidation Loss: 0.651\tTime: 18.825539588928223\n",
            "  Epoch 29 / 100\n",
            "    Training Loss: 0.644\tValidation Loss: 0.649\tTime: 18.725534915924072\n",
            "  Epoch 30 / 100\n",
            "    Training Loss: 0.644\tValidation Loss: 0.647\tTime: 18.511986017227173\n",
            "  Epoch 31 / 100\n",
            "    Training Loss: 0.640\tValidation Loss: 0.646\tTime: 18.520659923553467\n",
            "  Epoch 32 / 100\n",
            "    Training Loss: 0.637\tValidation Loss: 0.643\tTime: 18.24077844619751\n",
            "  Epoch 33 / 100\n",
            "    Training Loss: 0.638\tValidation Loss: 0.642\tTime: 18.286375761032104\n",
            "  Epoch 34 / 100\n",
            "    Training Loss: 0.632\tValidation Loss: 0.639\tTime: 18.400252103805542\n",
            "  Epoch 35 / 100\n",
            "    Training Loss: 0.630\tValidation Loss: 0.637\tTime: 18.39997673034668\n",
            "  Epoch 36 / 100\n",
            "    Training Loss: 0.630\tValidation Loss: 0.635\tTime: 18.437098503112793\n",
            "  Epoch 37 / 100\n",
            "    Training Loss: 0.627\tValidation Loss: 0.632\tTime: 18.339142084121704\n",
            "  Epoch 38 / 100\n",
            "    Training Loss: 0.624\tValidation Loss: 0.630\tTime: 18.449485063552856\n",
            "  Epoch 39 / 100\n",
            "    Training Loss: 0.620\tValidation Loss: 0.627\tTime: 18.480100631713867\n",
            "  Epoch 40 / 100\n",
            "    Training Loss: 0.619\tValidation Loss: 0.625\tTime: 18.725181341171265\n",
            "  Epoch 41 / 100\n",
            "    Training Loss: 0.616\tValidation Loss: 0.623\tTime: 18.708908319473267\n",
            "  Epoch 42 / 100\n",
            "    Training Loss: 0.613\tValidation Loss: 0.621\tTime: 18.575581312179565\n",
            "  Epoch 43 / 100\n",
            "    Training Loss: 0.611\tValidation Loss: 0.618\tTime: 18.37130880355835\n",
            "  Epoch 44 / 100\n",
            "    Training Loss: 0.606\tValidation Loss: 0.615\tTime: 18.512470245361328\n",
            "  Epoch 45 / 100\n",
            "    Training Loss: 0.600\tValidation Loss: 0.613\tTime: 18.291259765625\n",
            "  Epoch 46 / 100\n",
            "    Training Loss: 0.597\tValidation Loss: 0.610\tTime: 18.36838722229004\n",
            "  Epoch 47 / 100\n",
            "    Training Loss: 0.597\tValidation Loss: 0.607\tTime: 18.434442043304443\n",
            "  Epoch 48 / 100\n",
            "    Training Loss: 0.590\tValidation Loss: 0.604\tTime: 18.277227640151978\n",
            "  Epoch 49 / 100\n",
            "    Training Loss: 0.593\tValidation Loss: 0.601\tTime: 18.63209867477417\n",
            "  Epoch 50 / 100\n",
            "    Training Loss: 0.584\tValidation Loss: 0.598\tTime: 18.458340644836426\n",
            "  Epoch 51 / 100\n",
            "    Training Loss: 0.581\tValidation Loss: 0.595\tTime: 18.49898624420166\n",
            "  Epoch 52 / 100\n",
            "    Training Loss: 0.579\tValidation Loss: 0.593\tTime: 18.51352334022522\n",
            "  Epoch 53 / 100\n",
            "    Training Loss: 0.579\tValidation Loss: 0.590\tTime: 18.637744426727295\n",
            "  Epoch 54 / 100\n",
            "    Training Loss: 0.574\tValidation Loss: 0.588\tTime: 18.649269342422485\n",
            "  Epoch 55 / 100\n",
            "    Training Loss: 0.572\tValidation Loss: 0.584\tTime: 18.728217363357544\n",
            "  Epoch 56 / 100\n",
            "    Training Loss: 0.564\tValidation Loss: 0.580\tTime: 18.302502155303955\n",
            "  Epoch 57 / 100\n",
            "    Training Loss: 0.561\tValidation Loss: 0.577\tTime: 18.217754364013672\n",
            "  Epoch 58 / 100\n",
            "    Training Loss: 0.559\tValidation Loss: 0.575\tTime: 18.30047917366028\n",
            "  Epoch 59 / 100\n",
            "    Training Loss: 0.552\tValidation Loss: 0.570\tTime: 18.29014539718628\n",
            "  Epoch 60 / 100\n",
            "    Training Loss: 0.552\tValidation Loss: 0.567\tTime: 18.525677919387817\n",
            "  Epoch 61 / 100\n",
            "    Training Loss: 0.547\tValidation Loss: 0.563\tTime: 18.387346506118774\n",
            "  Epoch 62 / 100\n",
            "    Training Loss: 0.542\tValidation Loss: 0.559\tTime: 18.447431802749634\n",
            "  Epoch 63 / 100\n",
            "    Training Loss: 0.542\tValidation Loss: 0.556\tTime: 18.490181922912598\n",
            "  Epoch 64 / 100\n",
            "    Training Loss: 0.537\tValidation Loss: 0.553\tTime: 18.59085440635681\n",
            "  Epoch 65 / 100\n",
            "    Training Loss: 0.532\tValidation Loss: 0.549\tTime: 18.75150203704834\n",
            "  Epoch 66 / 100\n",
            "    Training Loss: 0.523\tValidation Loss: 0.544\tTime: 18.971691370010376\n",
            "  Epoch 67 / 100\n",
            "    Training Loss: 0.517\tValidation Loss: 0.540\tTime: 18.775651693344116\n",
            "  Epoch 68 / 100\n",
            "    Training Loss: 0.519\tValidation Loss: 0.535\tTime: 18.647130012512207\n",
            "  Epoch 69 / 100\n",
            "    Training Loss: 0.517\tValidation Loss: 0.532\tTime: 18.47140097618103\n",
            "  Epoch 70 / 100\n",
            "    Training Loss: 0.510\tValidation Loss: 0.528\tTime: 18.423173666000366\n",
            "  Epoch 71 / 100\n",
            "    Training Loss: 0.510\tValidation Loss: 0.526\tTime: 18.490107774734497\n",
            "  Epoch 72 / 100\n",
            "    Training Loss: 0.504\tValidation Loss: 0.521\tTime: 18.457548141479492\n",
            "  Epoch 73 / 100\n",
            "    Training Loss: 0.497\tValidation Loss: 0.517\tTime: 18.24619960784912\n",
            "  Epoch 74 / 100\n",
            "    Training Loss: 0.496\tValidation Loss: 0.512\tTime: 18.264586925506592\n",
            "  Epoch 75 / 100\n",
            "    Training Loss: 0.492\tValidation Loss: 0.508\tTime: 18.250462293624878\n",
            "  Epoch 76 / 100\n",
            "    Training Loss: 0.489\tValidation Loss: 0.505\tTime: 18.57640242576599\n",
            "  Epoch 77 / 100\n",
            "    Training Loss: 0.476\tValidation Loss: 0.500\tTime: 18.62996506690979\n",
            "  Epoch 78 / 100\n",
            "    Training Loss: 0.477\tValidation Loss: 0.495\tTime: 18.77433681488037\n",
            "  Epoch 79 / 100\n",
            "    Training Loss: 0.470\tValidation Loss: 0.492\tTime: 18.766305685043335\n",
            "  Epoch 80 / 100\n",
            "    Training Loss: 0.470\tValidation Loss: 0.487\tTime: 18.80766248703003\n",
            "  Epoch 81 / 100\n",
            "    Training Loss: 0.462\tValidation Loss: 0.484\tTime: 18.681562662124634\n",
            "  Epoch 82 / 100\n",
            "    Training Loss: 0.459\tValidation Loss: 0.478\tTime: 18.523858547210693\n",
            "  Epoch 83 / 100\n",
            "    Training Loss: 0.449\tValidation Loss: 0.474\tTime: 18.542198181152344\n",
            "  Epoch 84 / 100\n",
            "    Training Loss: 0.449\tValidation Loss: 0.471\tTime: 18.551900625228882\n",
            "  Epoch 85 / 100\n",
            "    Training Loss: 0.451\tValidation Loss: 0.468\tTime: 18.473067045211792\n",
            "  Epoch 86 / 100\n",
            "    Training Loss: 0.446\tValidation Loss: 0.463\tTime: 18.525652408599854\n",
            "  Epoch 87 / 100\n",
            "    Training Loss: 0.439\tValidation Loss: 0.458\tTime: 18.368650913238525\n",
            "  Epoch 88 / 100\n",
            "    Training Loss: 0.431\tValidation Loss: 0.455\tTime: 18.360461711883545\n",
            "  Epoch 89 / 100\n",
            "    Training Loss: 0.430\tValidation Loss: 0.450\tTime: 18.51937460899353\n",
            "  Epoch 90 / 100\n",
            "    Training Loss: 0.421\tValidation Loss: 0.445\tTime: 18.509171962738037\n",
            "  Epoch 91 / 100\n",
            "    Training Loss: 0.425\tValidation Loss: 0.441\tTime: 18.711475372314453\n",
            "  Epoch 92 / 100\n",
            "    Training Loss: 0.419\tValidation Loss: 0.437\tTime: 18.78800392150879\n",
            "  Epoch 93 / 100\n",
            "    Training Loss: 0.412\tValidation Loss: 0.435\tTime: 18.8234646320343\n",
            "  Epoch 94 / 100\n",
            "    Training Loss: 0.408\tValidation Loss: 0.430\tTime: 18.730170965194702\n",
            "  Epoch 95 / 100\n",
            "    Training Loss: 0.407\tValidation Loss: 0.423\tTime: 18.3933584690094\n",
            "  Epoch 96 / 100\n",
            "    Training Loss: 0.402\tValidation Loss: 0.421\tTime: 18.47633957862854\n",
            "  Epoch 97 / 100\n",
            "    Training Loss: 0.393\tValidation Loss: 0.417\tTime: 18.388601303100586\n",
            "  Epoch 98 / 100\n",
            "    Training Loss: 0.390\tValidation Loss: 0.412\tTime: 18.578636646270752\n",
            "  Epoch 99 / 100\n",
            "    Training Loss: 0.391\tValidation Loss: 0.408\tTime: 18.461864709854126\n",
            "  Epoch 100 / 100\n",
            "    Training Loss: 0.377\tValidation Loss: 0.404\tTime: 18.313795804977417\n",
            "Saving model 24\n",
            "Słownik został zapisany do pliku ./logs.json.\n",
            "Zapis do logs.json\n"
          ]
        }
      ],
      "source": [
        "lrs = [0.0001, 0.00001, 0.00005, 0.000005, 0.000001]\n",
        "exp_title = 'test_lr_unstable'\n",
        "# xlm-roberta-base xlm-roberta-large bert-base-uncased roberta-base roberta-large\n",
        "pretrained_model_names = ['xlm-roberta-base', 'xlm-roberta-large', 'bert-base-uncased', 'roberta-base', 'roberta-large']\n",
        "model_type = Ext_Arch\n",
        "lr_base = 0.00001\n",
        "optimizer_type = torch.optim.AdamW\n",
        "loss_func_type = nn.NLLLoss\n",
        "max_length = 25 \n",
        "padding = 'max_length'\n",
        "truncation=True\n",
        "batch_size = 32\n",
        "epoch_base=10\n",
        "\n",
        "\n",
        "\n",
        "exp_index = 0\n",
        "total_exp = len(lrs) * len(pretrained_model_names)\n",
        "for lr in lrs:\n",
        "    for pretrained_model_name in pretrained_model_names:\n",
        "        model = get_model(pretrained_model_name, model_type, device)\n",
        "        train_dataloader, val_dataloader, test_data, weights = tokenize_data(pretrained_model_name, max_length, padding, truncation, batch_size, device)\n",
        "        loss_func  = loss_func_type(weight=weights)\n",
        "\n",
        "        print(f'Experiment: {exp_index} / {total_exp}')\n",
        "        optimizer = optimizer_type(model.parameters(), lr=lr)\n",
        "        epochs = int(lr_base / lr * epoch_base)\n",
        "        if epochs < 1:\n",
        "            epochs = 1\n",
        "        do_exp(model, train_dataloader, val_dataloader, test_data, device, loss_func, optimizer, epochs, lr, \n",
        "            batch_size, max_length, padding, truncation, pretrained_model_name, exp_title, exp_index, root)\n",
        "        exp_index += 1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "a = \"\"\"A\n",
        "B\n",
        "A\n",
        "B\n",
        "A\n",
        "B\n",
        "A\n",
        "B\n",
        "A\n",
        "B\n",
        "A\n",
        "B\n",
        "A\n",
        "B\n",
        "A\n",
        "B\n",
        "A\n",
        "B\n",
        "A\n",
        "B\n",
        "A\n",
        "B\n",
        "A\n",
        "B\n",
        "A\n",
        "B\n",
        "A\n",
        "B\n",
        "A\n",
        "B\n",
        "A\n",
        "B\n",
        "A\n",
        "B\n",
        "A\n",
        "B\n",
        "A\n",
        "B\n",
        "A\n",
        "B\n",
        "A\n",
        "B\n",
        "A\n",
        "B\n",
        "A\n",
        "B\n",
        "A\n",
        "B\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "24"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(a.split('\\n'))//2\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "103e253e94924032bc2c613cbb9e0b36": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_378e12dd17b346fb8294eab15303850c",
            "max": 469,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2e2d61fc6ddd419fbc8f3d0512f08be2",
            "value": 469
          }
        },
        "12df9314815c4784b8c1da9a1552ab24": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_48b8b55ddb514e8d944358c069b4275a",
              "IPY_MODEL_103e253e94924032bc2c613cbb9e0b36",
              "IPY_MODEL_bf1c04a1673945219160c2bae7646ab9"
            ],
            "layout": "IPY_MODEL_5bd86b67abba4caa8779fddedc2887d2"
          }
        },
        "167ba6c6ebc447828d1fd2fb0af6e234": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1ac18f6e9ce9436c9b9373c2b370943a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "269461da8093449b8fa2e459e9fa86c2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c70b66bf6cf411d8790e500d43c2383": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2e2d61fc6ddd419fbc8f3d0512f08be2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3364b8e90a234415b239a90de7d99c8c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "378e12dd17b346fb8294eab15303850c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a363a8a79ab4f1dac9bcf7860353dc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_269461da8093449b8fa2e459e9fa86c2",
            "placeholder": "​",
            "style": "IPY_MODEL_2c70b66bf6cf411d8790e500d43c2383",
            "value": " 380/469 [00:07&lt;00:01, 58.19it/s]"
          }
        },
        "48b8b55ddb514e8d944358c069b4275a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec8d9210e2ea41879958426b058b3c0b",
            "placeholder": "​",
            "style": "IPY_MODEL_167ba6c6ebc447828d1fd2fb0af6e234",
            "value": "100%"
          }
        },
        "4c6e232a01aa440cb3dcad2315cf20aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ee4588cdbc10473ababf284d533af77b",
            "placeholder": "​",
            "style": "IPY_MODEL_cddbdf02e0fe4b919071416f62300d71",
            "value": " 81%"
          }
        },
        "4dd64aef55a94f11838fa8a380979db6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5bd86b67abba4caa8779fddedc2887d2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "669fcdd3c7b74673aa71293fe14e646a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "88b1f1b0b2b5422798627d32d16187bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4dd64aef55a94f11838fa8a380979db6",
            "max": 469,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c194336f053e42cfb33f9c857a29382d",
            "value": 380
          }
        },
        "bf1c04a1673945219160c2bae7646ab9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3364b8e90a234415b239a90de7d99c8c",
            "placeholder": "​",
            "style": "IPY_MODEL_669fcdd3c7b74673aa71293fe14e646a",
            "value": " 469/469 [00:07&lt;00:00, 62.80it/s]"
          }
        },
        "c194336f053e42cfb33f9c857a29382d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cddbdf02e0fe4b919071416f62300d71": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ec8d9210e2ea41879958426b058b3c0b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee4588cdbc10473ababf284d533af77b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fecd9089323645b3aca90497a98c7268": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4c6e232a01aa440cb3dcad2315cf20aa",
              "IPY_MODEL_88b1f1b0b2b5422798627d32d16187bc",
              "IPY_MODEL_3a363a8a79ab4f1dac9bcf7860353dc6"
            ],
            "layout": "IPY_MODEL_1ac18f6e9ce9436c9b9373c2b370943a"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
